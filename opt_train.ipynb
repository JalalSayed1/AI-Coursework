{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "anticipated-consumer",
      "metadata": {
        "id": "anticipated-consumer"
      },
      "source": [
        "In this assignment, we are going to implement see if we can optimally select a subset of training instances for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "higher-nebraska",
      "metadata": {
        "id": "higher-nebraska"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daily-internship",
      "metadata": {
        "id": "daily-internship"
      },
      "source": [
        "We are going to work with the MNIST dataset, a popular dataset for hand-written digit recognition. Here we load the datatset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "palestinian-texas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palestinian-texas",
        "outputId": "2e6494ee-e47d-4968-e8b5-01374df7d5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n",
            "Num of data points per class in train set: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1) # -1 means the last axis\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "\n",
        "#! scale down the training set to 10_000 samples\n",
        "import random\n",
        "random.seed(42)\n",
        "train_size = 10_200\n",
        "test_size = 200\n",
        "# make x_train have roughly same number of samples for each class:\n",
        "# x_train = np.concatenate([x_train[y_train == i][:train_size // 10] for i in range(10)])\n",
        "# y_train = np.concatenate([y_train[y_train == i][:train_size // 10] for i in range(10)])\n",
        "# # make x_test have roughly same number of samples for each class:\n",
        "# x_test = np.concatenate([x_test[y_test == i][:test_size // 10] for i in range(10)])\n",
        "# y_test = np.concatenate([y_test[y_test == i][:test_size // 10] for i in range(10)])\n",
        "\n",
        "# x_train = x_train[:train_size]\n",
        "# y_train = y_train[:train_size]\n",
        "# get test sets from x_train and y_train:\n",
        "# random_indices = np.random.choice(x_train.shape[0], size=test_size, replace=False)\n",
        "# x_test = x_train[random_indices]\n",
        "# y_test = y_train[random_indices]\n",
        "# delete the test sets from x_train and y_train:\n",
        "# x_train = np.delete(x_train, random_indices, axis=0)\n",
        "# y_train = np.delete(y_train, random_indices)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "print(f\"Num of data points per class in train set: {np.unique(y_train, return_counts=True)[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "95504f5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
          ]
        }
      ],
      "source": [
        "# count how many data points are in each class\n",
        "res = np.unique(y_train, return_counts=True)\n",
        "print({k:v for k,v in zip(res[0], res[1])})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-desert",
      "metadata": {
        "id": "empty-desert"
      },
      "source": [
        "Now corrupt the labels with common types of mistakes. The variable 'noise_probability' controls the amount of errors introduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "champion-technician",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "champion-technician",
        "outputId": "ab792401-d617-4afb-d634-5df238e0ee19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corruptions: {'5->6': 2666, '0->2': 2917, '4->7': 2888, '1->4': 3385, '9->0': 2997, '2->3': 2969, '3->5': 3027, '7->1': 3204, '8->9': 2911, '6->8': 2960}\n",
            "Number of corruptions: 29934\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "noise_probability = 0.5\n",
        "SEED = 314159\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def index(array, item):\n",
        "    for i in range(len(array)):\n",
        "        if item == array[i]:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def corrupt_label(y, y_index, err):\n",
        "    n = len(err)\n",
        "    # select an element at random (index != found)\n",
        "    if (y_index == n-1):\n",
        "        noisy_label = err[0]\n",
        "    else:\n",
        "        noisy_label = err[(y_index + 1)%n]\n",
        "    return noisy_label\n",
        "\n",
        "# We corrupt the MNIST data with some common mistakes, such as 3-->8, 8-->3, 1-->{4, 7}, 5-->6 etc.\n",
        "def corrupt_labels(y_train, noise_probability):\n",
        "    num_samples = y_train.shape[0]\n",
        "    err_es_1 = np.array([0, 2, 3, 5, 6, 8, 9])\n",
        "    err_es_2 = np.array([1, 4, 7])\n",
        "\n",
        "    corruptions = {}\n",
        "    corrupted_indexes = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability:\n",
        "            y = y_train[i]\n",
        "\n",
        "            y_index = index(err_es_1, y)\n",
        "            if y_index >= 0:\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_1)\n",
        "            else:\n",
        "                y_index = index(err_es_2, y)\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_2)\n",
        "\n",
        "            key = str(y_train[i]) + '->' + str(y_noisy)\n",
        "            corrupted_indexes[i] = i\n",
        "\n",
        "            if key in corruptions:\n",
        "                corruptions[key] += 1\n",
        "            else:\n",
        "                corruptions[key] = 0\n",
        "\n",
        "            y_train[i] = y_noisy\n",
        "\n",
        "    return corruptions, corrupted_indexes\n",
        "\n",
        "corruptions, corrupted_indexes = corrupt_labels(y_train, noise_probability)\n",
        "print (\"Corruptions: \" + str(corruptions))\n",
        "print (\"Number of corruptions: {}\".format(len(list(corrupted_indexes.keys()))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3fd83ff4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of pruned indexes: 29934\n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of pruned indexes: {len(corrupted_indexes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ee48cb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 6003, 1: 6561, 2: 5906, 3: 6073, 4: 6339, 5: 5782, 6: 5624, 7: 5949, 8: 5900, 9: 5863}\n"
          ]
        }
      ],
      "source": [
        "# count how many data points are in each class after corruption\n",
        "res = np.unique(y_train, return_counts=True)\n",
        "print({k:v for k,v in zip(res[0], res[1])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "quality-gauge",
      "metadata": {
        "id": "quality-gauge"
      },
      "outputs": [],
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-celebrity",
      "metadata": {
        "id": "fifth-celebrity"
      },
      "source": [
        "Supervised (parametric) training with the (noisy) labeled examples. Note that this model is trained on the entire dataset (the value of the parameter pruned_indexes is null here, which means that we leave out no points), which is noisy (20% of the labels are corrupted). Now the question is: is this the best model that we can train or can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "extreme-ethernet",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extreme-ethernet",
        "outputId": "cb7c5e23-7242-4c71-c830-88dabc51ca3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                54090     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54410 (212.54 KB)\n",
            "Trainable params: 54410 (212.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 3\n",
        "validation_split=0.1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "def prune_points(x_train, y_train, pruned_indexes):\n",
        "    num_samples = x_train.shape[0]\n",
        "    x_train_pruned = []\n",
        "    y_train_pruned = []\n",
        "    for i in range(num_samples):\n",
        "        if not i in pruned_indexes:\n",
        "            x_train_pruned.append(x_train[i])\n",
        "            y_train_pruned.append(y_train[i])\n",
        "\n",
        "    return np.array(x_train_pruned), np.array(y_train_pruned)\n",
        "\n",
        "def trainAndEvaluateModel(x_train, y_train, x_test, y_test, model, pruned_indexes):\n",
        "\n",
        "    if not pruned_indexes == None:\n",
        "        x_train_pruned, y_train_pruned = prune_points(x_train, y_train, pruned_indexes)\n",
        "    else:\n",
        "        x_train_pruned = x_train\n",
        "        y_train_pruned = y_train\n",
        "\n",
        "    model.fit(x_train_pruned, y_train_pruned, batch_size=batch_size, epochs=epochs)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    keras.backend.clear_session() # remove previous training weights\n",
        "    \n",
        "    return loss, accuracy\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-waterproof",
      "metadata": {
        "id": "indie-waterproof"
      },
      "source": [
        "And we call the following function to train a model on the entire dataset and evaluate it on the test set. The accuracy on the test set is quite good, but can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "embedded-staff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embedded-staff",
        "outputId": "707551e0-dc21-4016-8ba9-3b9e05de1069"
      },
      "outputs": [],
      "source": [
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "structured-lithuania",
      "metadata": {
        "id": "structured-lithuania"
      },
      "source": [
        "You need to implement a subset selection function that when called will return a subset of instances which will be used to train the model. This setup ensures that you also pass in another dictionary which contains the indexes of the instances that you would not want to use while training the model, i.e., it should contain a list of indexes that you would decide to **leave out** for training.\n",
        "\n",
        "Here's the code and a sample implementation that returns a randomly chosen set of instances that you are to be left out. Since we chose 70% probability of label corruption (check the **noise_probability** parameter), we also select a subset where we leave out the same proportion of points. This is a baseline implementation and obviously you should aim to achieve better results than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "unique-operator",
      "metadata": {
        "id": "unique-operator"
      },
      "outputs": [],
      "source": [
        "# Here 'x_train', 'y_train' and model' are an unused parameters. But you may get better results by leveraging these.\n",
        "def baseLinePrunedSubsetMethod(x_train, y_train, model):\n",
        "    pruned_indexes = {}\n",
        "    num_samples = x_train.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability: # this is the global variable (only useful for this naive approach)\n",
        "            pruned_indexes[i] = i\n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-steel",
      "metadata": {
        "id": "stunning-steel"
      },
      "source": [
        "Let's see how this naive baseline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "formed-refrigerator",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "formed-refrigerator",
        "outputId": "b37fa32f-1af8-417c-8a33-1f53d42157ba"
      },
      "outputs": [],
      "source": [
        "pruned_indexes = baseLinePrunedSubsetMethod(x_train, y_train, model)\n",
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backed-cache",
      "metadata": {
        "id": "backed-cache"
      },
      "source": [
        "Let's now see if we had known what points were actually corrupted (more of a hypothetical unrealistic situation), does leaving out those points actually improve the model's effectiveness. It turns out that it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "amino-orientation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amino-orientation",
        "outputId": "ed71db95-e2da-4d84-ddb0-8b815f73d1ed"
      },
      "outputs": [],
      "source": [
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bright-constitutional",
      "metadata": {
        "id": "bright-constitutional"
      },
      "source": [
        "Your task is to implement your own version of (say of name **myPrunedSubsetMethod** (which should take as arguments x_train, y_train, and the model). The function should return a dictionary of indexes that are to be left out. Plug your function in and evaluate the results. Write a thorough report on the methodology and analyse the results.\n",
        "\n",
        "Some hints:\n",
        "You can approach this as a discrete state space optimisation problem, where firstly you can define a \"selection batch size\" (this is not the same as training batch size), which decides which batch of instances you're going to leave out. For instance, if you are in a state where the training set is $X$, you may select (by some heuristics) which points you're gonna leave out (let that set be $\\delta \\subset X$) so that a child state becomes $X' = X - \\delta$. Similarly, if you choose a different $\\delta$ you get a different child state. You then need to train and evaluate (call the function *trainAndEvaluateModel*) to see if that child state led to an improvement or not.\n",
        "\n",
        "You are free to use any algorithm, e.g., simulated annealing, A* search, genetic algorithm etc. to implement this discrete state space optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1269e4",
      "metadata": {},
      "source": [
        "# Using Genetic Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f7834ad5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "\n",
        "INPUT_SIZE = x_train.shape[0]\n",
        "\n",
        "#' Define GA parameters\n",
        "POPULATION_SIZE = 100 # number of individuals in population\n",
        "SELECTION_SIZE = ceil(POPULATION_SIZE*0.5) # number of individuals to select for next generation\n",
        "MUTATION_RATE = 0.01 # probability of mutating each individual\n",
        "CROSSOVER_RATE = 0.3 # probability of crossing over two individuals\n",
        "CROSSOVER_POINTS = 3 # number of crossover points\n",
        "GENERATIONS = 100 # number of generations\n",
        "BATTLE_PARTICIPANTS = 4 # number of individuals to participate in a tournament\n",
        "ELITE_NUM = 1 # number of elite individuals to keep from one generation to the next\n",
        "BIAS_STRENGTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da72613c",
      "metadata": {},
      "source": [
        "## Initialise Population:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2d2d0871",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_population(population_size, input_size, y_train, bias_strength=100):\n",
        "    # 0 means prune the point, 1 means keep the point:\n",
        "    population = []\n",
        "    \n",
        "    POPULATION_SIZE = population_size\n",
        "    INPUT_SIZE = input_size\n",
        "\n",
        "    # Calculate inverse frequencies for prioritization\n",
        "    classes, freq = np.unique(y_train, return_counts=True)\n",
        "    class_frequencies = {k:v for k,v in zip(classes, freq)}\n",
        "    max_freq = max(class_frequencies.values())\n",
        "    prioritization_scores = {digit_class: round(max_freq / freq, 2) for digit_class, freq in class_frequencies.items()}\n",
        "    print(f\"Prioritization scores: {prioritization_scores}\")\n",
        "    for _ in range(POPULATION_SIZE):\n",
        "        individual = np.ones(INPUT_SIZE, dtype=int)\n",
        "        for i in range(INPUT_SIZE):\n",
        "            class_label = y_train[i]\n",
        "            # Bias towards selecting indices of more frequent classes\n",
        "            if random.random() < ((prioritization_scores[class_label] / max_freq) * bias_strength):\n",
        "                individual[i] = 0\n",
        "        \n",
        "        # check if individual is not in population already:\n",
        "        # bc of this, accuracy is low for the first few individuals then it gets better.\n",
        "        if not np.any([np.array_equal(individual, indv) for indv in population]):\n",
        "            population.append(individual)\n",
        "            \n",
        "    # convert to tuple for hashability:\n",
        "    population = [tuple(individual) for individual in population]\n",
        "\n",
        "    return population\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999d7d90",
      "metadata": {},
      "source": [
        "## Run Genetic Algorithm:\n",
        "- Evaluate fitness of each individual\n",
        "- Select parents\n",
        "- Crossover\n",
        "- Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ccb61cac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "import time\n",
        "import os\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def evaluate_fitness(model, individual):\n",
        "    '''Because hashing x_train is too slow, x_train and model need to be in the global scope for this to work.'''\n",
        "    pruned_indexes = {i:i for i,elt in enumerate(individual) if elt == 1}\n",
        "    if len(pruned_indexes) == INPUT_SIZE:\n",
        "        return 0.0\n",
        "    elif len(pruned_indexes) == 0:\n",
        "        loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)\n",
        "    else:\n",
        "        loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "    return accuracy\n",
        "\n",
        "def find_top_N_elements(population, fitness_scores, N):\n",
        "    top_N = []\n",
        "    for _ in range(N):\n",
        "        max_index = np.argmax(fitness_scores)\n",
        "        top_N.append(population[max_index])\n",
        "        fitness_scores[max_index] = -1\n",
        "    return top_N\n",
        "\n",
        "def select_individuals(population, fitness_scores, selection_size, battle_participants, elite_num):\n",
        "\n",
        "    selection_size = min(selection_size, len(population))\n",
        "    \n",
        "    # Select individuals for next generation\n",
        "    selected_individuals = []\n",
        "    \n",
        "    #' ELITISM:\n",
        "    selected_individuals.extend(find_top_N_elements(population, fitness_scores, elite_num))\n",
        "    \n",
        "    #' TOURNAMENT SELECTION:\n",
        "    while len(selected_individuals) < selection_size:\n",
        "        # Select BATTLE_PARTICIPANTS individuals at random\n",
        "        participants = random.sample(list(zip(population, fitness_scores)), battle_participants)\n",
        "        # Sort participants by fitness score\n",
        "        sorted_participants = sorted(participants, key=lambda x: x[1], reverse=True)\n",
        "        # Select the best individual from the tournament\n",
        "        selected_individuals.append(sorted_participants[0][0])\n",
        "    return selected_individuals\n",
        "\n",
        "def crossover(individual_1, individual_2, crossover_rate, crossover_points):\n",
        "    # Crossover individuals\n",
        "    if random.random() < crossover_rate:\n",
        "        crossover_points = random.sample(range(1, len(individual_1)), crossover_points)\n",
        "        crossover_points.sort()\n",
        "        new_individual_1 = individual_1[:crossover_points[0]] + individual_2[crossover_points[0]:crossover_points[1]] + individual_1[crossover_points[1]:]\n",
        "        new_individual_2 = individual_2[:crossover_points[0]] + individual_1[crossover_points[0]:crossover_points[1]] + individual_2[crossover_points[1]:]\n",
        "        return new_individual_1, new_individual_2\n",
        "    else:\n",
        "        return individual_1, individual_2\n",
        "    \n",
        "def mutate(individual, mutation_rate):\n",
        "    # Mutate individual\n",
        "    new_individual = []\n",
        "    for gene in individual:\n",
        "        if random.random() < mutation_rate:\n",
        "            new_individual.append(1 - gene) # flip the gene\n",
        "        else:\n",
        "            new_individual.append(gene)\n",
        "    return new_individual\n",
        "\n",
        "def create_next_generation(population, fitness_scores, selection_size, mutation_rate, crossover_rate, crossover_points, battle_participants, elite_num):\n",
        "    # Create next generation\n",
        "    next_generation = []\n",
        "    \n",
        "    # Select individuals for next generation\n",
        "    selected_individuals = select_individuals(population, fitness_scores, selection_size, battle_participants, elite_num)\n",
        "\n",
        "    #' Crossover individuals:\n",
        "    while len(next_generation) < len(population):\n",
        "        # Select two individuals at random\n",
        "        individual_1, individual_2 = random.sample(selected_individuals, 2)\n",
        "        new_individual_1, new_individual_2 = crossover(individual_1, individual_2, crossover_rate, crossover_points)\n",
        "        next_generation.append(new_individual_1)\n",
        "        next_generation.append(new_individual_2)\n",
        "\n",
        "    #' Mutate individuals:\n",
        "    for individual in next_generation:\n",
        "        individual = mutate(individual, mutation_rate)\n",
        "        \n",
        "    return next_generation\n",
        "\n",
        "def genetic_algorithm(x_train, y_train, model, population_size=100, selection_size=50, mutation_rate=0.01, crossover_rate=0.3, crossover_points=3, generations=10, battle_participants=4, elite_num=1, bias_strength=100):\n",
        "    \n",
        "    INPUT_SIZE = y_train.shape[0]\n",
        "    \n",
        "    # set up the file to write results to:\n",
        "    counter = 0\n",
        "    base_dir = \"results\"\n",
        "    if not os.path.exists(base_dir):\n",
        "        os.makedirs(base_dir)\n",
        "    while (filename := f\"GA_{counter}.csv\") in os.listdir(base_dir):\n",
        "        counter += 1\n",
        "    path = os.path.join(base_dir, filename)\n",
        "\n",
        "    with open(path, \"w\") as f:\n",
        "        # write hyper parameters to file:\n",
        "        f.write(f\"population_size: {population_size}\\n\")\n",
        "        f.write(f\"selection_size: {selection_size}\\n\")\n",
        "        f.write(f\"mutation_rate: {mutation_rate}\\n\")\n",
        "        f.write(f\"crossover_rate: {crossover_rate}\\n\")\n",
        "        f.write(f\"crossover_points: {crossover_points}\\n\")\n",
        "        f.write(f\"generations: {generations}\\n\")\n",
        "        f.write(f\"battle_participants: {battle_participants}\\n\")\n",
        "        f.write(f\"elite_num: {elite_num}\\n\")\n",
        "        f.write(f\"\\n\")\n",
        "        \n",
        "        # header:\n",
        "        f.write(\"generation,accuracy\\n\")\n",
        "    \n",
        "    best_indv = ()\n",
        "    best_fitness = 0.0\n",
        "    \n",
        "    # Create initial population\n",
        "    population = create_population(population_size, INPUT_SIZE, y_train, bias_strength)\n",
        "\n",
        "    # Evaluate initial population\n",
        "    fitness_scores = []\n",
        "    for individual in population:\n",
        "        # convert to tuples for hashability:\n",
        "        accuracy = evaluate_fitness(model, individual)\n",
        "        fitness_scores.append(accuracy)\n",
        "        \n",
        "        if accuracy > best_fitness:\n",
        "            best_fitness = accuracy\n",
        "            best_indv = individual\n",
        "    \n",
        "    print(f\"Initial population length: {len(population)}\")\n",
        "    print(f\"Initial fitness scores: max = {max(fitness_scores)}, min = {min(fitness_scores)}\")\n",
        "    print(f\"Best fitness: {best_fitness}\")\n",
        "    \n",
        "    # Iterate through generations\n",
        "    start_time = time.time()\n",
        "    for generation in range(generations):\n",
        "        # print(f\"Generation {generation}\")\n",
        "        # Create next generation\n",
        "        population = create_next_generation(population, fitness_scores, selection_size, mutation_rate, crossover_rate, crossover_points, battle_participants, elite_num)\n",
        "        # Evaluate next generation\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            accuracy = evaluate_fitness(model, individual)\n",
        "            fitness_scores.append(accuracy)\n",
        "            \n",
        "            if accuracy > best_fitness:\n",
        "                best_fitness = accuracy\n",
        "                best_indv = individual\n",
        "        \n",
        "        time_elapsed = round(time.time() - start_time, 2)\n",
        "        unit = \"seconds\"\n",
        "        if time_elapsed > 60:\n",
        "            time_elapsed = time_elapsed/60 \n",
        "            unit = \"minutes\"\n",
        "        if time_elapsed > 60:\n",
        "            time_elapsed = time_elapsed/60\n",
        "            unit = \"hours\"\n",
        "        print(f\"Best fitness after generation {generation}: {best_fitness} took {time_elapsed} {unit} to train.\")\n",
        "\n",
        "\n",
        "        with open(path, \"a\") as f:\n",
        "            f.write(f\"{generation},{best_fitness}\\n\")\n",
        "        \n",
        "    # clear the cache:\n",
        "    evaluate_fitness.cache_clear()\n",
        "    return best_indv, best_fitness\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cb9253aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def myPrunedSubsetMethod(x_train, y_train, model):\n",
        "    best_indv, best_accuracy = genetic_algorithm(x_train, y_train, model, population_size=POPULATION_SIZE, selection_size=SELECTION_SIZE, mutation_rate=MUTATION_RATE, crossover_rate=CROSSOVER_RATE, crossover_points=CROSSOVER_POINTS, generations=GENERATIONS, battle_participants=BATTLE_PARTICIPANTS, elite_num=ELITE_NUM, bias_strength=BIAS_STRENGTH)\n",
        "    print(f\"Best accuracy found: {best_accuracy}\")\n",
        "    pruned_indexes = {i:i for i,elt in enumerate(best_indv) if elt == 1}\n",
        "    print(f\"Length of pruned indexes: {len(pruned_indexes)}\")\n",
        "    \n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5efb83c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prioritization scores: {0: 1.09, 1: 1.0, 2: 1.11, 3: 1.08, 4: 1.04, 5: 1.13, 6: 1.17, 7: 1.1, 8: 1.11, 9: 1.12}\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 1s 16ms/step - loss: 2.2255 - accuracy: 0.1716\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.9750 - accuracy: 0.3512\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.7390 - accuracy: 0.4444\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6310 - accuracy: 0.4278\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.6604 - accuracy: 0.3888\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.5053 - accuracy: 0.4428\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.4007 - accuracy: 0.4342\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.3757 - accuracy: 0.4166\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.4021 - accuracy: 0.4243\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.3172 - accuracy: 0.4509\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.2587 - accuracy: 0.4652\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2916 - accuracy: 0.4112\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.3442 - accuracy: 0.4221\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.2781 - accuracy: 0.4314\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.2472 - accuracy: 0.4549\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2536 - accuracy: 0.3993\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.2713 - accuracy: 0.4326\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.2448 - accuracy: 0.4498\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1857 - accuracy: 0.4701\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2212 - accuracy: 0.4295\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.2935 - accuracy: 0.4286\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.2559 - accuracy: 0.4356\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.2079 - accuracy: 0.4525\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0875 - accuracy: 0.5137\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 1.2296 - accuracy: 0.4621\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1692 - accuracy: 0.4829\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1314 - accuracy: 0.4798\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1714 - accuracy: 0.4362\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1643 - accuracy: 0.4670\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1140 - accuracy: 0.4670\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0887 - accuracy: 0.4880\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1601 - accuracy: 0.4303\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.2099 - accuracy: 0.4263\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.1321 - accuracy: 0.4794\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.0898 - accuracy: 0.5226\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1444 - accuracy: 0.3972\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1497 - accuracy: 0.4536\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1275 - accuracy: 0.4485\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0821 - accuracy: 0.4784\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0527 - accuracy: 0.5026\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.1440 - accuracy: 0.4585\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.1362 - accuracy: 0.4663\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0978 - accuracy: 0.5015\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1369 - accuracy: 0.4532\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.2026 - accuracy: 0.4674\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1401 - accuracy: 0.4684\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1223 - accuracy: 0.4644\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0624 - accuracy: 0.4645\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.1032 - accuracy: 0.4796\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0592 - accuracy: 0.5155\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0251 - accuracy: 0.5097\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0666 - accuracy: 0.4607\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1060 - accuracy: 0.4776\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0618 - accuracy: 0.4915\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0416 - accuracy: 0.4936\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9924 - accuracy: 0.5335\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0737 - accuracy: 0.4624\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0367 - accuracy: 0.5093\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0156 - accuracy: 0.5210\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0292 - accuracy: 0.4579\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1054 - accuracy: 0.4755\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.0590 - accuracy: 0.4955\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0153 - accuracy: 0.5295\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0978 - accuracy: 0.3942\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.0900 - accuracy: 0.4565\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0466 - accuracy: 0.4839\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0106 - accuracy: 0.4956\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0005 - accuracy: 0.4859\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.1163 - accuracy: 0.4510\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0737 - accuracy: 0.4813\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0545 - accuracy: 0.5046\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9990 - accuracy: 0.5065\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0566 - accuracy: 0.4688\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0408 - accuracy: 0.4800\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9970 - accuracy: 0.5138\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0475 - accuracy: 0.4114\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.0384 - accuracy: 0.4860\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0308 - accuracy: 0.5131\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9782 - accuracy: 0.5440\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0286 - accuracy: 0.4387\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1109 - accuracy: 0.4475\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0701 - accuracy: 0.4904\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0463 - accuracy: 0.5000\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9674 - accuracy: 0.5183\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0022 - accuracy: 0.4990\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0088 - accuracy: 0.5176\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9760 - accuracy: 0.5029\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0055 - accuracy: 0.4467\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0255 - accuracy: 0.4967\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9941 - accuracy: 0.5110\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9577 - accuracy: 0.5348\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9805 - accuracy: 0.4558\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0384 - accuracy: 0.4724\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0146 - accuracy: 0.4921\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9757 - accuracy: 0.5167\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9275 - accuracy: 0.5475\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0202 - accuracy: 0.4772\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0126 - accuracy: 0.4740\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9672 - accuracy: 0.5101\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9645 - accuracy: 0.4823\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0506 - accuracy: 0.4571\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0264 - accuracy: 0.5035\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0006 - accuracy: 0.4934\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9667 - accuracy: 0.4738\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0189 - accuracy: 0.4496\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9886 - accuracy: 0.4797\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9620 - accuracy: 0.5194\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0190 - accuracy: 0.4074\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0528 - accuracy: 0.4626\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0019 - accuracy: 0.4990\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9920 - accuracy: 0.4970\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9625 - accuracy: 0.4686\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.0270 - accuracy: 0.4544\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9895 - accuracy: 0.4867\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9693 - accuracy: 0.5076\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9201 - accuracy: 0.5195\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.0015 - accuracy: 0.4928\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9628 - accuracy: 0.5138\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9455 - accuracy: 0.5177\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0052 - accuracy: 0.4457\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0263 - accuracy: 0.4362\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9846 - accuracy: 0.4851\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9581 - accuracy: 0.5170\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9694 - accuracy: 0.4482\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9729 - accuracy: 0.5034\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9724 - accuracy: 0.4995\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9359 - accuracy: 0.5236\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9386 - accuracy: 0.5035\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0114 - accuracy: 0.4990\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9952 - accuracy: 0.5039\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9950 - accuracy: 0.4922\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9507 - accuracy: 0.4958\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9905 - accuracy: 0.4995\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9630 - accuracy: 0.5129\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9431 - accuracy: 0.5324\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9099 - accuracy: 0.5411\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0351 - accuracy: 0.4638\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0024 - accuracy: 0.5072\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9876 - accuracy: 0.4990\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9741 - accuracy: 0.4382\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1.0372 - accuracy: 0.4544\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9819 - accuracy: 0.5113\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9750 - accuracy: 0.4897\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9414 - accuracy: 0.4935\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9949 - accuracy: 0.4929\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9782 - accuracy: 0.4970\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9675 - accuracy: 0.5091\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9603 - accuracy: 0.4753\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9884 - accuracy: 0.4883\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9607 - accuracy: 0.5138\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9341 - accuracy: 0.5189\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9601 - accuracy: 0.4669\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9786 - accuracy: 0.4925\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9407 - accuracy: 0.5216\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9105 - accuracy: 0.5397\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9352 - accuracy: 0.4668\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9611 - accuracy: 0.5025\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9473 - accuracy: 0.5116\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9102 - accuracy: 0.5358\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9052 - accuracy: 0.4961\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9867 - accuracy: 0.4679\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9536 - accuracy: 0.4985\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9205 - accuracy: 0.5439\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9428 - accuracy: 0.4646\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9786 - accuracy: 0.4875\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9276 - accuracy: 0.5052\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9159 - accuracy: 0.5156\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8909 - accuracy: 0.5279\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9703 - accuracy: 0.4850\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9654 - accuracy: 0.5082\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9198 - accuracy: 0.5218\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8780 - accuracy: 0.5375\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9938 - accuracy: 0.4794\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9893 - accuracy: 0.4844\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9436 - accuracy: 0.5055\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9037 - accuracy: 0.5085\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9801 - accuracy: 0.4820\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9527 - accuracy: 0.5149\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9501 - accuracy: 0.5149\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8792 - accuracy: 0.5624\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.0148 - accuracy: 0.4611\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9829 - accuracy: 0.4972\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9424 - accuracy: 0.5313\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9983 - accuracy: 0.4193\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9608 - accuracy: 0.5143\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9401 - accuracy: 0.5184\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9137 - accuracy: 0.5409\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9524 - accuracy: 0.4385\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9368 - accuracy: 0.5159\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9256 - accuracy: 0.5178\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8998 - accuracy: 0.5226\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8886 - accuracy: 0.5078\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9567 - accuracy: 0.4816\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9451 - accuracy: 0.4700\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8997 - accuracy: 0.4974\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9319 - accuracy: 0.4835\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9500 - accuracy: 0.4779\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9392 - accuracy: 0.4862\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9190 - accuracy: 0.5179\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9342 - accuracy: 0.4517\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9958 - accuracy: 0.4779\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9712 - accuracy: 0.5067\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9375 - accuracy: 0.5180\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9288 - accuracy: 0.4646\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 1.0015 - accuracy: 0.4752\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9734 - accuracy: 0.5170\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9530 - accuracy: 0.5228\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9249 - accuracy: 0.4836\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9818 - accuracy: 0.4889\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9269 - accuracy: 0.5322\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9029 - accuracy: 0.5352\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8693 - accuracy: 0.5454\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9586 - accuracy: 0.4956\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9572 - accuracy: 0.5015\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9269 - accuracy: 0.5306\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9909 - accuracy: 0.3979\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9550 - accuracy: 0.5014\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9558 - accuracy: 0.4995\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9413 - accuracy: 0.5265\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9031 - accuracy: 0.5092\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9988 - accuracy: 0.4584\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9672 - accuracy: 0.4802\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9253 - accuracy: 0.5177\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8692 - accuracy: 0.5458\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9810 - accuracy: 0.4623\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9193 - accuracy: 0.5264\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9261 - accuracy: 0.5150\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8879 - accuracy: 0.5179\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9631 - accuracy: 0.4738\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9309 - accuracy: 0.5087\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9129 - accuracy: 0.5485\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8993 - accuracy: 0.5064\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9577 - accuracy: 0.5066\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9274 - accuracy: 0.5056\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9004 - accuracy: 0.5025\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9715 - accuracy: 0.4350\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9725 - accuracy: 0.4891\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9406 - accuracy: 0.5109\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9153 - accuracy: 0.5347\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9365 - accuracy: 0.4554\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9725 - accuracy: 0.4817\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9354 - accuracy: 0.4921\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8957 - accuracy: 0.5246\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9352 - accuracy: 0.4573\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9194 - accuracy: 0.5124\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9161 - accuracy: 0.4955\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8881 - accuracy: 0.5372\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9039 - accuracy: 0.4953\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9426 - accuracy: 0.4964\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9329 - accuracy: 0.5291\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9093 - accuracy: 0.5178\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9411 - accuracy: 0.4358\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9264 - accuracy: 0.5110\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9007 - accuracy: 0.5269\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8623 - accuracy: 0.5687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9264 - accuracy: 0.4652\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9572 - accuracy: 0.4897\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9251 - accuracy: 0.5230\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9027 - accuracy: 0.5239\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9021 - accuracy: 0.4937\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9669 - accuracy: 0.4572\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9187 - accuracy: 0.5125\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8968 - accuracy: 0.5428\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8559 - accuracy: 0.5410\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9718 - accuracy: 0.4767\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9527 - accuracy: 0.4943\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9210 - accuracy: 0.5140\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9006 - accuracy: 0.4894\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9948 - accuracy: 0.4721\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9816 - accuracy: 0.4759\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9560 - accuracy: 0.5067\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9206 - accuracy: 0.4789\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9317 - accuracy: 0.4812\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9059 - accuracy: 0.5188\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8776 - accuracy: 0.5515\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8755 - accuracy: 0.5064\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9345 - accuracy: 0.4749\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9126 - accuracy: 0.5291\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9026 - accuracy: 0.5181\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9207 - accuracy: 0.4659\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9320 - accuracy: 0.4881\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9066 - accuracy: 0.5108\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.8641 - accuracy: 0.5564\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9324 - accuracy: 0.4536\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9379 - accuracy: 0.5057\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9087 - accuracy: 0.5123\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8756 - accuracy: 0.5501\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9044 - accuracy: 0.4613\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9546 - accuracy: 0.4796\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8987 - accuracy: 0.5336\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8818 - accuracy: 0.5649\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8799 - accuracy: 0.5203\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9263 - accuracy: 0.5170\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9017 - accuracy: 0.5444\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8769 - accuracy: 0.5539\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8625 - accuracy: 0.5562\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9192 - accuracy: 0.5141\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9099 - accuracy: 0.5091\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8903 - accuracy: 0.5474\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8502 - accuracy: 0.5422\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9154 - accuracy: 0.4930\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9030 - accuracy: 0.5170\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8784 - accuracy: 0.5360\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8793 - accuracy: 0.5003\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9226 - accuracy: 0.4819\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8876 - accuracy: 0.5076\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8733 - accuracy: 0.5506\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9160 - accuracy: 0.4686\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.9565 - accuracy: 0.4593\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9316 - accuracy: 0.5164\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8814 - accuracy: 0.5472\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8837 - accuracy: 0.4877\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9132 - accuracy: 0.5241\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8948 - accuracy: 0.5294\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9174 - accuracy: 0.5231\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9333 - accuracy: 0.4530\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9515 - accuracy: 0.4855\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9318 - accuracy: 0.5045\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9133 - accuracy: 0.5395\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9813 - accuracy: 0.4043\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9380 - accuracy: 0.4961\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9017 - accuracy: 0.5039\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8873 - accuracy: 0.5280\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9115 - accuracy: 0.4720\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9470 - accuracy: 0.4856\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9068 - accuracy: 0.5254\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8569 - accuracy: 0.5552\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8580 - accuracy: 0.5371\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9260 - accuracy: 0.4981\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8901 - accuracy: 0.5192\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8799 - accuracy: 0.5250\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8614 - accuracy: 0.5211\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9368 - accuracy: 0.4809\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9153 - accuracy: 0.5229\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8782 - accuracy: 0.5363\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8721 - accuracy: 0.5488\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9223 - accuracy: 0.4812\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9221 - accuracy: 0.4881\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8721 - accuracy: 0.5336\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8910 - accuracy: 0.4710\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9071 - accuracy: 0.5095\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8820 - accuracy: 0.5549\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8609 - accuracy: 0.5549\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9205 - accuracy: 0.4360\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9352 - accuracy: 0.4818\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9031 - accuracy: 0.5152\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8893 - accuracy: 0.5319\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8803 - accuracy: 0.5032\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9060 - accuracy: 0.5072\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9020 - accuracy: 0.4907\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8739 - accuracy: 0.5350\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8239 - accuracy: 0.5785\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9270 - accuracy: 0.5197\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9160 - accuracy: 0.5128\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9017 - accuracy: 0.5197\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8486 - accuracy: 0.5528\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9178 - accuracy: 0.5327\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9151 - accuracy: 0.5102\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8938 - accuracy: 0.5072\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8740 - accuracy: 0.4971\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9130 - accuracy: 0.5230\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8815 - accuracy: 0.5339\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8668 - accuracy: 0.5399\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8706 - accuracy: 0.4847\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9286 - accuracy: 0.5005\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8954 - accuracy: 0.5354\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8704 - accuracy: 0.5606\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8797 - accuracy: 0.5060\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9010 - accuracy: 0.5115\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8892 - accuracy: 0.5202\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8597 - accuracy: 0.5422\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8764 - accuracy: 0.5052\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9440 - accuracy: 0.4921\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9236 - accuracy: 0.4892\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8903 - accuracy: 0.5148\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8685 - accuracy: 0.4976\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9370 - accuracy: 0.4783\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9239 - accuracy: 0.4844\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8625 - accuracy: 0.5328\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8977 - accuracy: 0.4572\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9471 - accuracy: 0.4639\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.9062 - accuracy: 0.5266\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8946 - accuracy: 0.5048\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9143 - accuracy: 0.4724\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8974 - accuracy: 0.5169\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8653 - accuracy: 0.5610\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8538 - accuracy: 0.5385\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9416 - accuracy: 0.4181\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9553 - accuracy: 0.4801\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9218 - accuracy: 0.5031\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8828 - accuracy: 0.5439\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8791 - accuracy: 0.4965\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9006 - accuracy: 0.5195\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8659 - accuracy: 0.5315\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8607 - accuracy: 0.5604\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8474 - accuracy: 0.5609\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.9099 - accuracy: 0.4869\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8668 - accuracy: 0.5452\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8641 - accuracy: 0.5569\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8715 - accuracy: 0.5313\n",
            "Initial population length: 100\n",
            "Initial fitness scores: max = 0.578499972820282, min = 0.39419999718666077\n",
            "Best fitness: 0.578499972820282\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9154 - accuracy: 0.5091\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8966 - accuracy: 0.5201\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8751 - accuracy: 0.5352\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8934 - accuracy: 0.4880\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9124 - accuracy: 0.5301\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8957 - accuracy: 0.5187\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8398 - accuracy: 0.5695\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8333 - accuracy: 0.5520\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9130 - accuracy: 0.5159\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9123 - accuracy: 0.5189\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8422 - accuracy: 0.5558\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8325 - accuracy: 0.5653\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9206 - accuracy: 0.4990\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8951 - accuracy: 0.5364\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8659 - accuracy: 0.5437\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8386 - accuracy: 0.5107\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9211 - accuracy: 0.4863\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8961 - accuracy: 0.5238\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8575 - accuracy: 0.5491\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8640 - accuracy: 0.5407\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8996 - accuracy: 0.5051\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8744 - accuracy: 0.5348\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8492 - accuracy: 0.5491\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8228 - accuracy: 0.5668\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8671 - accuracy: 0.5221\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8632 - accuracy: 0.5040\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8242 - accuracy: 0.5704\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8466 - accuracy: 0.5199\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8788 - accuracy: 0.5309\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8505 - accuracy: 0.5606\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8328 - accuracy: 0.5851\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8644 - accuracy: 0.5091\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8680 - accuracy: 0.5339\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8508 - accuracy: 0.5319\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8392 - accuracy: 0.5368\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8397 - accuracy: 0.5348\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8951 - accuracy: 0.5056\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8710 - accuracy: 0.5392\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8494 - accuracy: 0.5566\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8289 - accuracy: 0.5541\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8675 - accuracy: 0.5438\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8613 - accuracy: 0.5409\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8320 - accuracy: 0.5592\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8488 - accuracy: 0.5304\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8321 - accuracy: 0.5662\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8060 - accuracy: 0.5945\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7916 - accuracy: 0.6103\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8357 - accuracy: 0.5396\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8838 - accuracy: 0.5215\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8783 - accuracy: 0.5410\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8569 - accuracy: 0.5584\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8107 - accuracy: 0.5791\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8866 - accuracy: 0.5088\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8624 - accuracy: 0.5325\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8463 - accuracy: 0.5562\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8112 - accuracy: 0.5714\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9016 - accuracy: 0.5070\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8536 - accuracy: 0.5436\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8430 - accuracy: 0.5530\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8657 - accuracy: 0.5086\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8877 - accuracy: 0.4985\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8494 - accuracy: 0.5219\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8596 - accuracy: 0.5648\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9349 - accuracy: 0.4593\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8811 - accuracy: 0.5093\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8474 - accuracy: 0.5102\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8456 - accuracy: 0.5463\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8304 - accuracy: 0.5661\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9028 - accuracy: 0.5291\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8714 - accuracy: 0.5499\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8343 - accuracy: 0.5666\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9007 - accuracy: 0.4653\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8923 - accuracy: 0.5072\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8505 - accuracy: 0.5463\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8373 - accuracy: 0.5525\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8751 - accuracy: 0.5249\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8469 - accuracy: 0.5617\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8279 - accuracy: 0.5576\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8143 - accuracy: 0.5751\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8360 - accuracy: 0.5570\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.8848 - accuracy: 0.5519\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8657 - accuracy: 0.5509\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8629 - accuracy: 0.5694\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8255 - accuracy: 0.5612\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9245 - accuracy: 0.5120\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8827 - accuracy: 0.5320\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8551 - accuracy: 0.5510\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8770 - accuracy: 0.4996\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8828 - accuracy: 0.5079\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8559 - accuracy: 0.5475\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8250 - accuracy: 0.5614\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8757 - accuracy: 0.4817\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8458 - accuracy: 0.5478\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8550 - accuracy: 0.5536\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8404 - accuracy: 0.5682\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8809 - accuracy: 0.5070\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.9123 - accuracy: 0.5304\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8766 - accuracy: 0.5510\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8467 - accuracy: 0.5696\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9168 - accuracy: 0.5122\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9271 - accuracy: 0.5125\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9048 - accuracy: 0.5240\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8775 - accuracy: 0.5595\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8831 - accuracy: 0.4957\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8716 - accuracy: 0.5450\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8372 - accuracy: 0.5596\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8133 - accuracy: 0.5921\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8560 - accuracy: 0.5248\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.9004 - accuracy: 0.5047\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8765 - accuracy: 0.5208\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8446 - accuracy: 0.5623\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8685 - accuracy: 0.5149\n",
            "Best fitness after generation 0: 0.5791000127792358 took 36.66 seconds to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8582 - accuracy: 0.5370\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8316 - accuracy: 0.5670\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8277 - accuracy: 0.5720\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8971 - accuracy: 0.4757\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.8677 - accuracy: 0.5569\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8514 - accuracy: 0.5560\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8165 - accuracy: 0.5895\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8550 - accuracy: 0.5345\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8508 - accuracy: 0.5544\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8308 - accuracy: 0.5678\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8064 - accuracy: 0.5893\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8531 - accuracy: 0.5405\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.8481 - accuracy: 0.5568\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8164 - accuracy: 0.5985\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 18ms/step - loss: 0.7890 - accuracy: 0.6155\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8718 - accuracy: 0.5106\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8024 - accuracy: 0.5943\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7757 - accuracy: 0.6209\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7679 - accuracy: 0.5932\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8850 - accuracy: 0.5125\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8672 - accuracy: 0.5450\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8170 - accuracy: 0.5612\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7960 - accuracy: 0.5683\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8383 - accuracy: 0.5408\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8779 - accuracy: 0.5241\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8594 - accuracy: 0.5670\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8351 - accuracy: 0.5885\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8666 - accuracy: 0.5072\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8358 - accuracy: 0.5617\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.7925 - accuracy: 0.5724\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.7864 - accuracy: 0.5860\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8637 - accuracy: 0.5196\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8514 - accuracy: 0.5508\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8157 - accuracy: 0.5832\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8048 - accuracy: 0.6010\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8144 - accuracy: 0.5772\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8603 - accuracy: 0.5353\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8340 - accuracy: 0.5537\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8079 - accuracy: 0.5682\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8769 - accuracy: 0.5049\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7826 - accuracy: 0.6196\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7782 - accuracy: 0.6057\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7643 - accuracy: 0.6245\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8637 - accuracy: 0.5274\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.8260 - accuracy: 0.5829\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.7989 - accuracy: 0.6045\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.7602 - accuracy: 0.6177\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8793 - accuracy: 0.5156\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8650 - accuracy: 0.5286\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8315 - accuracy: 0.5592\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7967 - accuracy: 0.5929\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8249 - accuracy: 0.5664\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7514 - accuracy: 0.6411\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7342 - accuracy: 0.6441\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7066 - accuracy: 0.6726\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8607 - accuracy: 0.5327\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7715 - accuracy: 0.6037\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7866 - accuracy: 0.5820\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7699 - accuracy: 0.6006\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8632 - accuracy: 0.5251\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8205 - accuracy: 0.5678\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8034 - accuracy: 0.5882\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7593 - accuracy: 0.6330\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8322 - accuracy: 0.5551\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8430 - accuracy: 0.5525\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8154 - accuracy: 0.5652\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.7906 - accuracy: 0.6012\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8411 - accuracy: 0.5453\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8272 - accuracy: 0.5620\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7841 - accuracy: 0.5986\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7547 - accuracy: 0.6057\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8772 - accuracy: 0.5297\n",
            "Best fitness after generation 1: 0.5791000127792358 took 1.008 minutes to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8306 - accuracy: 0.5379\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8156 - accuracy: 0.5881\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7780 - accuracy: 0.6055\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8503 - accuracy: 0.5350\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7550 - accuracy: 0.6275\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7494 - accuracy: 0.6304\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7115 - accuracy: 0.6670\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8413 - accuracy: 0.5414\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7659 - accuracy: 0.6114\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7416 - accuracy: 0.6429\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7089 - accuracy: 0.6592\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8584 - accuracy: 0.5367\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7976 - accuracy: 0.5739\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7656 - accuracy: 0.5957\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7321 - accuracy: 0.6319\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8358 - accuracy: 0.5535\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7566 - accuracy: 0.6228\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7230 - accuracy: 0.6300\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7406 - accuracy: 0.6372\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8499 - accuracy: 0.5371\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.7999 - accuracy: 0.5894\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.7635 - accuracy: 0.6282\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.7601 - accuracy: 0.6074\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8572 - accuracy: 0.5317\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7349 - accuracy: 0.6232\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7168 - accuracy: 0.6283\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6893 - accuracy: 0.6497\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8587 - accuracy: 0.5305\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8222 - accuracy: 0.5661\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7725 - accuracy: 0.5994\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7598 - accuracy: 0.5984\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8169 - accuracy: 0.5651\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7980 - accuracy: 0.5933\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7928 - accuracy: 0.5944\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7579 - accuracy: 0.6170\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8476 - accuracy: 0.5406\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7515 - accuracy: 0.6254\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7279 - accuracy: 0.6175\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6899 - accuracy: 0.6519\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8609 - accuracy: 0.5331\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6836 - accuracy: 0.6556\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6779 - accuracy: 0.6636\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.6721 - accuracy: 0.6707\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8591 - accuracy: 0.5308\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6877 - accuracy: 0.6594\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6821 - accuracy: 0.6521\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.6494 - accuracy: 0.6792\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8415 - accuracy: 0.5487\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6412 - accuracy: 0.7109\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6410 - accuracy: 0.6886\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6369 - accuracy: 0.6971\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8155 - accuracy: 0.5830\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6785 - accuracy: 0.6799\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6664 - accuracy: 0.6750\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6382 - accuracy: 0.6899\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8609 - accuracy: 0.5316\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7866 - accuracy: 0.5978\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7517 - accuracy: 0.6166\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7199 - accuracy: 0.6324\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8301 - accuracy: 0.5465\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8812 - accuracy: 0.5278\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8359 - accuracy: 0.5561\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8528 - accuracy: 0.5672\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8450 - accuracy: 0.5373\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6658 - accuracy: 0.6854\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6697 - accuracy: 0.6814\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6483 - accuracy: 0.7035\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8390 - accuracy: 0.5514\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7808 - accuracy: 0.5881\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7434 - accuracy: 0.6304\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7344 - accuracy: 0.6324\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8864 - accuracy: 0.4932\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7006 - accuracy: 0.6750\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6867 - accuracy: 0.6700\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6574 - accuracy: 0.7011\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8886 - accuracy: 0.4964\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6566 - accuracy: 0.7114\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6621 - accuracy: 0.6912\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6162 - accuracy: 0.7154\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8778 - accuracy: 0.5282\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6961 - accuracy: 0.6626\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6675 - accuracy: 0.6890\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6502 - accuracy: 0.7053\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8969 - accuracy: 0.5074\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7497 - accuracy: 0.6094\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7509 - accuracy: 0.6333\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7082 - accuracy: 0.6552\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8312 - accuracy: 0.5598\n",
            "Best fitness after generation 2: 0.5830000042915344 took 1.4883333333333333 minutes to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5714 - accuracy: 0.7417\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5705 - accuracy: 0.7345\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5378 - accuracy: 0.7665\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8530 - accuracy: 0.5482\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7292 - accuracy: 0.6212\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6976 - accuracy: 0.6415\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6674 - accuracy: 0.6772\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8585 - accuracy: 0.5346\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6127 - accuracy: 0.7124\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.5732 - accuracy: 0.7342\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.5582 - accuracy: 0.7508\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8328 - accuracy: 0.5631\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7723 - accuracy: 0.6142\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7309 - accuracy: 0.6518\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7231 - accuracy: 0.6467\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8818 - accuracy: 0.5074\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7399 - accuracy: 0.6336\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7129 - accuracy: 0.6438\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.6622\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8890 - accuracy: 0.5085\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8254 - accuracy: 0.5762\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7643 - accuracy: 0.6182\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7177 - accuracy: 0.6318\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8293 - accuracy: 0.5542\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6681 - accuracy: 0.6647\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6504 - accuracy: 0.6864\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.6076 - accuracy: 0.7191\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8608 - accuracy: 0.5481\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6506 - accuracy: 0.6886\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6171 - accuracy: 0.6957\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5958 - accuracy: 0.7363\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8629 - accuracy: 0.5392\n",
            "Best fitness after generation 3: 0.5830000042915344 took 1.6686666666666667 minutes to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6746 - accuracy: 0.6741\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6391 - accuracy: 0.7056\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6230 - accuracy: 0.6995\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8984 - accuracy: 0.5145\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6105 - accuracy: 0.7155\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5906 - accuracy: 0.7310\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5806 - accuracy: 0.7373\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8564 - accuracy: 0.5511\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5511 - accuracy: 0.7508\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.5510 - accuracy: 0.7487\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5450 - accuracy: 0.7508\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8858 - accuracy: 0.5202\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.6524 - accuracy: 0.6995\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6231 - accuracy: 0.7130\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.6044 - accuracy: 0.7223\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8769 - accuracy: 0.5370\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5868 - accuracy: 0.7319\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5879 - accuracy: 0.7288\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5734 - accuracy: 0.7319\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8952 - accuracy: 0.5301\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5574 - accuracy: 0.7497\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5367 - accuracy: 0.7632\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5132 - accuracy: 0.7880\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8812 - accuracy: 0.5369\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5641 - accuracy: 0.7490\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5547 - accuracy: 0.7579\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5684 - accuracy: 0.7312\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9069 - accuracy: 0.5185\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5070 - accuracy: 0.7830\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5015 - accuracy: 0.7830\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4812 - accuracy: 0.7979\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8968 - accuracy: 0.5377\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4779 - accuracy: 0.8004\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4682 - accuracy: 0.8119\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4716 - accuracy: 0.8046\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8534 - accuracy: 0.5703\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5704 - accuracy: 0.7323\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5654 - accuracy: 0.7414\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.5486 - accuracy: 0.7566\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9270 - accuracy: 0.5136\n",
            "Best fitness after generation 4: 0.5830000042915344 took 1.8913333333333333 minutes to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4754 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4613 - accuracy: 0.7961\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4500 - accuracy: 0.8177\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9145 - accuracy: 0.5309\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5699 - accuracy: 0.7472\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5417 - accuracy: 0.7666\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5259 - accuracy: 0.7728\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9067 - accuracy: 0.5366\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.5023 - accuracy: 0.7738\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4823 - accuracy: 0.7896\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4728 - accuracy: 0.8201\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9880 - accuracy: 0.4822\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.5281 - accuracy: 0.7594\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4815 - accuracy: 0.7981\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4751 - accuracy: 0.7991\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8731 - accuracy: 0.5704\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4885 - accuracy: 0.7773\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4849 - accuracy: 0.7916\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.4686 - accuracy: 0.7906\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9429 - accuracy: 0.5229\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4750 - accuracy: 0.8080\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4657 - accuracy: 0.7905\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4619 - accuracy: 0.8050\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9197 - accuracy: 0.5346\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4578 - accuracy: 0.8175\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4769 - accuracy: 0.7904\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.4342 - accuracy: 0.8345\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9803 - accuracy: 0.5017\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4728 - accuracy: 0.8076\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4573 - accuracy: 0.8097\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4564 - accuracy: 0.8076\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8516 - accuracy: 0.5921\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4640 - accuracy: 0.7921\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4439 - accuracy: 0.8133\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4252 - accuracy: 0.8254\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9582 - accuracy: 0.5244\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4533 - accuracy: 0.8004\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.4462 - accuracy: 0.8150\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4366 - accuracy: 0.8140\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9281 - accuracy: 0.5443\n",
            "Best fitness after generation 5: 0.5921000242233276 took 2.1151666666666666 minutes to train.\n",
            "Best fitness after generation 6: 0.5921000242233276 took 2.1241666666666665 minutes to train.\n",
            "Best fitness after generation 7: 0.5921000242233276 took 2.132833333333333 minutes to train.\n",
            "Best fitness after generation 8: 0.5921000242233276 took 2.1416666666666666 minutes to train.\n",
            "Best fitness after generation 9: 0.5921000242233276 took 2.1505 minutes to train.\n",
            "Best fitness after generation 10: 0.5921000242233276 took 2.159 minutes to train.\n",
            "Best fitness after generation 11: 0.5921000242233276 took 2.167333333333333 minutes to train.\n",
            "Best fitness after generation 12: 0.5921000242233276 took 2.176 minutes to train.\n",
            "Best fitness after generation 13: 0.5921000242233276 took 2.1846666666666668 minutes to train.\n",
            "Best fitness after generation 14: 0.5921000242233276 took 2.1933333333333334 minutes to train.\n",
            "Best fitness after generation 15: 0.5921000242233276 took 2.202 minutes to train.\n",
            "Best fitness after generation 16: 0.5921000242233276 took 2.2105 minutes to train.\n",
            "Best fitness after generation 17: 0.5921000242233276 took 2.219 minutes to train.\n",
            "Best fitness after generation 18: 0.5921000242233276 took 2.2275 minutes to train.\n",
            "Best fitness after generation 19: 0.5921000242233276 took 2.2358333333333333 minutes to train.\n",
            "Best fitness after generation 20: 0.5921000242233276 took 2.244333333333333 minutes to train.\n",
            "Best fitness after generation 21: 0.5921000242233276 took 2.252666666666667 minutes to train.\n",
            "Best fitness after generation 22: 0.5921000242233276 took 2.261 minutes to train.\n",
            "Best fitness after generation 23: 0.5921000242233276 took 2.2695 minutes to train.\n",
            "Best fitness after generation 24: 0.5921000242233276 took 2.2781666666666665 minutes to train.\n",
            "Best fitness after generation 25: 0.5921000242233276 took 2.2868333333333335 minutes to train.\n",
            "Best fitness after generation 26: 0.5921000242233276 took 2.2953333333333332 minutes to train.\n",
            "Best fitness after generation 27: 0.5921000242233276 took 2.3036666666666665 minutes to train.\n",
            "Best fitness after generation 28: 0.5921000242233276 took 2.3121666666666667 minutes to train.\n",
            "Best fitness after generation 29: 0.5921000242233276 took 2.3208333333333333 minutes to train.\n",
            "Best fitness after generation 30: 0.5921000242233276 took 2.329333333333333 minutes to train.\n",
            "Best fitness after generation 31: 0.5921000242233276 took 2.338 minutes to train.\n",
            "Best fitness after generation 32: 0.5921000242233276 took 2.3465 minutes to train.\n",
            "Best fitness after generation 33: 0.5921000242233276 took 2.355166666666667 minutes to train.\n",
            "Best fitness after generation 34: 0.5921000242233276 took 2.3636666666666666 minutes to train.\n",
            "Best fitness after generation 35: 0.5921000242233276 took 2.372 minutes to train.\n",
            "Best fitness after generation 36: 0.5921000242233276 took 2.3805 minutes to train.\n",
            "Best fitness after generation 37: 0.5921000242233276 took 2.3891666666666667 minutes to train.\n",
            "Best fitness after generation 38: 0.5921000242233276 took 2.397666666666667 minutes to train.\n",
            "Best fitness after generation 39: 0.5921000242233276 took 2.4061666666666666 minutes to train.\n",
            "Best fitness after generation 40: 0.5921000242233276 took 2.4146666666666667 minutes to train.\n",
            "Best fitness after generation 41: 0.5921000242233276 took 2.4233333333333333 minutes to train.\n",
            "Best fitness after generation 42: 0.5921000242233276 took 2.431833333333333 minutes to train.\n",
            "Best fitness after generation 43: 0.5921000242233276 took 2.4403333333333332 minutes to train.\n",
            "Best fitness after generation 44: 0.5921000242233276 took 2.4488333333333334 minutes to train.\n",
            "Best fitness after generation 45: 0.5921000242233276 took 2.4571666666666667 minutes to train.\n",
            "Best fitness after generation 46: 0.5921000242233276 took 2.4656666666666665 minutes to train.\n",
            "Best fitness after generation 47: 0.5921000242233276 took 2.4741666666666666 minutes to train.\n",
            "Best fitness after generation 48: 0.5921000242233276 took 2.482666666666667 minutes to train.\n",
            "Best fitness after generation 49: 0.5921000242233276 took 2.491333333333333 minutes to train.\n",
            "Best fitness after generation 50: 0.5921000242233276 took 2.5 minutes to train.\n",
            "Best fitness after generation 51: 0.5921000242233276 took 2.5083333333333333 minutes to train.\n",
            "Best fitness after generation 52: 0.5921000242233276 took 2.5166666666666666 minutes to train.\n",
            "Best fitness after generation 53: 0.5921000242233276 took 2.5251666666666663 minutes to train.\n",
            "Best fitness after generation 54: 0.5921000242233276 took 2.533666666666667 minutes to train.\n",
            "Best fitness after generation 55: 0.5921000242233276 took 2.542333333333333 minutes to train.\n",
            "Best fitness after generation 56: 0.5921000242233276 took 2.551 minutes to train.\n",
            "Best fitness after generation 57: 0.5921000242233276 took 2.5595 minutes to train.\n",
            "Best fitness after generation 58: 0.5921000242233276 took 2.568166666666667 minutes to train.\n",
            "Best fitness after generation 59: 0.5921000242233276 took 2.5768333333333335 minutes to train.\n",
            "Best fitness after generation 60: 0.5921000242233276 took 2.5853333333333333 minutes to train.\n",
            "Best fitness after generation 61: 0.5921000242233276 took 2.5936666666666666 minutes to train.\n",
            "Best fitness after generation 62: 0.5921000242233276 took 2.6021666666666667 minutes to train.\n",
            "Best fitness after generation 63: 0.5921000242233276 took 2.6106666666666665 minutes to train.\n",
            "Best fitness after generation 64: 0.5921000242233276 took 2.619333333333333 minutes to train.\n",
            "Best fitness after generation 65: 0.5921000242233276 took 2.6278333333333332 minutes to train.\n",
            "Best fitness after generation 66: 0.5921000242233276 took 2.6363333333333334 minutes to train.\n",
            "Best fitness after generation 67: 0.5921000242233276 took 2.644833333333333 minutes to train.\n",
            "Best fitness after generation 68: 0.5921000242233276 took 2.6533333333333333 minutes to train.\n",
            "Best fitness after generation 69: 0.5921000242233276 took 2.662 minutes to train.\n",
            "Best fitness after generation 70: 0.5921000242233276 took 2.670666666666667 minutes to train.\n",
            "Best fitness after generation 71: 0.5921000242233276 took 2.6791666666666667 minutes to train.\n",
            "Best fitness after generation 72: 0.5921000242233276 took 2.6876666666666664 minutes to train.\n",
            "Best fitness after generation 73: 0.5921000242233276 took 2.6959999999999997 minutes to train.\n",
            "Best fitness after generation 74: 0.5921000242233276 took 2.7045000000000003 minutes to train.\n",
            "Best fitness after generation 75: 0.5921000242233276 took 2.7131666666666665 minutes to train.\n",
            "Best fitness after generation 76: 0.5921000242233276 took 2.7218333333333335 minutes to train.\n",
            "Best fitness after generation 77: 0.5921000242233276 took 2.7303333333333333 minutes to train.\n",
            "Best fitness after generation 78: 0.5921000242233276 took 2.7388333333333335 minutes to train.\n",
            "Best fitness after generation 79: 0.5921000242233276 took 2.7478333333333333 minutes to train.\n",
            "Best fitness after generation 80: 0.5921000242233276 took 2.756333333333333 minutes to train.\n",
            "Best fitness after generation 81: 0.5921000242233276 took 2.765 minutes to train.\n",
            "Best fitness after generation 82: 0.5921000242233276 took 2.7735 minutes to train.\n",
            "Best fitness after generation 83: 0.5921000242233276 took 2.781833333333333 minutes to train.\n",
            "Best fitness after generation 84: 0.5921000242233276 took 2.7905 minutes to train.\n",
            "Best fitness after generation 85: 0.5921000242233276 took 2.7991666666666664 minutes to train.\n",
            "Best fitness after generation 86: 0.5921000242233276 took 2.807666666666667 minutes to train.\n",
            "Best fitness after generation 87: 0.5921000242233276 took 2.8161666666666667 minutes to train.\n",
            "Best fitness after generation 88: 0.5921000242233276 took 2.8245 minutes to train.\n",
            "Best fitness after generation 89: 0.5921000242233276 took 2.8331666666666666 minutes to train.\n",
            "Best fitness after generation 90: 0.5921000242233276 took 2.841666666666667 minutes to train.\n",
            "Best fitness after generation 91: 0.5921000242233276 took 2.8503333333333334 minutes to train.\n",
            "Best fitness after generation 92: 0.5921000242233276 took 2.8588333333333336 minutes to train.\n",
            "Best fitness after generation 93: 0.5921000242233276 took 2.8680000000000003 minutes to train.\n",
            "Best fitness after generation 94: 0.5921000242233276 took 2.8771666666666667 minutes to train.\n",
            "Best fitness after generation 95: 0.5921000242233276 took 2.886 minutes to train.\n",
            "Best fitness after generation 96: 0.5921000242233276 took 2.8946666666666667 minutes to train.\n",
            "Best fitness after generation 97: 0.5921000242233276 took 2.9035 minutes to train.\n",
            "Best fitness after generation 98: 0.5921000242233276 took 2.9123333333333337 minutes to train.\n",
            "Best fitness after generation 99: 0.5921000242233276 took 2.921 minutes to train.\n",
            "Best accuracy found: 0.5921000242233276\n",
            "Length of pruned indexes: 59049\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4265 - accuracy: 0.8212\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.4180 - accuracy: 0.8412\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.3941 - accuracy: 0.8496\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8663 - accuracy: 0.5875\n",
            "Accuracy: 0.5874999761581421\n"
          ]
        }
      ],
      "source": [
        "pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bd4e4e19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# POPULATION_SIZE  = 10\n",
        "# SELECTION_SIZE   = 5\n",
        "# MUTATION_RATE    = 0.01\n",
        "# CROSSOVER_RATE   = 0.3\n",
        "# CROSSOVER_POINTS = 3\n",
        "# GENERATIONS      = 4\n",
        "# BATTLE_PARTICIPANTS = 2\n",
        "# ELITE_NUM = 1\n",
        "\n",
        "# pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "# loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "# print(f\"Accuracy on test set: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3eae6e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import time\n",
        "\n",
        "# # test a range of hyper parameters:\n",
        "# population_list = [50, 100, 200]\n",
        "# selection_list = [50, 100, 150]\n",
        "# mutation_list = [0.01, 0.05, 0.1]\n",
        "# crossover_rate_list = [0.3, 0.5, 0.7]\n",
        "# crossover_list = [3, 5, 10]\n",
        "# generation_list = [10, 50, 100, 200]\n",
        "# battle_participants_list = [2, 4, 10]\n",
        "# elitism_list = [0, 1, 2]\n",
        "\n",
        "# with open(\"results/combinations.csv\", \"w\") as f:\n",
        "#     f.write(\"POPULATION_SIZE,SELECTION_SIZE,MUTATION_RATE,CROSSOVER_RATE,CROSSOVER_POINTS,GENERATIONS,BATTLE_PARTICIPANTS,ELITE_NUM\\n\")\n",
        "\n",
        "# for POPULATION_SIZE in population_list:\n",
        "#     for SELECTION_SIZE in selection_list:\n",
        "#         for MUTATION_RATE in mutation_list:\n",
        "#             for CROSSOVER_RATE in crossover_rate_list:\n",
        "#                 for CROSSOVER_POINTS in crossover_list:\n",
        "#                     for GENERATIONS in generation_list:\n",
        "#                         for BATTLE_PARTICIPANTS in battle_participants_list:\n",
        "#                             for ELITE_NUM in elitism_list:\n",
        "#                                 print(f\"POPULATION_SIZE: {POPULATION_SIZE}, SELECTION_SIZE: {SELECTION_SIZE}, MUTATION_RATE: {MUTATION_RATE}, CROSSOVER_RATE: {CROSSOVER_RATE}, CROSSOVER_POINTS: {CROSSOVER_POINTS}, GENERATIONS: {GENERATIONS}, BATTLE_PARTICIPANTS: {BATTLE_PARTICIPANTS}, ELITE_NUM: {ELITE_NUM}\")\n",
        "#                                 print(f\"Finished at: {time.ctime().split()[3]}\")\n",
        "#                                 pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "#                                 loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "#                                 print(f\"Accuracy: {accuracy}\")\n",
        "#                                 with open(\"results/combinations.csv\", \"a\") as f:\n",
        "#                                     f.write(f\"{POPULATION_SIZE},{SELECTION_SIZE},{MUTATION_RATE},{CROSSOVER_RATE},{CROSSOVER_POINTS},{GENERATIONS},{BATTLE_PARTICIPANTS},{ELITE_NUM},{accuracy}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
