{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "anticipated-consumer",
      "metadata": {
        "id": "anticipated-consumer"
      },
      "source": [
        "In this assignment, we are going to implement see if we can optimally select a subset of training instances for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "higher-nebraska",
      "metadata": {
        "id": "higher-nebraska"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daily-internship",
      "metadata": {
        "id": "daily-internship"
      },
      "source": [
        "We are going to work with the MNIST dataset, a popular dataset for hand-written digit recognition. Here we load the datatset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "palestinian-texas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palestinian-texas",
        "outputId": "2e6494ee-e47d-4968-e8b5-01374df7d5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n",
            "Num of data points per class in train set: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1) # -1 means the last axis\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "\n",
        "#! scale down the training set to 10_000 samples\n",
        "import random\n",
        "random.seed(42)\n",
        "train_size = 10_200\n",
        "test_size = 200\n",
        "# make x_train have roughly same number of samples for each class:\n",
        "# x_train = np.concatenate([x_train[y_train == i][:train_size // 10] for i in range(10)])\n",
        "# y_train = np.concatenate([y_train[y_train == i][:train_size // 10] for i in range(10)])\n",
        "# # make x_test have roughly same number of samples for each class:\n",
        "# x_test = np.concatenate([x_test[y_test == i][:test_size // 10] for i in range(10)])\n",
        "# y_test = np.concatenate([y_test[y_test == i][:test_size // 10] for i in range(10)])\n",
        "\n",
        "# x_train = x_train[:train_size]\n",
        "# y_train = y_train[:train_size]\n",
        "# get test sets from x_train and y_train:\n",
        "# random_indices = np.random.choice(x_train.shape[0], size=test_size, replace=False)\n",
        "# x_test = x_train[random_indices]\n",
        "# y_test = y_train[random_indices]\n",
        "# delete the test sets from x_train and y_train:\n",
        "# x_train = np.delete(x_train, random_indices, axis=0)\n",
        "# y_train = np.delete(y_train, random_indices)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "print(f\"Num of data points per class in train set: {np.unique(y_train, return_counts=True)[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "95504f5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
          ]
        }
      ],
      "source": [
        "# count how many data points are in each class\n",
        "res = np.unique(y_train, return_counts=True)\n",
        "print({k:v for k,v in zip(res[0], res[1])})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-desert",
      "metadata": {
        "id": "empty-desert"
      },
      "source": [
        "Now corrupt the labels with common types of mistakes. The variable 'noise_probability' controls the amount of errors introduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "champion-technician",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "champion-technician",
        "outputId": "ab792401-d617-4afb-d634-5df238e0ee19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corruptions: {'5->6': 2666, '0->2': 2917, '4->7': 2888, '1->4': 3385, '9->0': 2997, '2->3': 2969, '3->5': 3027, '7->1': 3204, '8->9': 2911, '6->8': 2960}\n",
            "Number of corruptions: 29934\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "noise_probability = 0.5\n",
        "SEED = 314159\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def index(array, item):\n",
        "    for i in range(len(array)):\n",
        "        if item == array[i]:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def corrupt_label(y, y_index, err):\n",
        "    n = len(err)\n",
        "    # select an element at random (index != found)\n",
        "    if (y_index == n-1):\n",
        "        noisy_label = err[0]\n",
        "    else:\n",
        "        noisy_label = err[(y_index + 1)%n]\n",
        "    return noisy_label\n",
        "\n",
        "# We corrupt the MNIST data with some common mistakes, such as 3-->8, 8-->3, 1-->{4, 7}, 5-->6 etc.\n",
        "def corrupt_labels(y_train, noise_probability):\n",
        "    num_samples = y_train.shape[0]\n",
        "    err_es_1 = np.array([0, 2, 3, 5, 6, 8, 9])\n",
        "    err_es_2 = np.array([1, 4, 7])\n",
        "\n",
        "    corruptions = {}\n",
        "    corrupted_indexes = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability:\n",
        "            y = y_train[i]\n",
        "\n",
        "            y_index = index(err_es_1, y)\n",
        "            if y_index >= 0:\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_1)\n",
        "            else:\n",
        "                y_index = index(err_es_2, y)\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_2)\n",
        "\n",
        "            key = str(y_train[i]) + '->' + str(y_noisy)\n",
        "            corrupted_indexes[i] = i\n",
        "\n",
        "            if key in corruptions:\n",
        "                corruptions[key] += 1\n",
        "            else:\n",
        "                corruptions[key] = 0\n",
        "\n",
        "            y_train[i] = y_noisy\n",
        "\n",
        "    return corruptions, corrupted_indexes\n",
        "\n",
        "corruptions, corrupted_indexes = corrupt_labels(y_train, noise_probability)\n",
        "print (\"Corruptions: \" + str(corruptions))\n",
        "print (\"Number of corruptions: {}\".format(len(list(corrupted_indexes.keys()))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4ee48cb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 6003, 1: 6561, 2: 5906, 3: 6073, 4: 6339, 5: 5782, 6: 5624, 7: 5949, 8: 5900, 9: 5863}\n"
          ]
        }
      ],
      "source": [
        "# count how many data points are in each class after corruption\n",
        "res = np.unique(y_train, return_counts=True)\n",
        "print({k:v for k,v in zip(res[0], res[1])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "quality-gauge",
      "metadata": {
        "id": "quality-gauge"
      },
      "outputs": [],
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-celebrity",
      "metadata": {
        "id": "fifth-celebrity"
      },
      "source": [
        "Supervised (parametric) training with the (noisy) labeled examples. Note that this model is trained on the entire dataset (the value of the parameter pruned_indexes is null here, which means that we leave out no points), which is noisy (20% of the labels are corrupted). Now the question is: is this the best model that we can train or can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "extreme-ethernet",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extreme-ethernet",
        "outputId": "cb7c5e23-7242-4c71-c830-88dabc51ca3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                54090     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54410 (212.54 KB)\n",
            "Trainable params: 54410 (212.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 3\n",
        "validation_split=0.1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "def prune_points(x_train, y_train, pruned_indexes):\n",
        "    num_samples = x_train.shape[0]\n",
        "    x_train_pruned = []\n",
        "    y_train_pruned = []\n",
        "    for i in range(num_samples):\n",
        "        if not i in pruned_indexes:\n",
        "            x_train_pruned.append(x_train[i])\n",
        "            y_train_pruned.append(y_train[i])\n",
        "\n",
        "    return np.array(x_train_pruned), np.array(y_train_pruned)\n",
        "\n",
        "def trainAndEvaluateModel(x_train, y_train, x_test, y_test, model, pruned_indexes):\n",
        "\n",
        "    if not pruned_indexes == None:\n",
        "        x_train_pruned, y_train_pruned = prune_points(x_train, y_train, pruned_indexes)\n",
        "    else:\n",
        "        x_train_pruned = x_train\n",
        "        y_train_pruned = y_train\n",
        "\n",
        "    model.fit(x_train_pruned, y_train_pruned, batch_size=batch_size, epochs=epochs)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    keras.backend.clear_session() # remove previous training weights\n",
        "    \n",
        "    return loss, accuracy\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-waterproof",
      "metadata": {
        "id": "indie-waterproof"
      },
      "source": [
        "And we call the following function to train a model on the entire dataset and evaluate it on the test set. The accuracy on the test set is quite good, but can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "embedded-staff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embedded-staff",
        "outputId": "707551e0-dc21-4016-8ba9-3b9e05de1069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 8s 16ms/step - loss: 1.2006 - accuracy: 0.4448\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.9896 - accuracy: 0.4737\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.9462 - accuracy: 0.4854\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8635 - accuracy: 0.5187\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8635265827178955, 0.5187000036239624)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "structured-lithuania",
      "metadata": {
        "id": "structured-lithuania"
      },
      "source": [
        "You need to implement a subset selection function that when called will return a subset of instances which will be used to train the model. This setup ensures that you also pass in another dictionary which contains the indexes of the instances that you would not want to use while training the model, i.e., it should contain a list of indexes that you would decide to **leave out** for training.\n",
        "\n",
        "Here's the code and a sample implementation that returns a randomly chosen set of instances that you are to be left out. Since we chose 70% probability of label corruption (check the **noise_probability** parameter), we also select a subset where we leave out the same proportion of points. This is a baseline implementation and obviously you should aim to achieve better results than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "unique-operator",
      "metadata": {
        "id": "unique-operator"
      },
      "outputs": [],
      "source": [
        "# Here 'x_train', 'y_train' and model' are an unused parameters. But you may get better results by leveraging these.\n",
        "def baseLinePrunedSubsetMethod(x_train, y_train, model):\n",
        "    pruned_indexes = {}\n",
        "    num_samples = x_train.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability: # this is the global variable (only useful for this naive approach)\n",
        "            pruned_indexes[i] = i\n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-steel",
      "metadata": {
        "id": "stunning-steel"
      },
      "source": [
        "Let's see how this naive baseline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "formed-refrigerator",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "formed-refrigerator",
        "outputId": "b37fa32f-1af8-417c-8a33-1f53d42157ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.9215 - accuracy: 0.4976\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.9125 - accuracy: 0.4986\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.9000 - accuracy: 0.5088\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8529 - accuracy: 0.5106\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8529094457626343, 0.5105999708175659)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruned_indexes = baseLinePrunedSubsetMethod(x_train, y_train, model)\n",
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backed-cache",
      "metadata": {
        "id": "backed-cache"
      },
      "source": [
        "Let's now see if we had known what points were actually corrupted (more of a hypothetical unrealistic situation), does leaving out those points actually improve the model's effectiveness. It turns out that it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "amino-orientation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amino-orientation",
        "outputId": "ed71db95-e2da-4d84-ddb0-8b815f73d1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.2381 - accuracy: 0.9282\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.1430 - accuracy: 0.9580\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.1219 - accuracy: 0.9647\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0951 - accuracy: 0.9733\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.0950542464852333, 0.9732999801635742)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bright-constitutional",
      "metadata": {
        "id": "bright-constitutional"
      },
      "source": [
        "Your task is to implement your own version of (say of name **myPrunedSubsetMethod** (which should take as arguments x_train, y_train, and the model). The function should return a dictionary of indexes that are to be left out. Plug your function in and evaluate the results. Write a thorough report on the methodology and analyse the results.\n",
        "\n",
        "Some hints:\n",
        "You can approach this as a discrete state space optimisation problem, where firstly you can define a \"selection batch size\" (this is not the same as training batch size), which decides which batch of instances you're going to leave out. For instance, if you are in a state where the training set is $X$, you may select (by some heuristics) which points you're gonna leave out (let that set be $\\delta \\subset X$) so that a child state becomes $X' = X - \\delta$. Similarly, if you choose a different $\\delta$ you get a different child state. You then need to train and evaluate (call the function *trainAndEvaluateModel*) to see if that child state led to an improvement or not.\n",
        "\n",
        "You are free to use any algorithm, e.g., simulated annealing, A* search, genetic algorithm etc. to implement this discrete state space optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1269e4",
      "metadata": {},
      "source": [
        "# Using Genetic Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f7834ad5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "\n",
        "INPUT_SIZE = x_train.shape[0]\n",
        "\n",
        "#' Define GA parameters\n",
        "POPULATION_SIZE = 1 # number of individuals in population\n",
        "SELECTION_SIZE = ceil(0.5*POPULATION_SIZE) # number of individuals to select for next generation\n",
        "BATTLE_PARTICIPANTS = 4 # number of individuals to participate in a tournament\n",
        "MUTATION_RATE = 0.01 # probability of mutating each individual\n",
        "CROSSOVER_RATE = 0.3 # probability of crossing over two individuals\n",
        "CROSSOVER_POINTS = 2 # number of crossover points\n",
        "GENERATIONS = 1 # number of generations\n",
        "ELITE_NUM = 2 # number of elite individuals to keep from one generation to the next"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da72613c",
      "metadata": {},
      "source": [
        "## Initialise Population:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2d2d0871",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "430/430 [==============================] - 8s 19ms/step - loss: 0.7984 - accuracy: 0.5439\n",
            "Epoch 2/3\n",
            "430/430 [==============================] - 8s 18ms/step - loss: 0.7940 - accuracy: 0.5446\n",
            "Epoch 3/3\n",
            "430/430 [==============================] - 8s 18ms/step - loss: 0.7928 - accuracy: 0.5508\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8355 - accuracy: 0.4534\n",
            "bias: 500, accuracy: 0.45339998602867126\n"
          ]
        }
      ],
      "source": [
        "def create_population(population_size, input_size, y_train, bias_strength=0.5):\n",
        "    # 0 means prune the point, 1 means keep the point:\n",
        "    population = []\n",
        "    \n",
        "    POPULATION_SIZE = population_size\n",
        "    INPUT_SIZE = input_size\n",
        "    \n",
        "    #! random\n",
        "    # if len(population) < POPULATION_SIZE:\n",
        "        # print(f\"making population of random\")\n",
        "    # while len(population) < POPULATION_SIZE:\n",
        "    #     individual = np.random.choice([0, 1], size=INPUT_SIZE)\n",
        "    #     population.append(individual)\n",
        "\n",
        "    # Calculate inverse frequencies for prioritization\n",
        "    classes, freq = np.unique(y_train, return_counts=True)\n",
        "    class_frequencies = {k:v for k,v in zip(classes, freq)}\n",
        "    max_freq = max(class_frequencies.values())\n",
        "    prioritization_scores = {digit_class: max_freq / freq for digit_class, freq in class_frequencies.items()}\n",
        "\n",
        "    for _ in range(population_size):\n",
        "        individual = np.zeros(input_size, dtype=int)\n",
        "        for i in range(input_size):\n",
        "            # random.seed(SEED)\n",
        "            class_label = y_train[i]\n",
        "            # Bias towards selecting indices of less frequent classes\n",
        "            if random.random() < (prioritization_scores[class_label] * bias_strength / max_freq):\n",
        "                individual[i] = 1\n",
        "        population.append(individual)\n",
        "\n",
        "\n",
        "    # convert to tuple for hashability:\n",
        "    population = [tuple(individual) for individual in population]\n",
        "\n",
        "    return population\n",
        "\n",
        "# population = create_population(POPULATION_SIZE, INPUT_SIZE)\n",
        "# print(f\"Sample individual: {population[0]}\")\n",
        "# print(f\"We created {len(population)} individuals in the population, each with {len(population[0])} genes\")\n",
        "\n",
        "#! Example usage:\n",
        "population_size = 1\n",
        "accuracy = 0.0\n",
        "iterations = 0\n",
        "population = []\n",
        "# almost max is 6000\n",
        "bias = 500\n",
        "res = []\n",
        "\n",
        "\n",
        "population = create_population(population_size, INPUT_SIZE, y_train, bias_strength=bias)\n",
        "pruned_indexes = {i:i for i,elt in enumerate(population[0]) if elt == 1}\n",
        "loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "print(f\"bias: {bias}, accuracy: {accuracy}\")\n",
        "\n",
        "# while accuracy < 0.55:\n",
        "#     population = create_population(population_size, INPUT_SIZE, y_train, bias_strength=bias)\n",
        "#     pruned_indexes = {i:i for i,elt in enumerate(population[0]) if elt == 1}\n",
        "#     loss, new_accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "#     res.append((bias, new_accuracy))\n",
        "#     iterations += 1\n",
        "#     accuracy = new_accuracy\n",
        "#     print(f\"bias: {bias}, accuracy: {accuracy}\")\n",
        "#     bias += 0.1\n",
        "    \n",
        "    # print(f\"Iteration {iterations}: Accuracy: {new_accuracy}\")\n",
        "    # if new_accuracy >= 0.55:\n",
        "    #     population = [population[0]]\n",
        "    #     accuracy = new_accuracy\n",
        "    #     break\n",
        "    \n",
        "# print(f\"Took {iterations} iterations to get an accuracy of {accuracy}\")\n",
        "\n",
        "# counter = 0\n",
        "# for indv, loss, accuracy in res:\n",
        "#     print(f\"index {counter}: Accuracy: {accuracy}\")\n",
        "#     counter += 1\n",
        "    \n",
        "# with open('population.csv', 'w') as f:\n",
        "#     f.write('individual,loss,accuracy\\n')\n",
        "#     counter = 0\n",
        "#     for indv, loss, accuracy in res:\n",
        "#         f.write(f'{counter},{loss},{accuracy}\\n')\n",
        "\n",
        "#         counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f91c74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# sort by accuracy:\n",
        "res.sort(key=lambda x: x[2], reverse=True)\n",
        "print(f\"Best individual: {res[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999d7d90",
      "metadata": {},
      "source": [
        "## Run Genetic Algorithm:\n",
        "- Evaluate fitness of each individual\n",
        "- Select parents\n",
        "- Crossover\n",
        "- Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ba7899e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.8806 - accuracy: 0.5383\n",
            "Epoch 2/3\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.7810 - accuracy: 0.5651\n",
            "Epoch 3/3\n",
            "47/47 [==============================] - 1s 18ms/step - loss: 0.7551 - accuracy: 0.5875\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8270 - accuracy: 0.4714\n",
            "Epoch 1/3\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.8083 - accuracy: 0.5455\n",
            "Epoch 2/3\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.7758 - accuracy: 0.5763\n",
            "Epoch 3/3\n",
            "47/47 [==============================] - 1s 17ms/step - loss: 0.7594 - accuracy: 0.5899\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8101 - accuracy: 0.5073\n",
            "Best at generation 0: 0.4713999927043915\n",
            "Best solution so far: 0.51 at generation 0\n",
            "Epoch 1/3\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.7494 - accuracy: 0.5998\n",
            "Epoch 2/3\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.7386 - accuracy: 0.6102\n",
            "Epoch 3/3\n",
            "47/47 [==============================] - 1s 16ms/step - loss: 0.7157 - accuracy: 0.6226\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8157 - accuracy: 0.5050\n",
            "Best at generation 1: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 1\n",
            "Best at generation 2: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 2\n",
            "Best at generation 3: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 3\n",
            "Best at generation 4: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 4\n",
            "Best at generation 5: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 5\n",
            "Best at generation 6: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 6\n",
            "Best at generation 7: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 7\n",
            "Best at generation 8: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 8\n",
            "Best at generation 9: 0.5049999952316284\n",
            "Best solution so far: 0.51 at generation 9\n",
            "Finished in 9.82 seconds:\n",
            "\tpopulation_size: 2 \n",
            "\tselection_size: 1 \n",
            "\tmutation_rate: 0.05 \n",
            "\tcrossover_rate: 0.3 \n",
            "\tgenerations: 10 \n",
            "\telite_num: 1\n"
          ]
        }
      ],
      "source": [
        "from functools import lru_cache\n",
        "import time\n",
        "\n",
        "# cache the results of this function so that it doesn't have to be recalculated each time. Cache size is unlimited:\n",
        "@lru_cache(maxsize=None)\n",
        "def evaluate_fitness(population):\n",
        "    fitness_scores = []\n",
        "    good_enough = []\n",
        "    good_enough_fitness = 0.00\n",
        "    \n",
        "    for individual in population:\n",
        "        corrupted_indexes = {index: index for index, value in enumerate(individual) if value == 1}\n",
        "        accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)[1]\n",
        "\n",
        "        if accuracy > good_enough_fitness:\n",
        "            good_enough = individual\n",
        "            good_enough_fitness = round(accuracy, 2)\n",
        "            \n",
        "        if accuracy > 0.6:\n",
        "            print(\"Found a good enough solution!\")\n",
        "            break\n",
        "        \n",
        "        fitness_scores.append(accuracy)\n",
        "        \n",
        "    return fitness_scores, good_enough, good_enough_fitness\n",
        "\n",
        "def myPrunedSubsetMethod(x_train, y_train, model, population_size, selection_size, mutation_rate, crossover_rate, generations, elite_num, crossover_points, battle_participants, population):\n",
        "    #! global population, ELITE_NUM, SELECTION_SIZE\n",
        "    good_enough = []\n",
        "    good_enough_fitness = 0.00\n",
        "    \n",
        "    #! \n",
        "    POPULATION_SIZE = population_size\n",
        "    SELECTION_SIZE = selection_size\n",
        "    MUTATION_RATE = mutation_rate\n",
        "    CROSSOVER_RATE = crossover_rate\n",
        "    GENERATIONS = generations\n",
        "    ELITE_NUM = elite_num\n",
        "    CROSSOVER_POINTS = crossover_points\n",
        "    BATTLE_PARTICIPANTS = battle_participants\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Run GA\n",
        "    for generation in range(GENERATIONS):\n",
        "        random.seed(SEED) # reset the seed so that we get the same results each time\n",
        "        \n",
        "        #' Evaluate fitness\n",
        "        # print(\"Evaluating fitness...\")\n",
        "        fitness_scores, new_best_individual, new_best_fitness = evaluate_fitness(tuple(population))\n",
        "            \n",
        "        if new_best_fitness > good_enough_fitness:\n",
        "            good_enough = new_best_individual\n",
        "            good_enough_fitness = new_best_fitness\n",
        "        \n",
        "        if good_enough_fitness > 0.6:\n",
        "            print(\"Found a good enough solution!\")\n",
        "            break\n",
        "        \n",
        "        selected_population = []\n",
        "        \n",
        "        #' Elitism:\n",
        "        # print(\"Elitism...\")\n",
        "        # print(f\"conditions: {elite_num > 0} and {len(population)} > {elite_num}\")\n",
        "        elite_num = min(ELITE_NUM, POPULATION_SIZE) # make sure that the number of elite individuals is not greater than the population size\n",
        "        sorted_population = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)\n",
        "        if elite_num > 0 and len(population) >= elite_num:\n",
        "            selected_population += [individual[0] for individual in sorted_population[:elite_num]]\n",
        "            sorted_population = sorted_population[elite_num:]\n",
        "            \n",
        "            \n",
        "        #' Selection\n",
        "        # print(\"Selecting...\")\n",
        "        # print(f\"conditions: {SELECTION_SIZE} > {len(sorted_population)}\")\n",
        "        battle_participants = min(BATTLE_PARTICIPANTS, len(sorted_population)) # make sure that the number of battle participants is not greater than the population size\n",
        "        while len(selected_population) < SELECTION_SIZE:\n",
        "            to_battle = random.sample(sorted_population, battle_participants)\n",
        "            # select the fitter individual (who has the higher fitness score). x[1] is the fitness score, max()[0] to get individual array only (not fitness score):\n",
        "            selected_population.append(max(to_battle, key=lambda x: x[1])[0])\n",
        "            \n",
        "        # replace the old population with the new one for the next generation:\n",
        "        # print(f\"Len before: {len(population)}\")\n",
        "        if len(selected_population) > 0:\n",
        "            population = selected_population\n",
        "        # print(f\"Len after: {len(population)}\")\n",
        "            \n",
        "        #' Crossover and mutation\n",
        "        # print(\"Crossover and mutation...\")\n",
        "        # print(f\"Conditions: {len(population)} < {POPULATION_SIZE} and {len(population)} >= 2\")\n",
        "        while len(population) < POPULATION_SIZE and len(population) >= 2:\n",
        "            # print(\"Crossing over...\")\n",
        "            parent_1, parent_2 = random.sample(population, 2)\n",
        "            # by default, the child is a copy of the fittest parent:\n",
        "            # print(f\"1. {fitness_scores[population.index(parent_1)]}\")\n",
        "            # print(f\"2. {fitness_scores[population.index(parent_2)]}\")\n",
        "            child = parent_1 #if fitness_scores[population.index(parent_1)] > fitness_scores[population.index(parent_2)] else parent_2\n",
        "            # fitter = max(parent_1, parent_2, key=lambda x: fitness_scores[population.index(x)])\n",
        "            if random.random() < CROSSOVER_RATE:\n",
        "                # swap the genes from parents and append new child to selected population after mutating:\n",
        "                child = np.concatenate((parent_1[:CROSSOVER_POINTS], parent_2[CROSSOVER_POINTS:]))\n",
        "            \n",
        "            # print(\"Mutating...\")\n",
        "            for i, gene in enumerate(child):\n",
        "                if random.random() < MUTATION_RATE:\n",
        "                    # flip the gene:\n",
        "                    child = np.concatenate((child[:i], [1 if gene == 0 else 0], child[i+1:]))\n",
        "\n",
        "            population.append(tuple(child))\n",
        "        \n",
        "        \n",
        "        #' Report the progress\n",
        "        sorted_population = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)\n",
        "        print(f\"Best at generation {generation}: {sorted_population[0][1]}\")\n",
        "        print(f\"Best solution so far: {good_enough_fitness} at generation {generation}\")\n",
        "        with open('results.csv', 'a') as f:\n",
        "            f.write(f\"{SEED},{POPULATION_SIZE},{SELECTION_SIZE},{MUTATION_RATE},{CROSSOVER_RATE},{GENERATIONS},{ELITE_NUM},{sorted_population[0][1]},{round(time.time() - start_time, 2)}\\n\")\n",
        "        \n",
        "        \n",
        "\n",
        "    # best_solution = sorted(zip(population, fitness_scores), key=lambda x: x[1])[0][0]\n",
        "    \n",
        "    # write this result to CSV file with the hyperparameters:\n",
        "    with open('results.csv', 'a') as f:\n",
        "        f.write(f\"\\n\")\n",
        "        f.write(f\"{SEED},{POPULATION_SIZE},{SELECTION_SIZE},{MUTATION_RATE},{CROSSOVER_RATE},{GENERATIONS},{ELITE_NUM},{good_enough_fitness},{round(time.time() - start_time, 2)}\\n\")\n",
        "        \n",
        "    \n",
        "    # Return the indexes of the points to prune\n",
        "    return {index: index for index, value in enumerate(good_enough) if value == 0}\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# set up the CSV file:\n",
        "with open('results.csv', 'w') as f:\n",
        "    # reset it first:\n",
        "    f.write(\"\")\n",
        "    f.write(\"SEED,POPULATION_SIZE,SELECTION_SIZE,MUTATION_RATE,CROSSOVER_RATE,GENERATIONS,ELITE_NUM,ACCURACY,TIME_TAKEN\\n\")\n",
        "\n",
        "population_size = 2\n",
        "selection_size = ceil(0.5*population_size)\n",
        "mutation_rate = 0.05\n",
        "crossover_rate = 0.3\n",
        "generations = 10\n",
        "elite_num = 1\n",
        "crossover_points = int(0.5*INPUT_SIZE)\n",
        "battle_participants = 4\n",
        "# population = create_population(population_size, x_train.shape[0])\n",
        "# make population of 2 individuals, each mostly 1s with a random weight of 90% 1s and 10% 0s:\n",
        "population = [tuple(np.random.choice([0, 1], size=INPUT_SIZE, p=[0.1, 0.9])) for _ in range(population_size)]\n",
        "\n",
        "start_time = time.time()\n",
        "prune_indexes = myPrunedSubsetMethod(x_train, y_train, model, population_size, selection_size, mutation_rate, crossover_rate, generations, elite_num, crossover_points, battle_participants, population)\n",
        "time_elapsed = round(time.time() - start_time, 2)\n",
        "unit = \"seconds\"\n",
        "if time_elapsed > 60:\n",
        "    time_elapsed = time_elapsed/60 \n",
        "    unit = \"minutes\"\n",
        "if time_elapsed > 60:\n",
        "    time_elapsed = time_elapsed/60\n",
        "    unit = \"hours\"\n",
        "print(f\"Finished in {time_elapsed} {unit}:\")\n",
        "print(f\"\\tpopulation_size: {population_size} \\n\\tselection_size: {selection_size} \\n\\tmutation_rate: {mutation_rate} \\n\\tcrossover_rate: {crossover_rate} \\n\\tgenerations: {generations} \\n\\telite_num: {elite_num}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93fb9fb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # global POPULATION_SIZE, SELECTION_SIZE, MUTATION_RATE, CROSSOVER_RATE, GENERATIONS, ELITE_NUM\n",
        "# DEBUG = False\n",
        "# if DEBUG:\n",
        "#     population_size_list = [1]\n",
        "#     selection_size_list = [0.5]\n",
        "#     mutation_rate_list = [0.01]\n",
        "#     crossover_rate_list = [0.3]\n",
        "#     generations_list = [1]\n",
        "#     elite_num_list = [1]\n",
        "#     crossover_points_list = [2]\n",
        "#     battle_participants_list = [2]\n",
        "    \n",
        "# else:\n",
        "#     population_size_list = sorted([50, 100, 150, 200])\n",
        "#     selection_size_list = [0.3, 0.5, 0.7, 1] # list(map(lambda x: ceil(x*POPULATION_SIZE), [0.3, 0.5, 0.7, 1]))\n",
        "#     mutation_rate_list = [0.01, 0.05, 0.1, 0.3]\n",
        "#     crossover_rate_list = [0.1, 0.3, 0.5]\n",
        "#     generations_list = [10, 30, 100]\n",
        "#     elite_num_list = [2]\n",
        "#     crossover_points_list = [2, 4, 8, 10]\n",
        "#     battle_participants_list = [2, 4, 8, 10]\n",
        "\n",
        "# population = create_population(POPULATION_SIZE, INPUT_SIZE)\n",
        "\n",
        "# for population_size in population_size_list:\n",
        "#     # Clear the cache with each new population size:\n",
        "#     evaluate_fitness.cache_clear()\n",
        "    \n",
        "#     for selection_size in selection_size_list:\n",
        "#         selection_size = ceil(selection_size*population_size)\n",
        "#         for mutation_rate in mutation_rate_list:\n",
        "#             for crossover_rate in crossover_rate_list:\n",
        "#                 if crossover_rate > mutation_rate:\n",
        "#                     for generations in generations_list:\n",
        "#                         for elite_num in elite_num_list:\n",
        "#                             for crossover_points in crossover_points_list:\n",
        "#                                 for battle_participants in battle_participants_list:\n",
        "#                                     start_time = time.time()\n",
        "#                                     myPrunedSubsetMethod(x_train, y_train, model, population_size, selection_size, mutation_rate, crossover_rate, generations, elite_num, crossover_points, battle_participants, population)\n",
        "#                                     time_elapsed = round(time.time() - start_time, 2)\n",
        "#                                     unit = \"seconds\"\n",
        "#                                     if time_elapsed > 60:\n",
        "#                                         time_elapsed = time_elapsed/60\n",
        "#                                         unit = \"minutes\"\n",
        "#                                     if time_elapsed > 60:\n",
        "#                                         time_elapsed = time_elapsed/60\n",
        "#                                         unit = \"hours\"\n",
        "#                                     print(f\"Finished in {time_elapsed} {unit}:\")\n",
        "#                                     print(f\"\\tpopulation_size: {population_size} \\n\\tselection_size: {selection_size} \\n\\tmutation_rate: {mutation_rate} \\n\\tcrossover_rate: {crossover_rate} \\n\\tgenerations: {generations} \\n\\telite_num: {elite_num}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eae6e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
