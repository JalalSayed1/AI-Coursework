{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "anticipated-consumer",
      "metadata": {
        "id": "anticipated-consumer"
      },
      "source": [
        "In this assignment, we are going to implement see if we can optimally select a subset of training instances for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "higher-nebraska",
      "metadata": {
        "id": "higher-nebraska"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-06 18:15:13.507172: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-01-06 18:15:13.530424: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-01-06 18:15:13.646180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-06 18:15:13.646213: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-06 18:15:13.646884: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-06 18:15:13.708029: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-01-06 18:15:13.708905: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-06 18:15:14.478756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daily-internship",
      "metadata": {
        "id": "daily-internship"
      },
      "source": [
        "We are going to work with the MNIST dataset, a popular dataset for hand-written digit recognition. Here we load the datatset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "palestinian-texas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palestinian-texas",
        "outputId": "2e6494ee-e47d-4968-e8b5-01374df7d5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1) # -1 means the last axis\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "\n",
        "#! scale down the training set to 10_000 samples\n",
        "# import random\n",
        "# random.seed(42)\n",
        "# cut_factor = 3\n",
        "# x_train = x_train[:int(x_train.shape[0]/cut_factor)]\n",
        "# y_train = y_train[:int(y_train.shape[0]/cut_factor)]\n",
        "# x_test = x_test[:int(x_test.shape[0]/cut_factor)]\n",
        "# y_test = y_test[:int(y_test.shape[0]/cut_factor)]\n",
        "# print(\"x_train shape:\", x_train.shape)\n",
        "# print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "# print(\"Loaded {} test samples\".format(x_test.shape[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-desert",
      "metadata": {
        "id": "empty-desert"
      },
      "source": [
        "Now corrupt the labels with common types of mistakes. The variable 'noise_probability' controls the amount of errors introduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "champion-technician",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "champion-technician",
        "outputId": "ab792401-d617-4afb-d634-5df238e0ee19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corruptions: {'5->6': 2666, '0->2': 2917, '4->7': 2888, '1->4': 3385, '9->0': 2997, '2->3': 2969, '3->5': 3027, '7->1': 3204, '8->9': 2911, '6->8': 2960}\n",
            "Number of corruptions: 29934\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "noise_probability = 0.5\n",
        "SEED = 314159\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def index(array, item):\n",
        "    for i in range(len(array)):\n",
        "        if item == array[i]:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def corrupt_label(y, y_index, err):\n",
        "    n = len(err)\n",
        "    # select an element at random (index != found)\n",
        "    if (y_index == n-1):\n",
        "        noisy_label = err[0]\n",
        "    else:\n",
        "        noisy_label = err[(y_index + 1)%n]\n",
        "    return noisy_label\n",
        "\n",
        "# We corrupt the MNIST data with some common mistakes, such as 3-->8, 8-->3, 1-->{4, 7}, 5-->6 etc.\n",
        "def corrupt_labels(y_train, noise_probability):\n",
        "    num_samples = y_train.shape[0]\n",
        "    err_es_1 = np.array([0, 2, 3, 5, 6, 8, 9])\n",
        "    err_es_2 = np.array([1, 4, 7])\n",
        "\n",
        "    corruptions = {}\n",
        "    corrupted_indexes = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability:\n",
        "            y = y_train[i]\n",
        "\n",
        "            y_index = index(err_es_1, y)\n",
        "            if y_index >= 0:\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_1)\n",
        "            else:\n",
        "                y_index = index(err_es_2, y)\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_2)\n",
        "\n",
        "            key = str(y_train[i]) + '->' + str(y_noisy)\n",
        "            corrupted_indexes[i] = i\n",
        "\n",
        "            if key in corruptions:\n",
        "                corruptions[key] += 1\n",
        "            else:\n",
        "                corruptions[key] = 0\n",
        "\n",
        "            y_train[i] = y_noisy\n",
        "\n",
        "    return corruptions, corrupted_indexes\n",
        "\n",
        "corruptions, corrupted_indexes = corrupt_labels(y_train, noise_probability)\n",
        "print (\"Corruptions: \" + str(corruptions))\n",
        "print (\"Number of corruptions: {}\".format(len(list(corrupted_indexes.keys()))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "quality-gauge",
      "metadata": {
        "id": "quality-gauge"
      },
      "outputs": [],
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-celebrity",
      "metadata": {
        "id": "fifth-celebrity"
      },
      "source": [
        "Supervised (parametric) training with the (noisy) labeled examples. Note that this model is trained on the entire dataset (the value of the parameter pruned_indexes is null here, which means that we leave out no points), which is noisy (20% of the labels are corrupted). Now the question is: is this the best model that we can train or can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "extreme-ethernet",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extreme-ethernet",
        "outputId": "cb7c5e23-7242-4c71-c830-88dabc51ca3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                54090     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54410 (212.54 KB)\n",
            "Trainable params: 54410 (212.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 3\n",
        "validation_split=0.1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "def prune_points(x_train, y_train, pruned_indexes):\n",
        "    num_samples = x_train.shape[0]\n",
        "    x_train_pruned = []\n",
        "    y_train_pruned = []\n",
        "    for i in range(num_samples):\n",
        "        if not i in pruned_indexes:\n",
        "            x_train_pruned.append(x_train[i])\n",
        "            y_train_pruned.append(y_train[i])\n",
        "\n",
        "    return np.array(x_train_pruned), np.array(y_train_pruned)\n",
        "\n",
        "def trainAndEvaluateModel(x_train, y_train, x_test, y_test, model, pruned_indexes):\n",
        "\n",
        "    if not pruned_indexes == None:\n",
        "        x_train_pruned, y_train_pruned = prune_points(x_train, y_train, pruned_indexes)\n",
        "    else:\n",
        "        x_train_pruned = x_train\n",
        "        y_train_pruned = y_train\n",
        "\n",
        "    model.fit(x_train_pruned, y_train_pruned, batch_size=batch_size, epochs=epochs)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    keras.backend.clear_session() # remove previous training weights\n",
        "    \n",
        "    return loss, accuracy\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-waterproof",
      "metadata": {
        "id": "indie-waterproof"
      },
      "source": [
        "And we call the following function to train a model on the entire dataset and evaluate it on the test set. The accuracy on the test set is quite good, but can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "embedded-staff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embedded-staff",
        "outputId": "707551e0-dc21-4016-8ba9-3b9e05de1069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.1997 - accuracy: 0.4423\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.9864 - accuracy: 0.4759\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.9420 - accuracy: 0.4850\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 0.8952 - accuracy: 0.4449\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8952493071556091, 0.4449000060558319)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "structured-lithuania",
      "metadata": {
        "id": "structured-lithuania"
      },
      "source": [
        "You need to implement a subset selection function that when called will return a subset of instances which will be used to train the model. This setup ensures that you also pass in another dictionary which contains the indexes of the instances that you would not want to use while training the model, i.e., it should contain a list of indexes that you would decide to **leave out** for training.\n",
        "\n",
        "Here's the code and a sample implementation that returns a randomly chosen set of instances that you are to be left out. Since we chose 70% probability of label corruption (check the **noise_probability** parameter), we also select a subset where we leave out the same proportion of points. This is a baseline implementation and obviously you should aim to achieve better results than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "unique-operator",
      "metadata": {
        "id": "unique-operator"
      },
      "outputs": [],
      "source": [
        "# Here 'x_train', 'y_train' and model' are an unused parameters. But you may get better results by leveraging these.\n",
        "def baseLinePrunedSubsetMethod(x_train, y_train, model):\n",
        "    pruned_indexes = {}\n",
        "    num_samples = x_train.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability: # this is the global variable (only useful for this naive approach)\n",
        "            pruned_indexes[i] = i\n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-steel",
      "metadata": {
        "id": "stunning-steel"
      },
      "source": [
        "Let's see how this naive baseline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "formed-refrigerator",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "formed-refrigerator",
        "outputId": "b37fa32f-1af8-417c-8a33-1f53d42157ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "237/237 [==============================] - 3s 13ms/step - loss: 0.9221 - accuracy: 0.4942\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 3s 12ms/step - loss: 0.9064 - accuracy: 0.5010\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 3s 13ms/step - loss: 0.8990 - accuracy: 0.5062\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.8263 - accuracy: 0.5539\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.826309859752655, 0.5539000034332275)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruned_indexes = baseLinePrunedSubsetMethod(x_train, y_train, model)\n",
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backed-cache",
      "metadata": {
        "id": "backed-cache"
      },
      "source": [
        "Let's now see if we had known what points were actually corrupted (more of a hypothetical unrealistic situation), does leaving out those points actually improve the model's effectiveness. It turns out that it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "amino-orientation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amino-orientation",
        "outputId": "ed71db95-e2da-4d84-ddb0-8b815f73d1ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.2335 - accuracy: 0.9298\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.1396 - accuracy: 0.9591\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.1181 - accuracy: 0.9657\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0935 - accuracy: 0.9732\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.09345085918903351, 0.9732000231742859)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bright-constitutional",
      "metadata": {
        "id": "bright-constitutional"
      },
      "source": [
        "Your task is to implement your own version of (say of name **myPrunedSubsetMethod** (which should take as arguments x_train, y_train, and the model). The function should return a dictionary of indexes that are to be left out. Plug your function in and evaluate the results. Write a thorough report on the methodology and analyse the results.\n",
        "\n",
        "Some hints:\n",
        "You can approach this as a discrete state space optimisation problem, where firstly you can define a \"selection batch size\" (this is not the same as training batch size), which decides which batch of instances you're going to leave out. For instance, if you are in a state where the training set is $X$, you may select (by some heuristics) which points you're gonna leave out (let that set be $\\delta \\subset X$) so that a child state becomes $X' = X - \\delta$. Similarly, if you choose a different $\\delta$ you get a different child state. You then need to train and evaluate (call the function *trainAndEvaluateModel*) to see if that child state led to an improvement or not.\n",
        "\n",
        "You are free to use any algorithm, e.g., simulated annealing, A* search, genetic algorithm etc. to implement this discrete state space optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1269e4",
      "metadata": {},
      "source": [
        "# Using Genetic Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f7834ad5",
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_SIZE = x_train.shape[0]\n",
        "# OUTPUT_SIZE = INPUT_SIZE\n",
        "# Define GA parameters\n",
        "POPULATION_SIZE = 100 # number of individuals in population\n",
        "MUTATION_RATE = 0.01 # probability of mutating each individual\n",
        "CROSSOVER_RATE = 0.3 # probability of crossing over two individuals\n",
        "GENERATIONS = 1 # number of generations\n",
        "ELITE_NUM = 2 # number of elite individuals to keep from one generation to the next\n",
        "\n",
        "# different from the training batch size. This is the batch size used for selecting the number of points to prune.\n",
        "SELECTION_BATCH_SIZE = int(POPULATION_SIZE * 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da72613c",
      "metadata": {},
      "source": [
        "## Initialise Population:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2d2d0871",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "making population of all ones\n",
            "making population of half ones, half zeros\n",
            "making population of half zeros, half ones\n",
            "making population of 1/4 ones, 1/4 zeros, 1/4 ones, 1/4 zeros\n",
            "making population of 1/4 zeros, 1/4 ones, 1/4 zeros, 1/4 ones\n",
            "making population of random\n",
            "Sample individual: [1 1 1 ... 1 1 1]\n",
            "We created 100 individuals in the population, each with 60000 genes\n"
          ]
        }
      ],
      "source": [
        "# 0 means prune the point, 1 means keep the point:\n",
        "population = []\n",
        "\n",
        "# all ones:\n",
        "if len(population) < POPULATION_SIZE:\n",
        "    print(f\"making population of all ones\")\n",
        "    # need hashable type for set so use tuple:\n",
        "    population.append(np.ones(INPUT_SIZE, dtype=int))\n",
        "\n",
        "# half ones, half zeros\n",
        "half_size = int(INPUT_SIZE/2)\n",
        "if len(population) < POPULATION_SIZE:\n",
        "    print(f\"making population of half ones, half zeros\")\n",
        "    population.append(np.concatenate((np.ones(half_size, dtype=int), np.zeros(\n",
        "        half_size, dtype=int))))\n",
        "\n",
        "# half zeros, half ones\n",
        "if len(population) < POPULATION_SIZE:\n",
        "    print(f\"making population of half zeros, half ones\")\n",
        "    population.append(np.concatenate((np.zeros(half_size, dtype=int), np.ones(\n",
        "        half_size, dtype=int))))\n",
        "    \n",
        "# 1/4 ones, 1/4 zeros, 1/4 ones, 1/4 zeros:\n",
        "quarter_size = int(INPUT_SIZE/4)\n",
        "if len(population) < POPULATION_SIZE:\n",
        "    print(f\"making population of 1/4 ones, 1/4 zeros, 1/4 ones, 1/4 zeros\")\n",
        "    population.append(np.concatenate((np.ones(quarter_size, dtype=int), np.zeros(\n",
        "        quarter_size, dtype=int), np.ones(quarter_size, dtype=int), np.zeros(quarter_size, dtype=int))))\n",
        "\n",
        "# 1/4 zeros, 1/4 ones, 1/4 zeros, 1/4 ones:\n",
        "if len(population) < POPULATION_SIZE:\n",
        "    print(f\"making population of 1/4 zeros, 1/4 ones, 1/4 zeros, 1/4 ones\")\n",
        "    population.append(np.concatenate((np.zeros(quarter_size, dtype=int), np.ones(\n",
        "        quarter_size, dtype=int), np.zeros(quarter_size, dtype=int), np.ones(quarter_size, dtype=int))))\n",
        "\n",
        "# random\n",
        "if len(population) < POPULATION_SIZE:\n",
        "    print(f\"making population of random\")\n",
        "    \n",
        "while len(population) < POPULATION_SIZE:\n",
        "    individual = np.random.choice([0, 1], size=INPUT_SIZE)\n",
        "    population.append(individual)\n",
        "\n",
        "    \n",
        "print(f\"Sample individual: {population[0]}\")\n",
        "print(f\"We created {len(population)} individuals in the population, each with {len(population[0])} genes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999d7d90",
      "metadata": {},
      "source": [
        "## Run Genetic Algorithm:\n",
        "- Evaluate fitness of each individual\n",
        "- Select parents\n",
        "- Crossover\n",
        "- Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ba7899e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating fitness...\n",
            "Epoch 1/3\n",
            "469/469 [==============================] - 9s 19ms/step - loss: 0.7373 - accuracy: 0.5893\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.7346 - accuracy: 0.5909\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.7369 - accuracy: 0.5901\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8210 - accuracy: 0.4828\n",
            "Accuracy: 0.4828000068664551\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.7353 - accuracy: 0.5880\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7190 - accuracy: 0.6047\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.7234 - accuracy: 0.6002\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8236 - accuracy: 0.4813\n",
            "Accuracy: 0.4812999963760376\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7460 - accuracy: 0.5771\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7370 - accuracy: 0.5877\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7262 - accuracy: 0.5980\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8067 - accuracy: 0.5167\n",
            "Accuracy: 0.516700029373169\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7276 - accuracy: 0.5975\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.7270 - accuracy: 0.5956\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7201 - accuracy: 0.6047\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8078 - accuracy: 0.5072\n",
            "Accuracy: 0.5072000026702881\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.7445 - accuracy: 0.5825\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7376 - accuracy: 0.5891\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7314 - accuracy: 0.5897\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8265 - accuracy: 0.4785\n",
            "Accuracy: 0.47850000858306885\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7283 - accuracy: 0.5940\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7174 - accuracy: 0.6046\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7179 - accuracy: 0.6029\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8123 - accuracy: 0.4919\n",
            "Accuracy: 0.4918999969959259\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7334 - accuracy: 0.5891\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 16ms/step - loss: 0.7239 - accuracy: 0.5986\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 19ms/step - loss: 0.7171 - accuracy: 0.6028\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8163 - accuracy: 0.4841\n",
            "Accuracy: 0.48410001397132874\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7331 - accuracy: 0.5931\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7304 - accuracy: 0.5950\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7243 - accuracy: 0.6040\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8249 - accuracy: 0.4911\n",
            "Accuracy: 0.491100013256073\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7323 - accuracy: 0.5958\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7247 - accuracy: 0.5987\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7182 - accuracy: 0.6057\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8132 - accuracy: 0.5040\n",
            "Accuracy: 0.5040000081062317\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7299 - accuracy: 0.5945\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.7246 - accuracy: 0.5996\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.7201 - accuracy: 0.6034\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8218 - accuracy: 0.5023\n",
            "Accuracy: 0.5023000240325928\n",
            "Epoch 1/3\n",
            "232/232 [==============================] - 5s 21ms/step - loss: 0.7302 - accuracy: 0.5938\n",
            "Epoch 2/3\n",
            "232/232 [==============================] - 4s 19ms/step - loss: 0.7234 - accuracy: 0.6030\n",
            "Epoch 3/3\n",
            "232/232 [==============================] - 5s 20ms/step - loss: 0.7173 - accuracy: 0.6042\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8229 - accuracy: 0.4898\n",
            "Accuracy: 0.48980000615119934\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 7s 29ms/step - loss: 0.7313 - accuracy: 0.5912\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 8s 35ms/step - loss: 0.7235 - accuracy: 0.5999\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7187 - accuracy: 0.6063\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8308 - accuracy: 0.4857\n",
            "Accuracy: 0.48570001125335693\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7330 - accuracy: 0.5970\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.7249 - accuracy: 0.5995\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.7227 - accuracy: 0.6000\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8074 - accuracy: 0.5150\n",
            "Accuracy: 0.5149999856948853\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7359 - accuracy: 0.5905\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7256 - accuracy: 0.6007\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7217 - accuracy: 0.6049\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8211 - accuracy: 0.4920\n",
            "Accuracy: 0.492000013589859\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7314 - accuracy: 0.5944\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7228 - accuracy: 0.5997\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7185 - accuracy: 0.6036\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8173 - accuracy: 0.4994\n",
            "Accuracy: 0.49939998984336853\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 4s 16ms/step - loss: 0.7301 - accuracy: 0.5956\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7273 - accuracy: 0.5979\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 17ms/step - loss: 0.7166 - accuracy: 0.6027\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8327 - accuracy: 0.4805\n",
            "Accuracy: 0.4805000126361847\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7336 - accuracy: 0.5937\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7222 - accuracy: 0.6010\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7183 - accuracy: 0.6043\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8164 - accuracy: 0.5132\n",
            "Accuracy: 0.5131999850273132\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7314 - accuracy: 0.5935\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7230 - accuracy: 0.6020\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7193 - accuracy: 0.6035\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8249 - accuracy: 0.4872\n",
            "Accuracy: 0.487199991941452\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 3s 13ms/step - loss: 0.7312 - accuracy: 0.5946\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7258 - accuracy: 0.6004\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7193 - accuracy: 0.6042\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8130 - accuracy: 0.5153\n",
            "Accuracy: 0.5152999758720398\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.7282 - accuracy: 0.5932\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.7264 - accuracy: 0.5970\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7197 - accuracy: 0.6035\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8401 - accuracy: 0.4645\n",
            "Accuracy: 0.4645000100135803\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7332 - accuracy: 0.5941\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7289 - accuracy: 0.5958\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7163 - accuracy: 0.6029\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.8135 - accuracy: 0.5008\n",
            "Accuracy: 0.5008000135421753\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 3s 13ms/step - loss: 0.7310 - accuracy: 0.5937\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7202 - accuracy: 0.6024\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 3s 13ms/step - loss: 0.7118 - accuracy: 0.6069\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8208 - accuracy: 0.4932\n",
            "Accuracy: 0.49320000410079956\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7296 - accuracy: 0.5950\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7264 - accuracy: 0.6004\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.7203 - accuracy: 0.6032\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.8083 - accuracy: 0.5077\n",
            "Accuracy: 0.5077000260353088\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7338 - accuracy: 0.5915\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 3s 13ms/step - loss: 0.7229 - accuracy: 0.6001\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7167 - accuracy: 0.6059\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8147 - accuracy: 0.5070\n",
            "Accuracy: 0.5070000290870667\n",
            "Epoch 1/3\n",
            "237/237 [==============================] - 3s 13ms/step - loss: 0.7291 - accuracy: 0.5960\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 3s 14ms/step - loss: 0.7214 - accuracy: 0.6043\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 3s 13ms/step - loss: 0.7156 - accuracy: 0.6081\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8091 - accuracy: 0.5109\n",
            "Accuracy: 0.5109000205993652\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 15ms/step - loss: 0.7287 - accuracy: 0.5942\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 3s 14ms/step - loss: 0.7175 - accuracy: 0.6044\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 3s 14ms/step - loss: 0.7128 - accuracy: 0.6134\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8381 - accuracy: 0.4946\n",
            "Accuracy: 0.49459999799728394\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.7301 - accuracy: 0.5942\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7190 - accuracy: 0.6012\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7173 - accuracy: 0.6057\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8239 - accuracy: 0.4932\n",
            "Accuracy: 0.49320000410079956\n",
            "Epoch 1/3\n",
            "237/237 [==============================] - 4s 16ms/step - loss: 0.7280 - accuracy: 0.5935\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 4s 17ms/step - loss: 0.7206 - accuracy: 0.5999\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.7175 - accuracy: 0.6031\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8208 - accuracy: 0.5001\n",
            "Accuracy: 0.5001000165939331\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 19ms/step - loss: 0.7263 - accuracy: 0.5988\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 5s 22ms/step - loss: 0.7216 - accuracy: 0.6020\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7180 - accuracy: 0.6103\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8107 - accuracy: 0.5064\n",
            "Accuracy: 0.5063999891281128\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 5s 20ms/step - loss: 0.7269 - accuracy: 0.5944\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 6s 24ms/step - loss: 0.7223 - accuracy: 0.6020\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 6s 26ms/step - loss: 0.7158 - accuracy: 0.6106\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8170 - accuracy: 0.5092\n",
            "Accuracy: 0.5091999769210815\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.7312 - accuracy: 0.5984\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7268 - accuracy: 0.5987\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.7225 - accuracy: 0.6050\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8149 - accuracy: 0.5024\n",
            "Accuracy: 0.5023999810218811\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 5s 22ms/step - loss: 0.7299 - accuracy: 0.5960\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 19ms/step - loss: 0.7225 - accuracy: 0.5998\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 16ms/step - loss: 0.7213 - accuracy: 0.6041\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8392 - accuracy: 0.4743\n",
            "Accuracy: 0.47429999709129333\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7250 - accuracy: 0.5975\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7159 - accuracy: 0.6041\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7156 - accuracy: 0.6050\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8106 - accuracy: 0.5086\n",
            "Accuracy: 0.5085999965667725\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7279 - accuracy: 0.5937\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7210 - accuracy: 0.6026\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.7159 - accuracy: 0.6089\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8146 - accuracy: 0.4994\n",
            "Accuracy: 0.49939998984336853\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 5s 22ms/step - loss: 0.7250 - accuracy: 0.6002\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 19ms/step - loss: 0.7187 - accuracy: 0.6015\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 19ms/step - loss: 0.7109 - accuracy: 0.6090\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8155 - accuracy: 0.5075\n",
            "Accuracy: 0.5074999928474426\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 19ms/step - loss: 0.7296 - accuracy: 0.5954\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 5s 19ms/step - loss: 0.7284 - accuracy: 0.5962\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 5s 22ms/step - loss: 0.7215 - accuracy: 0.6054\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7927 - accuracy: 0.5362\n",
            "Accuracy: 0.5361999869346619\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7240 - accuracy: 0.5999\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7166 - accuracy: 0.6039\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7130 - accuracy: 0.6105\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8268 - accuracy: 0.4896\n",
            "Accuracy: 0.4896000027656555\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7306 - accuracy: 0.5929\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7218 - accuracy: 0.6023\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7154 - accuracy: 0.6075\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8248 - accuracy: 0.4947\n",
            "Accuracy: 0.49470001459121704\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7297 - accuracy: 0.5981\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7222 - accuracy: 0.6026\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7161 - accuracy: 0.6082\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8198 - accuracy: 0.4942\n",
            "Accuracy: 0.4941999912261963\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 19ms/step - loss: 0.7296 - accuracy: 0.5951\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7183 - accuracy: 0.6028\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7173 - accuracy: 0.6050\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8208 - accuracy: 0.5006\n",
            "Accuracy: 0.5005999803543091\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7220 - accuracy: 0.5999\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7181 - accuracy: 0.6037\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7117 - accuracy: 0.6107\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8056 - accuracy: 0.5293\n",
            "Accuracy: 0.5292999744415283\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7257 - accuracy: 0.5975\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7195 - accuracy: 0.6062\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7109 - accuracy: 0.6106\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8182 - accuracy: 0.5036\n",
            "Accuracy: 0.503600001335144\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7274 - accuracy: 0.5953\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7201 - accuracy: 0.6003\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7165 - accuracy: 0.6043\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8154 - accuracy: 0.4934\n",
            "Accuracy: 0.4934000074863434\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7299 - accuracy: 0.5977\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7171 - accuracy: 0.6045\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7143 - accuracy: 0.6065\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8212 - accuracy: 0.5022\n",
            "Accuracy: 0.5022000074386597\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7322 - accuracy: 0.5970\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7246 - accuracy: 0.6002\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7210 - accuracy: 0.6032\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8232 - accuracy: 0.5001\n",
            "Accuracy: 0.5001000165939331\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7271 - accuracy: 0.5955\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7193 - accuracy: 0.6028\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7133 - accuracy: 0.6057\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8302 - accuracy: 0.4795\n",
            "Accuracy: 0.4794999957084656\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 5s 21ms/step - loss: 0.7307 - accuracy: 0.5957\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 5s 21ms/step - loss: 0.7229 - accuracy: 0.6013\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 5s 21ms/step - loss: 0.7207 - accuracy: 0.6003\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8217 - accuracy: 0.4906\n",
            "Accuracy: 0.49059998989105225\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7297 - accuracy: 0.5944\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7203 - accuracy: 0.6007\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7155 - accuracy: 0.6054\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8323 - accuracy: 0.4790\n",
            "Accuracy: 0.4790000021457672\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7284 - accuracy: 0.5976\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7198 - accuracy: 0.6015\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7171 - accuracy: 0.6071\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8100 - accuracy: 0.5197\n",
            "Accuracy: 0.5196999907493591\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7275 - accuracy: 0.5948\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7164 - accuracy: 0.6025\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7158 - accuracy: 0.6093\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8244 - accuracy: 0.5025\n",
            "Accuracy: 0.5024999976158142\n",
            "Epoch 1/3\n",
            "237/237 [==============================] - 4s 17ms/step - loss: 0.7252 - accuracy: 0.6009\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 4s 17ms/step - loss: 0.7152 - accuracy: 0.6056\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 4s 17ms/step - loss: 0.7140 - accuracy: 0.6088\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8163 - accuracy: 0.4965\n",
            "Accuracy: 0.4964999854564667\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7263 - accuracy: 0.5998\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7170 - accuracy: 0.6048\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7168 - accuracy: 0.6055\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8245 - accuracy: 0.4883\n",
            "Accuracy: 0.48829999566078186\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 5s 20ms/step - loss: 0.7275 - accuracy: 0.5977\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7205 - accuracy: 0.6035\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 16ms/step - loss: 0.7113 - accuracy: 0.6115\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8344 - accuracy: 0.4786\n",
            "Accuracy: 0.47859999537467957\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7230 - accuracy: 0.5985\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 16ms/step - loss: 0.7193 - accuracy: 0.6035\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7078 - accuracy: 0.6135\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8294 - accuracy: 0.4808\n",
            "Accuracy: 0.48080000281333923\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 15ms/step - loss: 0.7269 - accuracy: 0.5965\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7228 - accuracy: 0.6003\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.7148 - accuracy: 0.6077\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8285 - accuracy: 0.4862\n",
            "Accuracy: 0.4862000048160553\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 15ms/step - loss: 0.7246 - accuracy: 0.5981\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 3s 14ms/step - loss: 0.7175 - accuracy: 0.6027\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 15ms/step - loss: 0.7148 - accuracy: 0.6080\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8130 - accuracy: 0.5047\n",
            "Accuracy: 0.5047000050544739\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.7279 - accuracy: 0.5936\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.7194 - accuracy: 0.6052\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.7217 - accuracy: 0.6020\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8247 - accuracy: 0.4916\n",
            "Accuracy: 0.49160000681877136\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 3s 12ms/step - loss: 0.7271 - accuracy: 0.5963\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 3s 13ms/step - loss: 0.7207 - accuracy: 0.6001\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 15ms/step - loss: 0.7133 - accuracy: 0.6070\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8273 - accuracy: 0.4933\n",
            "Accuracy: 0.4932999908924103\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 3s 15ms/step - loss: 0.7289 - accuracy: 0.5968\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7174 - accuracy: 0.6008\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7161 - accuracy: 0.6053\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8287 - accuracy: 0.4833\n",
            "Accuracy: 0.48330000042915344\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7235 - accuracy: 0.5960\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7217 - accuracy: 0.6023\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7098 - accuracy: 0.6125\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8370 - accuracy: 0.4752\n",
            "Accuracy: 0.47519999742507935\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7264 - accuracy: 0.5969\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 19ms/step - loss: 0.7206 - accuracy: 0.6018\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7115 - accuracy: 0.6087\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8125 - accuracy: 0.5122\n",
            "Accuracy: 0.5121999979019165\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7238 - accuracy: 0.5981\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7195 - accuracy: 0.6023\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7121 - accuracy: 0.6083\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8079 - accuracy: 0.5059\n",
            "Accuracy: 0.5059000253677368\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7239 - accuracy: 0.5976\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7179 - accuracy: 0.6044\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7163 - accuracy: 0.6058\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8118 - accuracy: 0.5161\n",
            "Accuracy: 0.5160999894142151\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7275 - accuracy: 0.5987\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7228 - accuracy: 0.5992\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7151 - accuracy: 0.6066\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8329 - accuracy: 0.4730\n",
            "Accuracy: 0.4729999899864197\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7245 - accuracy: 0.6006\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7183 - accuracy: 0.6022\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7112 - accuracy: 0.6046\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8219 - accuracy: 0.4872\n",
            "Accuracy: 0.487199991941452\n",
            "Epoch 1/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.7271 - accuracy: 0.5956\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 4s 17ms/step - loss: 0.7234 - accuracy: 0.6031\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.7127 - accuracy: 0.6085\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8308 - accuracy: 0.4856\n",
            "Accuracy: 0.48559999465942383\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7263 - accuracy: 0.5959\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7212 - accuracy: 0.6008\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7144 - accuracy: 0.6064\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8319 - accuracy: 0.4774\n",
            "Accuracy: 0.477400004863739\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7270 - accuracy: 0.5939\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7215 - accuracy: 0.6019\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7147 - accuracy: 0.6071\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8246 - accuracy: 0.4846\n",
            "Accuracy: 0.4846000075340271\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7226 - accuracy: 0.5993\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7171 - accuracy: 0.6034\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7138 - accuracy: 0.6082\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8226 - accuracy: 0.4895\n",
            "Accuracy: 0.4894999861717224\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7216 - accuracy: 0.5992\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 17ms/step - loss: 0.7173 - accuracy: 0.6060\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 17ms/step - loss: 0.7131 - accuracy: 0.6071\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8146 - accuracy: 0.4990\n",
            "Accuracy: 0.49900001287460327\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7196 - accuracy: 0.6037\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7128 - accuracy: 0.6076\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7111 - accuracy: 0.6102\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8207 - accuracy: 0.5166\n",
            "Accuracy: 0.5166000127792358\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7193 - accuracy: 0.6021\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7168 - accuracy: 0.6055\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 19ms/step - loss: 0.7095 - accuracy: 0.6071\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8315 - accuracy: 0.4834\n",
            "Accuracy: 0.48339998722076416\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7237 - accuracy: 0.5965\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7170 - accuracy: 0.6070\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7141 - accuracy: 0.6065\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8270 - accuracy: 0.4906\n",
            "Accuracy: 0.49059998989105225\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7276 - accuracy: 0.5954\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7218 - accuracy: 0.6027\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 17ms/step - loss: 0.7164 - accuracy: 0.6058\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8377 - accuracy: 0.4770\n",
            "Accuracy: 0.47699999809265137\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7243 - accuracy: 0.6020\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7175 - accuracy: 0.6053\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 17ms/step - loss: 0.7113 - accuracy: 0.6115\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8481 - accuracy: 0.4588\n",
            "Accuracy: 0.45879998803138733\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7217 - accuracy: 0.6002\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7158 - accuracy: 0.6029\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7104 - accuracy: 0.6121\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8105 - accuracy: 0.5156\n",
            "Accuracy: 0.5156000256538391\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7299 - accuracy: 0.5946\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7192 - accuracy: 0.6037\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 17ms/step - loss: 0.7188 - accuracy: 0.6033\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8031 - accuracy: 0.5260\n",
            "Accuracy: 0.5260000228881836\n",
            "Epoch 1/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.7262 - accuracy: 0.5960\n",
            "Epoch 2/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.7178 - accuracy: 0.6030\n",
            "Epoch 3/3\n",
            "237/237 [==============================] - 4s 18ms/step - loss: 0.7109 - accuracy: 0.6110\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8188 - accuracy: 0.5079\n",
            "Accuracy: 0.5078999996185303\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7245 - accuracy: 0.5980\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7156 - accuracy: 0.6033\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7117 - accuracy: 0.6084\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8135 - accuracy: 0.5191\n",
            "Accuracy: 0.51910001039505\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7250 - accuracy: 0.6006\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7165 - accuracy: 0.6055\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7146 - accuracy: 0.6069\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8301 - accuracy: 0.4979\n",
            "Accuracy: 0.49790000915527344\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 5s 19ms/step - loss: 0.7211 - accuracy: 0.6009\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 19ms/step - loss: 0.7163 - accuracy: 0.6024\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 5s 19ms/step - loss: 0.7126 - accuracy: 0.6103\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8137 - accuracy: 0.5175\n",
            "Accuracy: 0.5174999833106995\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 17ms/step - loss: 0.7260 - accuracy: 0.5990\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 17ms/step - loss: 0.7140 - accuracy: 0.6080\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 17ms/step - loss: 0.7165 - accuracy: 0.6067\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8349 - accuracy: 0.4953\n",
            "Accuracy: 0.4952999949455261\n",
            "Epoch 1/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7242 - accuracy: 0.5992\n",
            "Epoch 2/3\n",
            "236/236 [==============================] - 4s 17ms/step - loss: 0.7156 - accuracy: 0.6076\n",
            "Epoch 3/3\n",
            "236/236 [==============================] - 4s 18ms/step - loss: 0.7110 - accuracy: 0.6093\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8224 - accuracy: 0.5036\n",
            "Accuracy: 0.503600001335144\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 17ms/step - loss: 0.7215 - accuracy: 0.6004\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7195 - accuracy: 0.6030\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7148 - accuracy: 0.6064\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8278 - accuracy: 0.4929\n",
            "Accuracy: 0.492900013923645\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7244 - accuracy: 0.5978\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 19ms/step - loss: 0.7123 - accuracy: 0.6073\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7113 - accuracy: 0.6075\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8171 - accuracy: 0.4968\n",
            "Accuracy: 0.4968000054359436\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7245 - accuracy: 0.5977\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7141 - accuracy: 0.6084\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7091 - accuracy: 0.6090\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8376 - accuracy: 0.4828\n",
            "Accuracy: 0.4828000068664551\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 19ms/step - loss: 0.7263 - accuracy: 0.5913\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7148 - accuracy: 0.6084\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7084 - accuracy: 0.6135\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8229 - accuracy: 0.4986\n",
            "Accuracy: 0.4986000061035156\n",
            "Epoch 1/3\n",
            "233/233 [==============================] - 4s 17ms/step - loss: 0.7197 - accuracy: 0.6028\n",
            "Epoch 2/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7132 - accuracy: 0.6090\n",
            "Epoch 3/3\n",
            "233/233 [==============================] - 4s 18ms/step - loss: 0.7130 - accuracy: 0.6093\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8274 - accuracy: 0.4876\n",
            "Accuracy: 0.4875999987125397\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7239 - accuracy: 0.5981\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7161 - accuracy: 0.6090\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7111 - accuracy: 0.6091\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8173 - accuracy: 0.5025\n",
            "Accuracy: 0.5024999976158142\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.7168 - accuracy: 0.6047\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.7158 - accuracy: 0.6061\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.7105 - accuracy: 0.6101\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8375 - accuracy: 0.4673\n",
            "Accuracy: 0.4672999978065491\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7195 - accuracy: 0.6026\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.7128 - accuracy: 0.6102\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7104 - accuracy: 0.6096\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8125 - accuracy: 0.5249\n",
            "Accuracy: 0.5249000191688538\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7241 - accuracy: 0.5987\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7136 - accuracy: 0.6113\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7099 - accuracy: 0.6121\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8440 - accuracy: 0.4631\n",
            "Accuracy: 0.46309998631477356\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7248 - accuracy: 0.5989\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7171 - accuracy: 0.6027\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 21ms/step - loss: 0.7116 - accuracy: 0.6068\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8160 - accuracy: 0.5031\n",
            "Accuracy: 0.5030999779701233\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7191 - accuracy: 0.6040\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 0.7169 - accuracy: 0.6063\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 3s 14ms/step - loss: 0.7110 - accuracy: 0.6103\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8358 - accuracy: 0.4838\n",
            "Accuracy: 0.4837999939918518\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 22ms/step - loss: 0.7228 - accuracy: 0.6006\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7182 - accuracy: 0.6044\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 4s 19ms/step - loss: 0.7135 - accuracy: 0.6063\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8469 - accuracy: 0.4658\n",
            "Accuracy: 0.4657999873161316\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7237 - accuracy: 0.6010\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7128 - accuracy: 0.6049\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7073 - accuracy: 0.6113\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8360 - accuracy: 0.4678\n",
            "Accuracy: 0.46779999136924744\n",
            "Epoch 1/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7226 - accuracy: 0.6008\n",
            "Epoch 2/3\n",
            "234/234 [==============================] - 4s 18ms/step - loss: 0.7120 - accuracy: 0.6065\n",
            "Epoch 3/3\n",
            "234/234 [==============================] - 5s 20ms/step - loss: 0.7078 - accuracy: 0.6101\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8105 - accuracy: 0.5133\n",
            "Accuracy: 0.5133000016212463\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 18ms/step - loss: 0.7185 - accuracy: 0.6015\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 19ms/step - loss: 0.7188 - accuracy: 0.6032\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7095 - accuracy: 0.6073\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8311 - accuracy: 0.4935\n",
            "Accuracy: 0.4934999942779541\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 4s 19ms/step - loss: 0.7207 - accuracy: 0.5989\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7146 - accuracy: 0.6067\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7069 - accuracy: 0.6111\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8230 - accuracy: 0.5100\n",
            "Accuracy: 0.5099999904632568\n",
            "Epoch 1/3\n",
            "235/235 [==============================] - 5s 20ms/step - loss: 0.7237 - accuracy: 0.6008\n",
            "Epoch 2/3\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.7150 - accuracy: 0.6072\n",
            "Epoch 3/3\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.7134 - accuracy: 0.6087\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.8383 - accuracy: 0.4744\n",
            "Accuracy: 0.47440001368522644\n",
            "Selecting...\n",
            "Crossing over...\n",
            "Mutating...\n",
            "Estimated time remaining: 0.0 seconds.\n",
            "Best solution so far: 0.5361999869346619 at generation 0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Return the indexes of the points to prune\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {index: index \u001b[38;5;28;01mfor\u001b[39;00m index, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(good_enough) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m}\n\u001b[0;32m---> 98\u001b[0m pruned_indexes \u001b[38;5;241m=\u001b[39m \u001b[43mmyPrunedSubsetMethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum of points to prune: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pruned_indexes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[29], line 93\u001b[0m, in \u001b[0;36mmyPrunedSubsetMethod\u001b[0;34m(x_train, y_train, model)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest solution so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgood_enough_fitness\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m best_solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(population, fitness_scores), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest solution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfitness_scores[\u001b[43mpopulation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_solution\u001b[49m\u001b[43m)\u001b[49m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at last generation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Return the indexes of the points to prune\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {index: index \u001b[38;5;28;01mfor\u001b[39;00m index, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(good_enough) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m}\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def evaluate_individual(individual):\n",
        "    corrupted_indexes = {index: index for index, value in enumerate(individual) if value == 0}\n",
        "    return trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)[1] # return the accuracy only\n",
        "\n",
        "def myPrunedSubsetMethod(x_train, y_train, model):\n",
        "    start_time = time.time()\n",
        "    global population\n",
        "    good_enough = []\n",
        "    good_enough_fitness = 0\n",
        "    # Run GA\n",
        "    for generation in range(GENERATIONS):\n",
        "        random.seed(SEED) # reset the seed so that we get the same results each time\n",
        "        \n",
        "        #' Evaluate fitness\n",
        "        # fitness is the accuracy of the model:\n",
        "        print(\"Evaluating fitness...\")\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            corrupted_indexes = {index: index for index, value in enumerate(individual) if value == 0}\n",
        "            accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)[1]\n",
        "            fitness_scores.append(accuracy)\n",
        "            print(f\"Accuracy: {accuracy}\")\n",
        "            if accuracy > good_enough_fitness:\n",
        "                good_enough_fitness = accuracy\n",
        "                good_enough = individual\n",
        "                if accuracy > 0.6:\n",
        "                    print(\"Found a good enough solution!\")\n",
        "                    good_enough = population[fitness_scores.index(max(fitness_scores))]\n",
        "                    return {index: index for index, value in enumerate(good_enough) if value == 0}\n",
        "        \n",
        "        \n",
        "        #' Selection\n",
        "        print(\"Selecting...\")\n",
        "        selected_population = []\n",
        "        # Elitism:\n",
        "        sorted_population = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)\n",
        "        for i in range(ELITE_NUM):\n",
        "            selected_population.append(sorted_population[i][0])\n",
        "\n",
        "        sorted_population = sorted_population[ELITE_NUM:]\n",
        "            \n",
        "            \n",
        "        while len(selected_population) < SELECTION_BATCH_SIZE:\n",
        "            # select two individuals at random\n",
        "            individual_index1 , individual_index2 = random.randint(0, len(sorted_population)-1), random.randint(0, len(sorted_population)-1)\n",
        "            # select the fitter individual (who has the higher fitness score):\n",
        "            selected_population.append(sorted_population[individual_index1][0] if fitness_scores[individual_index1] > fitness_scores[individual_index2] else sorted_population[individual_index2][0])\n",
        "        \n",
        "        \n",
        "        if generation < GENERATIONS - 1:\n",
        "        \n",
        "            #' Crossover\n",
        "            print(\"Crossing over...\")\n",
        "            for i, individual in enumerate(selected_population):\n",
        "                if random.random() < CROSSOVER_RATE:\n",
        "                    # select another individual at random:\n",
        "                    another_individual_index = random.randint(0, len(selected_population)-1)\n",
        "                    # select a random crossover point:\n",
        "                    crossover_point = random.randint(0, INPUT_SIZE-1)\n",
        "                    # swap the genes after the crossover point:\n",
        "                    selected_population[i] = np.concatenate((individual[:crossover_point], selected_population[another_individual_index][crossover_point:]))\n",
        "\n",
        "            \n",
        "            #' Mutation\n",
        "            print(\"Mutating...\")\n",
        "            for i, individual in enumerate(selected_population):\n",
        "                for j, gene in enumerate(individual):\n",
        "                    if random.random() < MUTATION_RATE:\n",
        "                        # flip the gene:\n",
        "                        selected_population[i][j] = 0 if gene == 1 else 1\n",
        "                        \n",
        "        # replace the old population with the new one for the next generation:\n",
        "        population = selected_population\n",
        "        \n",
        "        \n",
        "        #' Report the progress\n",
        "        estimated_time_remaining = (time.time() - start_time) * (GENERATIONS - generation - 1) # in seconds\n",
        "        units = \"seconds\"\n",
        "        if estimated_time_remaining > 60:\n",
        "            estimated_time_remaining = estimated_time_remaining / 60 # in minutes\n",
        "            units = \"minutes\" if estimated_time_remaining > 1 else \"minute\"\n",
        "        if estimated_time_remaining > 60:\n",
        "            estimated_time_remaining = estimated_time_remaining / 60 # in hours\n",
        "            units = \"hours\" if estimated_time_remaining > 1 else \"hour\"\n",
        "        if estimated_time_remaining > 24:\n",
        "            estimated_time_remaining = estimated_time_remaining / 24 # in days\n",
        "            units = \"days\" if estimated_time_remaining > 1 else \"day\"\n",
        "        print(f\"Estimated time remaining: {round(estimated_time_remaining, 2)} {units}.\")\n",
        "        print(f\"Best solution so far: {good_enough_fitness} at generation {generation}\")\n",
        "        \n",
        "    \n",
        "    best_solution = sorted(zip(population, fitness_scores), key=lambda x: x[1])[0][0]\n",
        "    \n",
        "    # Return the indexes of the points to prune\n",
        "    return {index: index for index, value in enumerate(good_enough) if value == 0}\n",
        "\n",
        "pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "print(f\"Num of points to prune: {len(pruned_indexes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6bed9a7",
      "metadata": {},
      "source": [
        "19 mins for 100 population, 1 generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eae6e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
