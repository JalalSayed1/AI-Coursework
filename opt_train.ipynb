{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "anticipated-consumer",
      "metadata": {
        "id": "anticipated-consumer"
      },
      "source": [
        "In this assignment, we are going to implement see if we can optimally select a subset of training instances for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "higher-nebraska",
      "metadata": {
        "id": "higher-nebraska"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daily-internship",
      "metadata": {
        "id": "daily-internship"
      },
      "source": [
        "We are going to work with the MNIST dataset, a popular dataset for hand-written digit recognition. Here we load the datatset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "palestinian-texas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palestinian-texas",
        "outputId": "2e6494ee-e47d-4968-e8b5-01374df7d5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n",
            "Num of data points per class in train set: [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1) # -1 means the last axis\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "\n",
        "#! scale down the training set to 10_000 samples\n",
        "import random\n",
        "random.seed(42)\n",
        "train_size = 10_200\n",
        "test_size = 200\n",
        "# make x_train have roughly same number of samples for each class:\n",
        "# x_train = np.concatenate([x_train[y_train == i][:train_size // 10] for i in range(10)])\n",
        "# y_train = np.concatenate([y_train[y_train == i][:train_size // 10] for i in range(10)])\n",
        "# # make x_test have roughly same number of samples for each class:\n",
        "# x_test = np.concatenate([x_test[y_test == i][:test_size // 10] for i in range(10)])\n",
        "# y_test = np.concatenate([y_test[y_test == i][:test_size // 10] for i in range(10)])\n",
        "\n",
        "# x_train = x_train[:train_size]\n",
        "# y_train = y_train[:train_size]\n",
        "# get test sets from x_train and y_train:\n",
        "# random_indices = np.random.choice(x_train.shape[0], size=test_size, replace=False)\n",
        "# x_test = x_train[random_indices]\n",
        "# y_test = y_train[random_indices]\n",
        "# delete the test sets from x_train and y_train:\n",
        "# x_train = np.delete(x_train, random_indices, axis=0)\n",
        "# y_train = np.delete(y_train, random_indices)\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))\n",
        "print(f\"Num of data points per class in train set: {np.unique(y_train, return_counts=True)[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "95504f5b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
          ]
        }
      ],
      "source": [
        "# count how many data points are in each class\n",
        "res = np.unique(y_train, return_counts=True)\n",
        "print({k:v for k,v in zip(res[0], res[1])})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-desert",
      "metadata": {
        "id": "empty-desert"
      },
      "source": [
        "Now corrupt the labels with common types of mistakes. The variable 'noise_probability' controls the amount of errors introduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "champion-technician",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "champion-technician",
        "outputId": "ab792401-d617-4afb-d634-5df238e0ee19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corruptions: {'5->6': 2666, '0->2': 2917, '4->7': 2888, '1->4': 3385, '9->0': 2997, '2->3': 2969, '3->5': 3027, '7->1': 3204, '8->9': 2911, '6->8': 2960}\n",
            "Number of corruptions: 29934\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "noise_probability = 0.5\n",
        "SEED = 314159\n",
        "\n",
        "random.seed(SEED)\n",
        "\n",
        "def index(array, item):\n",
        "    for i in range(len(array)):\n",
        "        if item == array[i]:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "def corrupt_label(y, y_index, err):\n",
        "    n = len(err)\n",
        "    # select an element at random (index != found)\n",
        "    if (y_index == n-1):\n",
        "        noisy_label = err[0]\n",
        "    else:\n",
        "        noisy_label = err[(y_index + 1)%n]\n",
        "    return noisy_label\n",
        "\n",
        "# We corrupt the MNIST data with some common mistakes, such as 3-->8, 8-->3, 1-->{4, 7}, 5-->6 etc.\n",
        "def corrupt_labels(y_train, noise_probability):\n",
        "    num_samples = y_train.shape[0]\n",
        "    err_es_1 = np.array([0, 2, 3, 5, 6, 8, 9])\n",
        "    err_es_2 = np.array([1, 4, 7])\n",
        "\n",
        "    corruptions = {}\n",
        "    corrupted_indexes = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability:\n",
        "            y = y_train[i]\n",
        "\n",
        "            y_index = index(err_es_1, y)\n",
        "            if y_index >= 0:\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_1)\n",
        "            else:\n",
        "                y_index = index(err_es_2, y)\n",
        "                y_noisy = corrupt_label(y, y_index, err_es_2)\n",
        "\n",
        "            key = str(y_train[i]) + '->' + str(y_noisy)\n",
        "            corrupted_indexes[i] = i\n",
        "\n",
        "            if key in corruptions:\n",
        "                corruptions[key] += 1\n",
        "            else:\n",
        "                corruptions[key] = 0\n",
        "\n",
        "            y_train[i] = y_noisy\n",
        "\n",
        "    return corruptions, corrupted_indexes\n",
        "\n",
        "corruptions, corrupted_indexes = corrupt_labels(y_train, noise_probability)\n",
        "print (\"Corruptions: \" + str(corruptions))\n",
        "print (\"Number of corruptions: {}\".format(len(list(corrupted_indexes.keys()))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3fd83ff4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of pruned indexes: 29934\n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of pruned indexes: {len(corrupted_indexes)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ee48cb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 6003, 1: 6561, 2: 5906, 3: 6073, 4: 6339, 5: 5782, 6: 5624, 7: 5949, 8: 5900, 9: 5863}\n"
          ]
        }
      ],
      "source": [
        "# count how many data points are in each class after corruption\n",
        "res = np.unique(y_train, return_counts=True)\n",
        "print({k:v for k,v in zip(res[0], res[1])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "quality-gauge",
      "metadata": {
        "id": "quality-gauge"
      },
      "outputs": [],
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-celebrity",
      "metadata": {
        "id": "fifth-celebrity"
      },
      "source": [
        "Supervised (parametric) training with the (noisy) labeled examples. Note that this model is trained on the entire dataset (the value of the parameter pruned_indexes is null here, which means that we leave out no points), which is noisy (20% of the labels are corrupted). Now the question is: is this the best model that we can train or can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "extreme-ethernet",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extreme-ethernet",
        "outputId": "cb7c5e23-7242-4c71-c830-88dabc51ca3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                54090     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54410 (212.54 KB)\n",
            "Trainable params: 54410 (212.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 3\n",
        "validation_split=0.1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "def prune_points(x_train, y_train, pruned_indexes):\n",
        "    num_samples = x_train.shape[0]\n",
        "    x_train_pruned = []\n",
        "    y_train_pruned = []\n",
        "    for i in range(num_samples):\n",
        "        if not i in pruned_indexes:\n",
        "            x_train_pruned.append(x_train[i])\n",
        "            y_train_pruned.append(y_train[i])\n",
        "\n",
        "    return np.array(x_train_pruned), np.array(y_train_pruned)\n",
        "\n",
        "def trainAndEvaluateModel(x_train, y_train, x_test, y_test, model, pruned_indexes):\n",
        "\n",
        "    if not pruned_indexes == None:\n",
        "        x_train_pruned, y_train_pruned = prune_points(x_train, y_train, pruned_indexes)\n",
        "    else:\n",
        "        x_train_pruned = x_train\n",
        "        y_train_pruned = y_train\n",
        "\n",
        "    model.fit(x_train_pruned, y_train_pruned, batch_size=batch_size, epochs=epochs)\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    keras.backend.clear_session() # remove previous training weights\n",
        "    \n",
        "    return loss, accuracy\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-waterproof",
      "metadata": {
        "id": "indie-waterproof"
      },
      "source": [
        "And we call the following function to train a model on the entire dataset and evaluate it on the test set. The accuracy on the test set is quite good, but can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "embedded-staff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embedded-staff",
        "outputId": "707551e0-dc21-4016-8ba9-3b9e05de1069"
      },
      "outputs": [],
      "source": [
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "structured-lithuania",
      "metadata": {
        "id": "structured-lithuania"
      },
      "source": [
        "You need to implement a subset selection function that when called will return a subset of instances which will be used to train the model. This setup ensures that you also pass in another dictionary which contains the indexes of the instances that you would not want to use while training the model, i.e., it should contain a list of indexes that you would decide to **leave out** for training.\n",
        "\n",
        "Here's the code and a sample implementation that returns a randomly chosen set of instances that you are to be left out. Since we chose 70% probability of label corruption (check the **noise_probability** parameter), we also select a subset where we leave out the same proportion of points. This is a baseline implementation and obviously you should aim to achieve better results than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "unique-operator",
      "metadata": {
        "id": "unique-operator"
      },
      "outputs": [],
      "source": [
        "# Here 'x_train', 'y_train' and model' are an unused parameters. But you may get better results by leveraging these.\n",
        "def baseLinePrunedSubsetMethod(x_train, y_train, model):\n",
        "    pruned_indexes = {}\n",
        "    num_samples = x_train.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability: # this is the global variable (only useful for this naive approach)\n",
        "            pruned_indexes[i] = i\n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-steel",
      "metadata": {
        "id": "stunning-steel"
      },
      "source": [
        "Let's see how this naive baseline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "formed-refrigerator",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "formed-refrigerator",
        "outputId": "b37fa32f-1af8-417c-8a33-1f53d42157ba"
      },
      "outputs": [],
      "source": [
        "pruned_indexes = baseLinePrunedSubsetMethod(x_train, y_train, model)\n",
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backed-cache",
      "metadata": {
        "id": "backed-cache"
      },
      "source": [
        "Let's now see if we had known what points were actually corrupted (more of a hypothetical unrealistic situation), does leaving out those points actually improve the model's effectiveness. It turns out that it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "amino-orientation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amino-orientation",
        "outputId": "ed71db95-e2da-4d84-ddb0-8b815f73d1ed"
      },
      "outputs": [],
      "source": [
        "# trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bright-constitutional",
      "metadata": {
        "id": "bright-constitutional"
      },
      "source": [
        "Your task is to implement your own version of (say of name **myPrunedSubsetMethod** (which should take as arguments x_train, y_train, and the model). The function should return a dictionary of indexes that are to be left out. Plug your function in and evaluate the results. Write a thorough report on the methodology and analyse the results.\n",
        "\n",
        "Some hints:\n",
        "You can approach this as a discrete state space optimisation problem, where firstly you can define a \"selection batch size\" (this is not the same as training batch size), which decides which batch of instances you're going to leave out. For instance, if you are in a state where the training set is $X$, you may select (by some heuristics) which points you're gonna leave out (let that set be $\\delta \\subset X$) so that a child state becomes $X' = X - \\delta$. Similarly, if you choose a different $\\delta$ you get a different child state. You then need to train and evaluate (call the function *trainAndEvaluateModel*) to see if that child state led to an improvement or not.\n",
        "\n",
        "You are free to use any algorithm, e.g., simulated annealing, A* search, genetic algorithm etc. to implement this discrete state space optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1269e4",
      "metadata": {},
      "source": [
        "# Using Genetic Algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7834ad5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "\n",
        "INPUT_SIZE = x_train.shape[0]\n",
        "\n",
        "#' Define GA parameters\n",
        "POPULATION_SIZE = 100 # number of individuals in population\n",
        "SELECTION_SIZE = ceil(POPULATION_SIZE*0.5) # number of individuals to select for next generation\n",
        "MUTATION_RATE = 0.01 # probability of mutating each individual\n",
        "CROSSOVER_RATE = 0.3 # probability of crossing over two individuals\n",
        "CROSSOVER_POINTS = 3 # number of crossover points\n",
        "GENERATIONS = 100 # number of generations\n",
        "BATTLE_PARTICIPANTS = 4 # number of individuals to participate in a tournament\n",
        "ELITE_NUM = 1 # number of elite individuals to keep from one generation to the next\n",
        "BIAS_STRENGTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da72613c",
      "metadata": {},
      "source": [
        "## Initialise Population:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2d2d0871",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_population(population_size, input_size, y_train, bias_strength=100):\n",
        "    # 0 means prune the point, 1 means keep the point:\n",
        "    population = []\n",
        "    \n",
        "    POPULATION_SIZE = population_size\n",
        "    INPUT_SIZE = input_size\n",
        "\n",
        "    # Calculate inverse frequencies for prioritization\n",
        "    classes, freq = np.unique(y_train, return_counts=True)\n",
        "    class_frequencies = {k:v for k,v in zip(classes, freq)}\n",
        "    max_freq = max(class_frequencies.values())\n",
        "    prioritization_scores = {digit_class: round(max_freq / freq, 2) for digit_class, freq in class_frequencies.items()}\n",
        "    print(f\"Prioritization scores: {prioritization_scores}\")\n",
        "    for _ in range(POPULATION_SIZE):\n",
        "        individual = np.ones(INPUT_SIZE, dtype=int)\n",
        "        for i in range(INPUT_SIZE):\n",
        "            class_label = y_train[i]\n",
        "            # Bias towards selecting indices of more frequent classes\n",
        "            if random.random() < ((prioritization_scores[class_label] / max_freq) * bias_strength):\n",
        "                individual[i] = 0\n",
        "        \n",
        "        # check if individual is not in population already:\n",
        "        # bc of this, accuracy is low for the first few individuals then it gets better.\n",
        "        if not np.any([np.array_equal(individual, indv) for indv in population]):\n",
        "            population.append(individual)\n",
        "            \n",
        "    # convert to tuple for hashability:\n",
        "    population = [tuple(individual) for individual in population]\n",
        "\n",
        "    return population\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999d7d90",
      "metadata": {},
      "source": [
        "## Run Genetic Algorithm:\n",
        "- Evaluate fitness of each individual\n",
        "- Select parents\n",
        "- Crossover\n",
        "- Mutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ccb61cac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "import time\n",
        "import os\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def evaluate_fitness(model, individual):\n",
        "    '''Because hashing x_train is too slow, x_train and model need to be in the global scope for this to work.'''\n",
        "    pruned_indexes = {i:i for i,elt in enumerate(individual) if elt == 1}\n",
        "    if len(pruned_indexes) == INPUT_SIZE:\n",
        "        return 0.0\n",
        "    elif len(pruned_indexes) == 0:\n",
        "        loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)\n",
        "    else:\n",
        "        loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "    return accuracy\n",
        "\n",
        "def find_top_N_elements(population, fitness_scores, N):\n",
        "    top_N = []\n",
        "    for _ in range(N):\n",
        "        max_index = np.argmax(fitness_scores)\n",
        "        top_N.append(population[max_index])\n",
        "        fitness_scores[max_index] = -1\n",
        "    return top_N\n",
        "\n",
        "def select_individuals(population, fitness_scores, selection_size, battle_participants, elite_num):\n",
        "\n",
        "    selection_size = min(selection_size, len(population))\n",
        "    \n",
        "    # Select individuals for next generation\n",
        "    selected_individuals = []\n",
        "    \n",
        "    #' ELITISM:\n",
        "    selected_individuals.extend(find_top_N_elements(population, fitness_scores, elite_num))\n",
        "    \n",
        "    #' TOURNAMENT SELECTION:\n",
        "    while len(selected_individuals) < selection_size:\n",
        "        # Select BATTLE_PARTICIPANTS individuals at random\n",
        "        participants = random.sample(list(zip(population, fitness_scores)), battle_participants)\n",
        "        # Sort participants by fitness score\n",
        "        sorted_participants = sorted(participants, key=lambda x: x[1], reverse=True)\n",
        "        # Select the best individual from the tournament\n",
        "        selected_individuals.append(sorted_participants[0][0])\n",
        "    return selected_individuals\n",
        "\n",
        "def crossover(individual_1, individual_2, crossover_rate, crossover_points):\n",
        "    # Crossover individuals\n",
        "    if random.random() < crossover_rate:\n",
        "        crossover_points = random.sample(range(1, len(individual_1)), crossover_points)\n",
        "        crossover_points.sort()\n",
        "        new_individual_1 = individual_1[:crossover_points[0]] + individual_2[crossover_points[0]:crossover_points[1]] + individual_1[crossover_points[1]:]\n",
        "        new_individual_2 = individual_2[:crossover_points[0]] + individual_1[crossover_points[0]:crossover_points[1]] + individual_2[crossover_points[1]:]\n",
        "        return new_individual_1, new_individual_2\n",
        "    else:\n",
        "        return individual_1, individual_2\n",
        "    \n",
        "def mutate(individual, mutation_rate):\n",
        "    # Mutate individual\n",
        "    new_individual = []\n",
        "    for gene in individual:\n",
        "        if random.random() < mutation_rate:\n",
        "            new_individual.append(1 - gene) # flip the gene\n",
        "        else:\n",
        "            new_individual.append(gene)\n",
        "    return new_individual\n",
        "\n",
        "def create_next_generation(population, fitness_scores, selection_size, mutation_rate, crossover_rate, crossover_points, battle_participants, elite_num):\n",
        "    # Create next generation\n",
        "    next_generation = []\n",
        "    \n",
        "    # Select individuals for next generation\n",
        "    selected_individuals = select_individuals(population, fitness_scores, selection_size, battle_participants, elite_num)\n",
        "\n",
        "    #' Crossover individuals:\n",
        "    while len(next_generation) < len(population):\n",
        "        # Select two individuals at random\n",
        "        individual_1, individual_2 = random.sample(selected_individuals, 2)\n",
        "        new_individual_1, new_individual_2 = crossover(individual_1, individual_2, crossover_rate, crossover_points)\n",
        "        next_generation.append(new_individual_1)\n",
        "        next_generation.append(new_individual_2)\n",
        "\n",
        "    #' Mutate individuals:\n",
        "    for individual in next_generation:\n",
        "        individual = mutate(individual, mutation_rate)\n",
        "        \n",
        "    return next_generation\n",
        "\n",
        "def genetic_algorithm(x_train, y_train, model, population_size=100, selection_size=50, mutation_rate=0.01, crossover_rate=0.3, crossover_points=3, generations=10, battle_participants=4, elite_num=1, bias_strength=100):\n",
        "    \n",
        "    INPUT_SIZE = y_train.shape[0]\n",
        "    \n",
        "    # set up the file to write results to:\n",
        "    counter = 0\n",
        "    base_dir = \"results\"\n",
        "    if not os.path.exists(base_dir):\n",
        "        os.makedirs(base_dir)\n",
        "    while (filename := f\"GA_{counter}.csv\") in os.listdir(base_dir):\n",
        "        counter += 1\n",
        "    path = os.path.join(base_dir, filename)\n",
        "\n",
        "    with open(path, \"w\") as f:\n",
        "        # write hyper parameters to file:\n",
        "        f.write(f\"population_size: {population_size}\\n\")\n",
        "        f.write(f\"selection_size: {selection_size}\\n\")\n",
        "        f.write(f\"mutation_rate: {mutation_rate}\\n\")\n",
        "        f.write(f\"crossover_rate: {crossover_rate}\\n\")\n",
        "        f.write(f\"crossover_points: {crossover_points}\\n\")\n",
        "        f.write(f\"generations: {generations}\\n\")\n",
        "        f.write(f\"battle_participants: {battle_participants}\\n\")\n",
        "        f.write(f\"elite_num: {elite_num}\\n\")\n",
        "        f.write(f\"\\n\")\n",
        "        \n",
        "        # header:\n",
        "        f.write(\"generation,accuracy\\n\")\n",
        "    \n",
        "    best_indv = ()\n",
        "    best_fitness = 0.0\n",
        "    \n",
        "    # Create initial population\n",
        "    population = create_population(population_size, INPUT_SIZE, y_train, bias_strength)\n",
        "\n",
        "    # Evaluate initial population\n",
        "    fitness_scores = []\n",
        "    for individual in population:\n",
        "        # convert to tuples for hashability:\n",
        "        accuracy = evaluate_fitness(model, individual)\n",
        "        fitness_scores.append(accuracy)\n",
        "        \n",
        "        if accuracy > best_fitness:\n",
        "            best_fitness = accuracy\n",
        "            best_indv = individual\n",
        "    \n",
        "    print(f\"Initial population length: {len(population)}\")\n",
        "    print(f\"Initial fitness scores: max = {max(fitness_scores)}, min = {min(fitness_scores)}\")\n",
        "    print(f\"Best fitness: {best_fitness}\")\n",
        "    \n",
        "    # Iterate through generations\n",
        "    start_time = time.time()\n",
        "    for generation in range(generations):\n",
        "        # print(f\"Generation {generation}\")\n",
        "        # Create next generation\n",
        "        population = create_next_generation(population, fitness_scores, selection_size, mutation_rate, crossover_rate, crossover_points, battle_participants, elite_num)\n",
        "        # Evaluate next generation\n",
        "        fitness_scores = []\n",
        "        for individual in population:\n",
        "            accuracy = evaluate_fitness(model, individual)\n",
        "            fitness_scores.append(accuracy)\n",
        "            \n",
        "            if accuracy > best_fitness:\n",
        "                best_fitness = accuracy\n",
        "                best_indv = individual\n",
        "        \n",
        "        time_elapsed = round(time.time() - start_time, 2)\n",
        "        unit = \"seconds\"\n",
        "        if time_elapsed > 60:\n",
        "            time_elapsed = time_elapsed/60 \n",
        "            unit = \"minutes\"\n",
        "        if time_elapsed > 60:\n",
        "            time_elapsed = time_elapsed/60\n",
        "            unit = \"hours\"\n",
        "        print(f\"Best fitness after generation {generation}: {best_fitness} took {time_elapsed} {unit} to train.\")\n",
        "\n",
        "\n",
        "        with open(path, \"a\") as f:\n",
        "            f.write(f\"{generation},{best_fitness}\\n\")\n",
        "        \n",
        "    # clear the cache:\n",
        "    evaluate_fitness.cache_clear()\n",
        "    return best_indv, best_fitness\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cb9253aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def myPrunedSubsetMethod(x_train, y_train, model):\n",
        "    \n",
        "    best_indv, best_accuracy = genetic_algorithm(x_train, y_train, model, population_size=POPULATION_SIZE, selection_size=SELECTION_SIZE, mutation_rate=MUTATION_RATE, crossover_rate=CROSSOVER_RATE, crossover_points=CROSSOVER_POINTS, generations=GENERATIONS, battle_participants=BATTLE_PARTICIPANTS, elite_num=ELITE_NUM, bias_strength=BIAS_STRENGTH)\n",
        "    print(f\"Best accuracy found: {best_accuracy}\")\n",
        "    pruned_indexes = {i:i for i,elt in enumerate(best_indv) if elt == 1}\n",
        "    print(f\"Length of pruned indexes: {len(pruned_indexes)}\")\n",
        "    \n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bd4e4e19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# POPULATION_SIZE  = 10\n",
        "# SELECTION_SIZE   = 5\n",
        "# MUTATION_RATE    = 0.01\n",
        "# CROSSOVER_RATE   = 0.3\n",
        "# CROSSOVER_POINTS = 3\n",
        "# GENERATIONS      = 4\n",
        "# BATTLE_PARTICIPANTS = 2\n",
        "# ELITE_NUM = 1\n",
        "\n",
        "# pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "# loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "# print(f\"Accuracy on test set: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3eae6e36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POPULATION_SIZE: 50, SELECTION_SIZE: 50, MUTATION_RATE: 0.01, CROSSOVER_RATE: 0.3, CROSSOVER_POINTS: 3, GENERATIONS: 10, BATTLE_PARTICIPANTS: 2, ELITE_NUM: 0\n",
            "Finished at: 11:26:48\n",
            "Prioritization scores: {0: 1.09, 1: 1.0, 2: 1.11, 3: 1.08, 4: 1.04, 5: 1.13, 6: 1.17, 7: 1.1, 8: 1.11, 9: 1.12}\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 1s 16ms/step - loss: 2.2378 - accuracy: 0.1577\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.9910 - accuracy: 0.3462\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.7545 - accuracy: 0.4385\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6535 - accuracy: 0.4300\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.6614 - accuracy: 0.3955\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.5138 - accuracy: 0.4125\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.4277 - accuracy: 0.4418\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4231 - accuracy: 0.3771\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.3667 - accuracy: 0.4356\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.3281 - accuracy: 0.4356\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.2647 - accuracy: 0.4673\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2873 - accuracy: 0.4350\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.3146 - accuracy: 0.4324\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.2727 - accuracy: 0.4518\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.2386 - accuracy: 0.4641\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2690 - accuracy: 0.3915\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.2797 - accuracy: 0.4154\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.2410 - accuracy: 0.4448\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1766 - accuracy: 0.4752\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2046 - accuracy: 0.4469\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.2849 - accuracy: 0.4266\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.2480 - accuracy: 0.4416\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1972 - accuracy: 0.4735\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0881 - accuracy: 0.5159\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.2281 - accuracy: 0.4590\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1881 - accuracy: 0.4569\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1462 - accuracy: 0.4725\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1252 - accuracy: 0.4730\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1432 - accuracy: 0.4610\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1148 - accuracy: 0.4790\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0738 - accuracy: 0.5180\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1684 - accuracy: 0.4066\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1713 - accuracy: 0.4646\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1354 - accuracy: 0.4764\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0895 - accuracy: 0.5098\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1338 - accuracy: 0.4023\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1514 - accuracy: 0.4351\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1203 - accuracy: 0.4515\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0819 - accuracy: 0.4845\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0571 - accuracy: 0.4803\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.1519 - accuracy: 0.4566\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.1065 - accuracy: 0.4927\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.1047 - accuracy: 0.4966\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1333 - accuracy: 0.3935\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1923 - accuracy: 0.4267\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1315 - accuracy: 0.4695\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0891 - accuracy: 0.4735\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0397 - accuracy: 0.4889\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.1037 - accuracy: 0.4806\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0604 - accuracy: 0.4913\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0341 - accuracy: 0.5117\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1476 - accuracy: 0.3661\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0932 - accuracy: 0.4648\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0648 - accuracy: 0.4904\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0226 - accuracy: 0.5128\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0344 - accuracy: 0.4946\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0892 - accuracy: 0.4615\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0480 - accuracy: 0.4907\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0297 - accuracy: 0.5083\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0704 - accuracy: 0.4491\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.1207 - accuracy: 0.4625\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1.0667 - accuracy: 0.5025\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0175 - accuracy: 0.5175\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0427 - accuracy: 0.4393\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0779 - accuracy: 0.4751\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0389 - accuracy: 0.4780\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0058 - accuracy: 0.5054\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9929 - accuracy: 0.4933\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1213 - accuracy: 0.4459\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0938 - accuracy: 0.4570\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0464 - accuracy: 0.4985\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0071 - accuracy: 0.4845\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0564 - accuracy: 0.4606\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0348 - accuracy: 0.4749\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0182 - accuracy: 0.4995\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0497 - accuracy: 0.3897\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0441 - accuracy: 0.4637\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0328 - accuracy: 0.5092\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0184 - accuracy: 0.5082\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0703 - accuracy: 0.4214\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.1017 - accuracy: 0.4400\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1.0580 - accuracy: 0.4829\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1.0663 - accuracy: 0.4882\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9647 - accuracy: 0.5022\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0216 - accuracy: 0.4756\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0090 - accuracy: 0.5039\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9835 - accuracy: 0.5146\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9791 - accuracy: 0.4607\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0203 - accuracy: 0.4900\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9895 - accuracy: 0.5033\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9654 - accuracy: 0.5567\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9744 - accuracy: 0.4821\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9906 - accuracy: 0.4882\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9794 - accuracy: 0.5108\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9758 - accuracy: 0.5266\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9275 - accuracy: 0.5435\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0465 - accuracy: 0.4358\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9957 - accuracy: 0.4931\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9714 - accuracy: 0.5175\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9326 - accuracy: 0.5290\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0577 - accuracy: 0.4480\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0308 - accuracy: 0.4783\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0176 - accuracy: 0.4914\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9676 - accuracy: 0.4648\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0042 - accuracy: 0.4748\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9977 - accuracy: 0.4806\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9397 - accuracy: 0.5271\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0593 - accuracy: 0.3756\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0468 - accuracy: 0.4545\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0271 - accuracy: 0.4899\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9934 - accuracy: 0.5000\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9596 - accuracy: 0.4769\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0205 - accuracy: 0.4867\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9975 - accuracy: 0.5038\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9727 - accuracy: 0.5143\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9482 - accuracy: 0.5031\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0131 - accuracy: 0.4947\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9954 - accuracy: 0.4938\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9531 - accuracy: 0.5253\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9698 - accuracy: 0.4788\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0209 - accuracy: 0.4511\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9887 - accuracy: 0.4819\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9537 - accuracy: 0.5298\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9745 - accuracy: 0.4657\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9751 - accuracy: 0.5178\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9595 - accuracy: 0.5072\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9414 - accuracy: 0.5043\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9442 - accuracy: 0.5053\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0325 - accuracy: 0.4942\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 1.0216 - accuracy: 0.4912\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9900 - accuracy: 0.5214\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9833 - accuracy: 0.4406\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0063 - accuracy: 0.4779\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9788 - accuracy: 0.4892\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9383 - accuracy: 0.5427\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9333 - accuracy: 0.5208\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9965 - accuracy: 0.4896\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9779 - accuracy: 0.5104\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9665 - accuracy: 0.5135\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9918 - accuracy: 0.4399\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 1.0349 - accuracy: 0.4514\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1.0103 - accuracy: 0.4946\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9619 - accuracy: 0.4848\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9269 - accuracy: 0.5046\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9956 - accuracy: 0.4879\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9801 - accuracy: 0.4980\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9688 - accuracy: 0.5071\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9641 - accuracy: 0.4681\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9748 - accuracy: 0.4893\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9586 - accuracy: 0.4995\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9487 - accuracy: 0.5169\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9541 - accuracy: 0.4721\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9691 - accuracy: 0.4824\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9642 - accuracy: 0.5095\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9312 - accuracy: 0.5246\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9400 - accuracy: 0.4607\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9511 - accuracy: 0.5106\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9373 - accuracy: 0.5257\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9093 - accuracy: 0.5308\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9099 - accuracy: 0.4905\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9779 - accuracy: 0.4679\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9701 - accuracy: 0.5074\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9235 - accuracy: 0.5370\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9414 - accuracy: 0.4851\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9701 - accuracy: 0.4948\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9371 - accuracy: 0.5240\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9077 - accuracy: 0.5198\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8951 - accuracy: 0.5265\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9722 - accuracy: 0.4966\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 13ms/step - loss: 0.9430 - accuracy: 0.5189\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9142 - accuracy: 0.5324\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9189 - accuracy: 0.4975\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9989 - accuracy: 0.4965\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9651 - accuracy: 0.4824\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9478 - accuracy: 0.5206\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8900 - accuracy: 0.5317\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9697 - accuracy: 0.5118\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9538 - accuracy: 0.5283\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9464 - accuracy: 0.5313\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8685 - accuracy: 0.5694\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 1.0053 - accuracy: 0.4753\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9798 - accuracy: 0.4905\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9641 - accuracy: 0.4953\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0132 - accuracy: 0.4163\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9681 - accuracy: 0.4928\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9494 - accuracy: 0.4908\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9235 - accuracy: 0.5204\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9544 - accuracy: 0.4420\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9434 - accuracy: 0.4966\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9204 - accuracy: 0.5303\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9157 - accuracy: 0.5419\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8909 - accuracy: 0.5277\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9652 - accuracy: 0.4826\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9267 - accuracy: 0.4984\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9103 - accuracy: 0.5079\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9487 - accuracy: 0.4410\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9572 - accuracy: 0.4923\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9449 - accuracy: 0.4851\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9129 - accuracy: 0.5179\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9466 - accuracy: 0.4356\n",
            "Initial population length: 50\n",
            "Initial fitness scores: max = 0.5694000124931335, min = 0.366100013256073\n",
            "Best fitness: 0.5694000124931335\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9414 - accuracy: 0.4852\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8908 - accuracy: 0.5550\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8860 - accuracy: 0.5646\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9370 - accuracy: 0.4893\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8887 - accuracy: 0.5379\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8970 - accuracy: 0.5235\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8627 - accuracy: 0.5562\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8702 - accuracy: 0.5456\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9616 - accuracy: 0.5295\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9269 - accuracy: 0.5215\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8849 - accuracy: 0.5516\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9247 - accuracy: 0.4942\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9653 - accuracy: 0.4990\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9466 - accuracy: 0.5192\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8988 - accuracy: 0.5314\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9000 - accuracy: 0.5282\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9851 - accuracy: 0.4673\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9717 - accuracy: 0.5015\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9198 - accuracy: 0.5266\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8789 - accuracy: 0.5294\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8991 - accuracy: 0.5330\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8763 - accuracy: 0.5598\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8442 - accuracy: 0.5856\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9357 - accuracy: 0.4488\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9668 - accuracy: 0.4864\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9349 - accuracy: 0.5166\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9183 - accuracy: 0.5226\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8768 - accuracy: 0.5512\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9098 - accuracy: 0.5365\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9057 - accuracy: 0.5397\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8629 - accuracy: 0.5687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9135 - accuracy: 0.5130\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9367 - accuracy: 0.5220\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9427 - accuracy: 0.5107\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9053 - accuracy: 0.5332\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8999 - accuracy: 0.5175\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9634 - accuracy: 0.4907\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9116 - accuracy: 0.5278\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.9032 - accuracy: 0.5463\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9219 - accuracy: 0.5060\n",
            "Best fitness after generation 0: 0.5694000124931335 took 12.81 seconds to train.\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9103 - accuracy: 0.5294\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9087 - accuracy: 0.5274\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 16ms/step - loss: 0.8564 - accuracy: 0.5765\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8696 - accuracy: 0.5495\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9627 - accuracy: 0.5123\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9291 - accuracy: 0.5299\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9212 - accuracy: 0.5447\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9103 - accuracy: 0.4984\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9567 - accuracy: 0.4948\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9294 - accuracy: 0.5052\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9061 - accuracy: 0.5405\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9058 - accuracy: 0.5045\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9362 - accuracy: 0.5053\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9201 - accuracy: 0.5011\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8903 - accuracy: 0.5630\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8831 - accuracy: 0.5203\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8430 - accuracy: 0.5686\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8360 - accuracy: 0.5756\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8257 - accuracy: 0.5796\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8724 - accuracy: 0.5255\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8629 - accuracy: 0.5506\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8555 - accuracy: 0.5671\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8575 - accuracy: 0.5554\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9153 - accuracy: 0.5049\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9157 - accuracy: 0.5480\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9000 - accuracy: 0.5582\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8509 - accuracy: 0.5888\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8927 - accuracy: 0.5118\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9679 - accuracy: 0.4896\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9328 - accuracy: 0.5146\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.9026 - accuracy: 0.5355\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8687 - accuracy: 0.5584\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.9136 - accuracy: 0.5149\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8891 - accuracy: 0.5322\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8523 - accuracy: 0.5525\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9178 - accuracy: 0.4834\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9302 - accuracy: 0.5144\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8985 - accuracy: 0.5283\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8995 - accuracy: 0.5492\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9012 - accuracy: 0.5199\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9206 - accuracy: 0.5156\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8852 - accuracy: 0.5368\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8638 - accuracy: 0.5670\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9106 - accuracy: 0.4888\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8224 - accuracy: 0.5960\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8440 - accuracy: 0.5841\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8179 - accuracy: 0.5930\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8643 - accuracy: 0.5442\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9354 - accuracy: 0.5039\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9313 - accuracy: 0.5187\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8922 - accuracy: 0.5512\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8733 - accuracy: 0.5328\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8957 - accuracy: 0.5442\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8567 - accuracy: 0.5832\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8492 - accuracy: 0.5684\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8636 - accuracy: 0.5451\n",
            "Best fitness after generation 1: 0.5694000124931335 took 30.76 seconds to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.8920 - accuracy: 0.5414\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.8753 - accuracy: 0.5576\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.8624 - accuracy: 0.5608\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8651 - accuracy: 0.5477\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8694 - accuracy: 0.5323\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8587 - accuracy: 0.5561\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8240 - accuracy: 0.5720\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8891 - accuracy: 0.5035\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8875 - accuracy: 0.5661\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8561 - accuracy: 0.5692\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.8573 - accuracy: 0.5671\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8684 - accuracy: 0.5428\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8637 - accuracy: 0.5557\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8229 - accuracy: 0.5938\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8041 - accuracy: 0.6124\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8870 - accuracy: 0.5252\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.9339 - accuracy: 0.5183\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9339 - accuracy: 0.5055\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8983 - accuracy: 0.5401\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9280 - accuracy: 0.4467\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8984 - accuracy: 0.5268\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 15ms/step - loss: 0.8493 - accuracy: 0.5609\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 14ms/step - loss: 0.8507 - accuracy: 0.5871\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8773 - accuracy: 0.5495\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.9230 - accuracy: 0.5354\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8875 - accuracy: 0.5530\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.8442 - accuracy: 0.5806\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8647 - accuracy: 0.5549\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8183 - accuracy: 0.5814\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7926 - accuracy: 0.6149\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7779 - accuracy: 0.6138\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8787 - accuracy: 0.5321\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7656 - accuracy: 0.6320\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7829 - accuracy: 0.6331\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.7466 - accuracy: 0.6331\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8627 - accuracy: 0.5479\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8993 - accuracy: 0.5577\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9153 - accuracy: 0.5298\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.8740 - accuracy: 0.5646\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9054 - accuracy: 0.4966\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.7965 - accuracy: 0.6152\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7889 - accuracy: 0.6038\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7887 - accuracy: 0.6121\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9055 - accuracy: 0.4947\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7988 - accuracy: 0.5904\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7755 - accuracy: 0.6050\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7552 - accuracy: 0.6280\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8885 - accuracy: 0.5345\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7589 - accuracy: 0.6290\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7469 - accuracy: 0.6453\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.7307 - accuracy: 0.6595\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9011 - accuracy: 0.5131\n",
            "Epoch 1/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.8166 - accuracy: 0.5682\n",
            "Epoch 2/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.8083 - accuracy: 0.6099\n",
            "Epoch 3/3\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.7771 - accuracy: 0.6215\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8716 - accuracy: 0.5616\n",
            "Best fitness after generation 2: 0.5694000124931335 took 48.98 seconds to train.\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.6900 - accuracy: 0.6917\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6944 - accuracy: 0.6874\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6881 - accuracy: 0.6980\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9014 - accuracy: 0.5313\n",
            "Epoch 1/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7158 - accuracy: 0.6646\n",
            "Epoch 2/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.7166 - accuracy: 0.6503\n",
            "Epoch 3/3\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.6861 - accuracy: 0.6708\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# test a range of hyper parameters:\n",
        "population_list = [50, 100, 200]\n",
        "selection_list = [50, 100, 150]\n",
        "mutation_list = [0.01, 0.05, 0.1]\n",
        "crossover_rate_list = [0.3, 0.5, 0.7]\n",
        "crossover_list = [3, 5, 10]\n",
        "generation_list = [10, 50, 100, 200]\n",
        "battle_participants_list = [2, 4, 10]\n",
        "elitism_list = [0, 1, 2]\n",
        "\n",
        "with open(\"results/combinations.csv\", \"w\") as f:\n",
        "    f.write(\"POPULATION_SIZE,SELECTION_SIZE,MUTATION_RATE,CROSSOVER_RATE,CROSSOVER_POINTS,GENERATIONS,BATTLE_PARTICIPANTS,ELITE_NUM\\n\")\n",
        "\n",
        "for POPULATION_SIZE in population_list:\n",
        "    for SELECTION_SIZE in selection_list:\n",
        "        for MUTATION_RATE in mutation_list:\n",
        "            for CROSSOVER_RATE in crossover_rate_list:\n",
        "                for CROSSOVER_POINTS in crossover_list:\n",
        "                    for GENERATIONS in generation_list:\n",
        "                        for BATTLE_PARTICIPANTS in battle_participants_list:\n",
        "                            for ELITE_NUM in elitism_list:\n",
        "                                print(f\"POPULATION_SIZE: {POPULATION_SIZE}, SELECTION_SIZE: {SELECTION_SIZE}, MUTATION_RATE: {MUTATION_RATE}, CROSSOVER_RATE: {CROSSOVER_RATE}, CROSSOVER_POINTS: {CROSSOVER_POINTS}, GENERATIONS: {GENERATIONS}, BATTLE_PARTICIPANTS: {BATTLE_PARTICIPANTS}, ELITE_NUM: {ELITE_NUM}\")\n",
        "                                print(f\"Finished at: {time.ctime().split()[3]}\")\n",
        "                                pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "                                loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "                                print(f\"Accuracy: {accuracy}\")\n",
        "                                with open(\"results/combinations.csv\", \"a\") as f:\n",
        "                                    f.write(f\"{POPULATION_SIZE},{SELECTION_SIZE},{MUTATION_RATE},{CROSSOVER_RATE},{CROSSOVER_POINTS},{GENERATIONS},{BATTLE_PARTICIPANTS},{ELITE_NUM},{accuracy}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
