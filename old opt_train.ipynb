{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JalalSayed1/AI-Coursework/blob/master/opt_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "anticipated-consumer",
      "metadata": {
        "id": "anticipated-consumer"
      },
      "source": [
        "In this assignment, we are going to implement see if we can optimally select a subset of training instances for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in /home/jalal/.local/lib/python3.10/site-packages (1.3.2)\n",
            "Requirement already satisfied: scipy in /home/jalal/.local/lib/python3.10/site-packages (1.11.3)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /home/jalal/.local/lib/python3.10/site-packages (3.8.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /home/jalal/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jalal/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/jalal/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/jalal/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/jalal/.local/lib/python3.10/site-packages (from matplotlib) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jalal/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/jalal/.local/lib/python3.10/site-packages (from matplotlib) (1.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/jalal/.local/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/jalal/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: scipy, matplotlib\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.3\n",
            "    Uninstalling scipy-1.11.3:\n",
            "      Successfully uninstalled scipy-1.11.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.1\n",
            "    Uninstalling matplotlib-3.8.1:\n",
            "      Successfully uninstalled matplotlib-3.8.1\n",
            "Successfully installed matplotlib-3.8.2 scipy-1.11.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U scikit-learn scipy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "higher-nebraska",
      "metadata": {
        "id": "higher-nebraska"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-20 10:24:14.612980: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-11-20 10:24:14.636092: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-20 10:24:14.746498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 10:24:14.746537: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 10:24:14.747205: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 10:24:14.806233: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-20 10:24:14.807279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-20 10:24:15.552917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daily-internship",
      "metadata": {
        "id": "daily-internship"
      },
      "source": [
        "We are going to work with the MNIST dataset, a popular dataset for hand-written digit recognition. Here we load the datatset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "palestinian-texas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "palestinian-texas",
        "outputId": "add68549-ff74-4c9e-b2f2-7199a243f486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Loaded 60000 train samples\n",
            "Loaded 10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1) # 28x28 pixel images with 1 colour channel (grayscale)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "# x = images, y = labels\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255 # 0-255 to 0-1\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1) # -1 means last dimension\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"Loaded {} train samples\".format(x_train.shape[0]))\n",
        "print(\"Loaded {} test samples\".format(x_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-desert",
      "metadata": {
        "id": "empty-desert"
      },
      "source": [
        "Now corrupt the labels with common types of mistakes. The variable 'noise_probability' controls the amount of errors introduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "champion-technician",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "champion-technician",
        "outputId": "76618bb0-b52f-4bfb-b5ce-8b3cf2eb9a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corruptions: {'0->2': 659, '1->8': 652, '9->8': 717, '3->2': 699, '1->5': 662, '3->9': 749, '5->2': 612, '6->5': 676, '1->6': 660, '7->0': 606, '2->3': 719, '8->6': 658, '6->3': 697, '4->5': 576, '0->6': 691, '1->2': 672, '1->9': 692, '4->8': 564, '2->8': 715, '7->6': 640, '8->2': 678, '6->0': 670, '5->9': 652, '0->3': 698, '7->8': 629, '6->9': 703, '9->6': 680, '9->3': 673, '3->8': 709, '9->5': 677, '4->0': 569, '6->2': 674, '4->9': 556, '5->8': 635, '1->0': 704, '0->8': 671, '7->3': 601, '9->2': 690, '2->0': 702, '3->6': 706, '7->2': 632, '3->0': 707, '5->0': 598, '1->3': 637, '0->5': 686, '2->6': 672, '8->5': 707, '4->2': 610, '8->9': 712, '5->3': 619, '8->0': 671, '9->0': 684, '7->9': 601, '7->5': 649, '5->6': 632, '2->5': 683, '3->5': 681, '8->3': 682, '0->9': 694, '4->3': 609, '6->8': 693, '2->9': 694, '4->6': 593}\n",
            "Corrupted indexes: [1, 3, 4, 7, 8, 10, 11, 13, 14, 15]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "noise_probability = 0.7\n",
        "\n",
        "def corrupt_label(y, err):\n",
        "    found = np.where(err == y)\n",
        "    if len(found) > 0:\n",
        "        # select an element at random (index != found)\n",
        "        noisy_label = random.choice(err)\n",
        "        while noisy_label == y:\n",
        "            noisy_label = random.choice(err)\n",
        "        return noisy_label\n",
        "    return y\n",
        "\n",
        "# We corrupt the MNIST data with some common mistakes, such as 3-->8, 8-->3, 1-->{4, 7}, 5-->6 etc.\n",
        "def corrupt_labels(y_train, noise_probability):\n",
        "    num_samples = y_train.shape[0]\n",
        "    err_es_1 = np.array([0, 2, 3, 5, 6, 8, 9])\n",
        "    err_es_2 = np.array([1, 4, 7])\n",
        "\n",
        "    corruptions = {}\n",
        "    corrupted_indexes = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # generate a random number between 0 and 1:\n",
        "        p = random.random()\n",
        "\n",
        "        #! if p > noise_probability, then we do not corrupt the label?\n",
        "        if p < noise_probability:\n",
        "            y = y_train[i]\n",
        "            y_noisy = corrupt_label(y, err_es_1)\n",
        "            if y_noisy == y:\n",
        "                y_noisy = corrupt_label(y, err_es_2)\n",
        "\n",
        "            key = str(y_train[i]) + '->' + str(y_noisy)\n",
        "            corrupted_indexes[i] = i\n",
        "\n",
        "            if key in corruptions:\n",
        "                corruptions[key] += 1\n",
        "            else:\n",
        "                corruptions[key] = 0\n",
        "\n",
        "            y_train[i] = y_noisy\n",
        "\n",
        "    return corruptions, corrupted_indexes\n",
        "\n",
        "corruptions, corrupted_indexes = corrupt_labels(y_train, noise_probability)\n",
        "print (\"Corruptions: \" + str(corruptions))\n",
        "print (\"Corrupted indexes: {}\".format(list(corrupted_indexes.keys())[0:10]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "quality-gauge",
      "metadata": {
        "id": "quality-gauge"
      },
      "outputs": [],
      "source": [
        "# convert class vectors to binary class matrices\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-celebrity",
      "metadata": {
        "id": "fifth-celebrity"
      },
      "source": [
        "Supervised (parametric) training with the (noisy) labeled examples. Note that this model is trained on the entire dataset (the value of the parameter pruned_indexes is null here, which means that we leave out no points), which is noisy (20% of the labels are corrupted). Now the question is: is this the best model that we can train or can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "extreme-ethernet",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extreme-ethernet",
        "outputId": "ffd3c54a-09d5-4f13-9fb8-b59425c1c78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5408)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                54090     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54410 (212.54 KB)\n",
            "Trainable params: 54410 (212.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 3 # very high epochs might overfit the model.\n",
        "validation_split=0.1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "def prune_points(x_train, y_train, pruned_indexes):\n",
        "    num_samples = x_train.shape[0] # = 60000\n",
        "    x_train_pruned = []\n",
        "    y_train_pruned = []\n",
        "    for i in range(num_samples):\n",
        "        if not i in pruned_indexes:\n",
        "            x_train_pruned.append(x_train[i])\n",
        "            y_train_pruned.append(y_train[i])\n",
        "\n",
        "    return np.array(x_train_pruned), np.array(y_train_pruned)\n",
        "\n",
        "def trainAndEvaluateModel(x_train, y_train, x_test, y_test, model, pruned_indexes):\n",
        "\n",
        "    if not pruned_indexes == None:\n",
        "        x_train_pruned, y_train_pruned = prune_points(x_train, y_train, pruned_indexes)\n",
        "    else:\n",
        "        x_train_pruned = x_train\n",
        "        y_train_pruned = y_train\n",
        "\n",
        "    # start training the model:\n",
        "    model.fit(x_train_pruned, y_train_pruned, batch_size=batch_size, epochs=epochs)\n",
        "    model.evaluate(x_test, y_test)\n",
        "    keras.backend.clear_session() # remove previous training weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-waterproof",
      "metadata": {
        "id": "indie-waterproof"
      },
      "source": [
        "And we call the following function to train a model on the entire dataset and evaluate it on the test set. The accuracy on the test set is quite good, but can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "embedded-staff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embedded-staff",
        "outputId": "a88bc645-fc02-40a7-d5a3-41e9e1d5ebb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 8s 15ms/step - loss: 1.9944 - accuracy: 0.2448\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 9s 18ms/step - loss: 1.9496 - accuracy: 0.2721\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 1.9398 - accuracy: 0.2789\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 1.3758 - accuracy: 0.8898\n"
          ]
        }
      ],
      "source": [
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "structured-lithuania",
      "metadata": {
        "id": "structured-lithuania"
      },
      "source": [
        "You need to implement a subset selection function that when called will return a subset of instances which will be used to train the model. This setup ensures that you also pass in another dictionary which contains the indexes of the instances that you would not want to use while training the model, i.e., it should contain a list of indexes that you would decide to **leave out** for training.\n",
        "\n",
        "Here's the code and a sample implementation that returns a randomly chosen set of instances that you are to be left out. Since we chose 70% probability of label corruption (check the **noise_probability** parameter), we also select a subset where we leave out the same proportion of points. This is a baseline implementation and obviously you should aim to achieve better results than this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "unique-operator",
      "metadata": {
        "id": "unique-operator"
      },
      "outputs": [],
      "source": [
        "# Here 'x_train', 'y_train' and model' are an unused parameters. But you may get better results by leveraging these.\n",
        "def baseLinePrunedSubsetMethod(x_train, y_train, model):\n",
        "    pruned_indexes = {}\n",
        "    num_samples = x_train.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        p = random.random()\n",
        "\n",
        "        if p < noise_probability: # this is the global variable (only useful for this naive approach)\n",
        "            pruned_indexes[i] = i\n",
        "    return pruned_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-steel",
      "metadata": {
        "id": "stunning-steel"
      },
      "source": [
        "Let's see how this naive baseline works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "formed-refrigerator",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "formed-refrigerator",
        "outputId": "234122b6-f927-40d9-a0de-38f4de226c5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "141/141 [==============================] - 2s 14ms/step - loss: 1.9359 - accuracy: 0.2781\n",
            "Epoch 2/3\n",
            "141/141 [==============================] - 2s 13ms/step - loss: 1.9265 - accuracy: 0.2807\n",
            "Epoch 3/3\n",
            "141/141 [==============================] - 2s 13ms/step - loss: 1.9198 - accuracy: 0.2808\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4109 - accuracy: 0.8898\n"
          ]
        }
      ],
      "source": [
        "pruned_indexes = baseLinePrunedSubsetMethod(x_train, y_train, model)\n",
        "# print (f\"pruned_indexes: {pruned_indexes}\")\n",
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backed-cache",
      "metadata": {
        "id": "backed-cache"
      },
      "source": [
        "Let's now see if we had known what points were actually corrupted (more of a hypothetical unrealistic situation), does leaving out those points actually improve the model's effectiveness. It turns out that it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "amino-orientation",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amino-orientation",
        "outputId": "db6011ba-9b80-4cc4-d030-97206836c785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "143/143 [==============================] - 2s 12ms/step - loss: 0.3704 - accuracy: 0.9185\n",
            "Epoch 2/3\n",
            "143/143 [==============================] - 2s 13ms/step - loss: 0.1829 - accuracy: 0.9486\n",
            "Epoch 3/3\n",
            "143/143 [==============================] - 2s 12ms/step - loss: 0.1485 - accuracy: 0.9594\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9637\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#! This keeps going up and up: bc we keep training the model on the same data. \n",
        "trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, corrupted_indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bright-constitutional",
      "metadata": {
        "id": "bright-constitutional"
      },
      "source": [
        "Your task is to implement your own version of (say of name **myPrunedSubsetMethod** (which should take as arguments x_train, y_train, and the model). The function should return a dictionary of indexes that are to be left out. Plug your function in and evaluate the results. Write a thorough report on the methodology and analyse the results.\n",
        "\n",
        "Some hints:\n",
        "You can approach this as a discrete state space optimisation problem, where firstly you can define a \"selection batch size\" (this is not the same as training batch size), which decides which batch of instances you're going to leave out. For instance, if you are in a state where the training set is $X$, you may select (by some heuristics) which points you're gonna leave out (let that set be $\\delta \\subset X$) so that a child state becomes $X' = X - \\delta$. Similarly, if you choose a different $\\delta$ you get a different child state. You then need to train and evaluate (call the function *trainAndEvaluateModel*) to see if that child state led to an improvement or not.\n",
        "\n",
        "You are free to use any algorithm, e.g., simulated annealing, A* search, genetic algorithm etc. to implement this discrete state space optimisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"color:red; height:300px\"> ____________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corruptions: {'5->0': 633, '0->5': 682, '4->5': 596, '1->2': 672, '9->5': 652, '2->3': 715, '1->8': 651, '3->8': 702, '4->0': 588, '3->0': 704, '3->6': 705, '6->0': 693, '1->0': 649, '7->9': 608, '2->0': 685, '6->5': 679, '0->3': 753, '9->6': 732, '1->3': 654, '2->8': 696, '4->2': 550, '3->9': 727, '2->5': 674, '7->8': 608, '8->6': 706, '5->9': 622, '7->6': 645, '8->9': 656, '9->0': 708, '9->8': 654, '0->6': 689, '8->2': 669, '4->3': 580, '4->9': 621, '1->9': 712, '0->9': 685, '6->8': 696, '7->3': 611, '9->2': 719, '6->3': 655, '0->8': 692, '4->6': 601, '7->0': 637, '1->5': 658, '3->5': 717, '8->3': 668, '6->9': 719, '2->6': 718, '9->3': 699, '5->2': 609, '1->6': 663, '2->9': 691, '7->2': 656, '8->5': 688, '6->2': 706, '5->3': 602, '7->5': 619, '5->8': 613, '0->2': 668, '4->8': 595, '3->2': 729, '5->6': 659, '8->0': 736}\n",
            "Corrupted indexes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 13, 13, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 5408)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5408)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                54090     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54410 (212.54 KB)\n",
            "Trainable params: 54410 (212.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "loss_list = []\n",
        "acc_list = []\n",
        "root = {\"State\": \"X\", \"Accuracy\" : 0, \"Loss\" : 0, \"Pruned\" : {}, \"Next States\": []}\n",
        "\n",
        "#! remove this:\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1) # 28x28 pixel images with 1 colour channel (grayscale)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "# x = images, y = labels\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255 # 0-255 to 0-1\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1) # -1 means last dimension\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "import random\n",
        "noise_probability = 0.7\n",
        "\n",
        "def corrupt_label(y, err):\n",
        "    found = np.where(err == y)\n",
        "    if len(found) > 0: #! if y (true label if is in err)\n",
        "        # select an element at random (index != found)\n",
        "        noisy_label = random.choice(err)\n",
        "        while noisy_label == y:\n",
        "            noisy_label = random.choice(err)\n",
        "        return noisy_label\n",
        "    return y #! if y is not in err, then return y (true label) then use error set 2\n",
        "\n",
        "# We corrupt the MNIST data with some common mistakes, such as 3-->8, 8-->3, 1-->{4, 7}, 5-->6 etc.\n",
        "def corrupt_labels(y_train, noise_probability):\n",
        "    num_samples = y_train.shape[0]\n",
        "    err_es_1 = np.array([0, 2, 3, 5, 6, 8, 9])\n",
        "    err_es_2 = np.array([1, 4, 7])\n",
        "\n",
        "    corruptions = {}\n",
        "    corrupted_indexes = {}\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # generate a random number between 0 and 1:\n",
        "        p = random.random()\n",
        "\n",
        "        #! if p > noise_probability, then we do not corrupt the label?\n",
        "        if p < noise_probability:\n",
        "            y = y_train[i] # true label\n",
        "            y_noisy = corrupt_label(y, err_es_1)\n",
        "            if y_noisy == y:\n",
        "                y_noisy = corrupt_label(y, err_es_2)\n",
        "\n",
        "            key = str(y_train[i]) + '->' + str(y_noisy)\n",
        "            corrupted_indexes[i] = i\n",
        "\n",
        "            if key in corruptions:\n",
        "                corruptions[key] += 1\n",
        "            else:\n",
        "                corruptions[key] = 0\n",
        "\n",
        "            y_train[i] = y_noisy\n",
        "\n",
        "    return corruptions, corrupted_indexes\n",
        "\n",
        "corruptions, corrupted_indexes = corrupt_labels(y_train, noise_probability)\n",
        "print (\"Corruptions: \" + str(corruptions))\n",
        "print (\"Corrupted indexes: {}\".format(list(corrupted_indexes.keys())[0:10]))\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 3 # very high epochs might overfit the model.\n",
        "validation_split=0.1\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "def prune_points(x_train, y_train, pruned_indexes):\n",
        "    num_samples = x_train.shape[0] # = 60000\n",
        "    x_train_pruned = []\n",
        "    y_train_pruned = []\n",
        "    for i in range(num_samples):\n",
        "        if not i in pruned_indexes:\n",
        "            x_train_pruned.append(x_train[i])\n",
        "            y_train_pruned.append(y_train[i])\n",
        "\n",
        "    return np.array(x_train_pruned), np.array(y_train_pruned)\n",
        "\n",
        "def trainAndEvaluateModel(x_train, y_train, x_test, y_test, model, pruned_indexes):\n",
        "\n",
        "    if not pruned_indexes == None:\n",
        "        x_train_pruned, y_train_pruned = prune_points(x_train, y_train, pruned_indexes)\n",
        "    else:\n",
        "        x_train_pruned = x_train\n",
        "        y_train_pruned = y_train\n",
        "\n",
        "    # start training the model:\n",
        "    model.fit(x_train_pruned, y_train_pruned, batch_size=batch_size, epochs=epochs)\n",
        "    \n",
        "    #! remove this:\n",
        "    y_prob = model.predict(x_test)  # Get predictions\n",
        "    y_pred = np.argmax(y_prob, axis=1)  # Convert to class labels\n",
        "    # If y_test is one-hot encoded, convert it back to labels\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "    accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    \n",
        "    #! if my algorithm must use the loss or accuracy, then I can mention in the report:\n",
        "    loss, accuracy = model.evaluate(x_test, y_test)\n",
        "    # loss_list.append(loss) #! remove this\n",
        "    # acc_list.append(accuracy) #! remove this\n",
        "    #! ------------------------\n",
        "\n",
        "    keras.backend.clear_session() # remove previous training weights\n",
        "\n",
        "    #! remove this:    \n",
        "    return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#! moved out to archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_random_index(y_train, risk_labels):\n",
        "    # Find indices of all elements in y_train that match any of the risk labels\n",
        "    risk_indices = np.where(np.isin(y_train, risk_labels))[0]\n",
        "\n",
        "    # If there are no matching indices, return None or handle as needed\n",
        "    if len(risk_indices) == 0:\n",
        "        return None\n",
        "\n",
        "    # Randomly select an index from the risk indices\n",
        "    return np.random.choice(risk_indices, size=1, replace=False)[0]\n",
        "\n",
        "def heuristic_to_identify_samples(x_train, y_train, num_samples_to_remove):\n",
        "    #! ASSUMPTION: might have to have num_samples_to_remove be less than len(y_train or x_train) for this to work\n",
        "    high_risk_labels = [0, 2, 3, 5, 6, 8, 9]\n",
        "    medium_risk_labels = [1, 4, 7]\n",
        "\n",
        "    # sample_risks = []\n",
        "\n",
        "    # for index, label in enumerate(y_train):\n",
        "    #     if label in high_risk_labels:\n",
        "    #         risk = 0.6  # 60% chance of being mislabeled\n",
        "    #     elif label in medium_risk_labels:\n",
        "    #         risk = 0.23  # 23% chance\n",
        "\n",
        "    #     sample_risks.append((index, risk))\n",
        "\n",
        "    # Sort samples by descending risk (higher risk first)\n",
        "    # sorted_samples = sorted(sample_risks, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top 'num_samples_to_remove' indexes\n",
        "    # indexes_to_remove = [index for index, risk in sorted_samples[:num_samples_to_remove]]\n",
        "\n",
        "    num_of_high_risk = int(num_samples_to_remove * 0.6)\n",
        "    num_of_medium_risk = int(num_samples_to_remove * 0.23)\n",
        "    possible_num_of_index_to_remove = num_of_high_risk + num_of_medium_risk\n",
        "    \n",
        "    # a set of indexes to remove:\n",
        "    indexes_to_remove = set()\n",
        "    \n",
        "    \n",
        "    #! risk this loop doesn't terminate bc we might not have enough samples to remove:\n",
        "    # num_of_high_risk > 0 and\n",
        "    # while len(indexes_to_remove) < num_samples_to_remove and (num_of_high_risk > 0 or num_of_medium_risk > 0):\n",
        "    # while len(indexes_to_remove) < possible_num_of_index_to_remove:\n",
        "    print(f\"len(indexes_to_remove): {len(indexes_to_remove)}\")\n",
        "    # index = np.random.choice(np.where(y_train == random.choice(high_risk_labels))[0], size=1, replace=False)\n",
        "    # get num_of_high_risk indexes from data:\n",
        "    while num_of_high_risk > 0 and len(indexes_to_remove) < possible_num_of_index_to_remove:\n",
        "        #! check if index is None:\n",
        "        #! ASSUMPTION: if there is no enough elements, get_random_index might return the same index over and over again. preventing the loop from terminating:\n",
        "        index = get_random_index(y_train, high_risk_labels)\n",
        "        if not index in indexes_to_remove:\n",
        "            indexes_to_remove.add(index)\n",
        "            num_of_high_risk -= 1\n",
        "            \n",
        "    print(f\"len(indexes_to_remove) 2: {len(indexes_to_remove)}\")\n",
        "    while num_of_medium_risk > 0 and len(indexes_to_remove) < possible_num_of_index_to_remove:\n",
        "        #! check if index is None:\n",
        "        index = get_random_index(y_train, medium_risk_labels)\n",
        "        if not index in indexes_to_remove:\n",
        "            indexes_to_remove.add(index)\n",
        "            num_of_medium_risk -= 1\n",
        "        \n",
        "        \n",
        "        # if not index in indexes_to_remove:\n",
        "        #     indexes_to_remove.extend(index)\n",
        "        #     num_of_high_risk -= 1\n",
        "            \n",
        "    # while num_of_medium_risk > 0 and len(indexes_to_remove) < num_samples_to_remove:\n",
        "    #     index = np.random.choice(np.where(y_train == random.choice(medium_risk_labels))[0], size=1, replace=False)\n",
        "    #     if not index in indexes_to_remove:\n",
        "    #         indexes_to_remove.extend(index)\n",
        "    #         num_of_medium_risk -= 1\n",
        "\n",
        "    return indexes_to_remove\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def test_heuristic_to_identify_samples():\n",
        "    # Create a mock dataset\n",
        "    y_train_mock = np.array([0, 2, 3, 1, 4, 7, 5, 6, 8, 9, 0, 2])  # Example labels\n",
        "    num_samples_to_remove = len(y_train_mock)-2  # Example number of samples to remove\n",
        "\n",
        "    # Expected outcome\n",
        "    expected_high_risk_labels = [0, 2, 3, 5, 6, 8, 9]\n",
        "    expected_indexes = []\n",
        "\n",
        "    for i, label in enumerate(y_train_mock):\n",
        "        if label in expected_high_risk_labels:\n",
        "            expected_indexes.append(i)\n",
        "            if len(expected_indexes) == num_samples_to_remove:\n",
        "                break\n",
        "\n",
        "    # Call the function\n",
        "    actual_indexes = heuristic_to_identify_samples(None, y_train_mock, num_samples_to_remove)\n",
        "    print(f\"{len(actual_indexes)} != {num_samples_to_remove}\")\n",
        "    # Assert the results\n",
        "    # assert len(actual_indexes) == num_samples_to_remove, \"Incorrect number of samples removed\"\n",
        "    # assert all([index in expected_indexes for index in actual_indexes]), \"Incorrect indexes identified for removal\"\n",
        "\n",
        "def divide_into_even_subsets(array, subset_percentage=10):\n",
        "    #! refactor this:\n",
        "    \"\"\"\n",
        "    Divides an array into smaller subsets, each approximately of a specified percentage size of the original array.\n",
        "    The function evenly distributes any extra elements among the subsets.\n",
        "    \n",
        "    :param array: The original NumPy array to divide.\n",
        "    :param subset_percentage: The size of each subset as a percentage of the original array.\n",
        "    :return: A list of NumPy arrays, each being a subset of the original array.\n",
        "    \"\"\"\n",
        "    total_size = len(array)\n",
        "    subset_base_size = total_size * subset_percentage // 100\n",
        "    extra_elements = total_size - (subset_base_size * 10)\n",
        "    \n",
        "    subsets = []\n",
        "    start_index = 0\n",
        "\n",
        "    for i in range(10):\n",
        "        end_index = start_index + subset_base_size + (1 if i < extra_elements else 0)\n",
        "        subsets.append(array[start_index:end_index])\n",
        "        start_index = end_index\n",
        "\n",
        "    return subsets\n",
        "\n",
        "def find_most_representative_subset(subsets, full_data):\n",
        "    \"\"\"\n",
        "    Finds the subset that is most representative of the full dataset based on mean and standard deviation.\n",
        "    \n",
        "    :param subsets: A list of NumPy arrays (subsets).\n",
        "    :param full_data: The original full dataset as a NumPy array.\n",
        "    :return: The index of the most representative subset.\n",
        "    \"\"\"\n",
        "    full_mean = np.mean(full_data)\n",
        "    full_std = np.std(full_data)\n",
        "\n",
        "    closest_mean_index = None\n",
        "    closest_mean_diff = float('inf')\n",
        "\n",
        "    for i, subset in enumerate(subsets):\n",
        "        subset_mean = np.mean(subset)\n",
        "        subset_std = np.std(subset)\n",
        "\n",
        "        mean_diff = abs(subset_mean - full_mean)\n",
        "        std_diff = abs(subset_std - full_std)\n",
        "\n",
        "        # Combine the differences in mean and standard deviation\n",
        "        total_diff = mean_diff + std_diff\n",
        "\n",
        "        if total_diff < closest_mean_diff:\n",
        "            closest_mean_diff = total_diff\n",
        "            closest_mean_index = i\n",
        "\n",
        "    return closest_mean_index\n",
        "\n",
        "# initially the pruned dataset is the entire dataset:\n",
        "x_train_pruned, y_train_pruned = x_train, y_train\n",
        "highest_accuracy = 0\n",
        "depth = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def myPrunedSubsetMethod(x_train, y_train, model):\n",
        "    \n",
        "    pruned_indexes = {} # to be returned\n",
        "    temp_pruned_indexes_1 = {}\n",
        "    temp_pruned_indexes_2 = {}\n",
        "    counter = 0 # for naming the nodes (first will 1 as 0 is technically the root)\n",
        "    \n",
        "    divide_factor = 10 # %\n",
        "    x_subsets = divide_into_even_subsets(x_train, subset_percentage=divide_factor)\n",
        "    y_subsets = divide_into_even_subsets(y_train, subset_percentage=divide_factor)\n",
        "    \n",
        "    most_representative_x_subset_index = find_most_representative_subset(x_subsets, x_train)\n",
        "    most_representative_y_subset_index = find_most_representative_subset(y_subsets, y_train)\n",
        "    \n",
        "    # validation_x_set = x_subsets[most_representative_x_subset_index]\n",
        "    # validation_y_set = y_subsets[most_representative_x_subset_index]\n",
        "    # all the other subsets are training sets:\n",
        "    # training_x_set = np.concatenate([x for i, x in enumerate(x_subsets) if i != most_representative_x_subset_index])\n",
        "    # training_y_set = np.concatenate([y for i, y in enumerate(y_subsets) if i != most_representative_y_subset_index])\n",
        "    \n",
        "    # loss, accuracy = trainAndEvaluateModel(training_x_set, training_y_set, validation_x_set, validation_y_set, model, None)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # to get incremented as training progresses to find optimal number of samples to remove:\n",
        "    min_num_samples_to_remove = 1\n",
        "    \n",
        "myPrunedSubsetMethod(x_train, y_train, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_86qcqRrYRxd",
      "metadata": {
        "id": "_86qcqRrYRxd"
      },
      "outputs": [],
      "source": [
        "# def myPrunedSubsetMethod(x_train, y_train, model):\n",
        "\n",
        "#     pruned_indexes = {} # to be returned\n",
        "#     temp_pruned_indexes_1 = {}\n",
        "#     temp_pruned_indexes_2 = {}\n",
        "#     counter = 0 # for naming the nodes (first will 1 as 0 is technically the root)\n",
        "    \n",
        "#     # global x_train_pruned\n",
        "#     # global y_train_pruned\n",
        "    \n",
        "#     # to get incremented as training progresses to find optimal number of samples to remove:\n",
        "#     min_num_samples_to_remove = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     base_loss, base_accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, None)\n",
        "#     highest_accuracy = base_accuracy  \n",
        "    \n",
        "\n",
        "#     indexes_to_remove = heuristic_to_identify_samples(x_train, y_train, min_num_samples_to_remove)\n",
        "    \n",
        "#     x_train_pruned = np.delete(x_train, indexes_to_remove, axis=0)\n",
        "#     y_train_pruned = np.delete(y_train, indexes_to_remove, axis=0)\n",
        "\n",
        "#     # 1. train the model on the entire dataset. (start state)\n",
        "#     # 2. get the predictions.\n",
        "#     base_loss, base_accuracy = trainAndEvaluateModel(x_train_pruned, y_train_onehot, x_test, y_test_onehot, model, None)\n",
        "    \n",
        "#     highest_accuracy = base_accuracy\n",
        "#     # 3. improved accuracy (accuracy - base_accuracy) is the reward function (what is the utility?). Make the state diagram:\n",
        "#     update_accuracy_loss(root, base_accuracy, base_loss) # nothing is pruned at the start state\n",
        "    \n",
        "    \n",
        "    \n",
        "    #! think about how to do this: I'm not making a graph anymore\n",
        "    \n",
        "    \n",
        "\n",
        "    # 4. retrain the model without a subset of the data (next state, first child of start state).\n",
        "    # remove 1% of the data:\n",
        "    # num_samples = x_train_pruned.shape[0]\n",
        "    \n",
        "    # # remove_percentage = 0.001\n",
        "    # # depth = 0\n",
        "    \n",
        "    # try:\n",
        "    #     while highest_accuracy < 0.90 and depth < 10:\n",
        "            \n",
        "    #         print(f\"\\nRunning iteration {depth}\")\n",
        "            \n",
        "    #         counter, accuracy_1, temp_pruned_indexes_1 = retrain_model(x_train_pruned, y_train_pruned, num_samples, counter)\n",
        "            \n",
        "    #         # 6. keep track of the highest accuracy. if we get >95% then stop.\n",
        "    #         if accuracy_1 > highest_accuracy:\n",
        "    #             print (\"Accuracy improved!\")\n",
        "    #             highest_accuracy = accuracy_1\n",
        "    #             if accuracy_1 > 0.95:\n",
        "    #                 break\n",
        "                \n",
        "\n",
        "    #         # 5. retrain the model without another subset of the data (next state, second child of start state).\n",
        "    #         counter, accuracy_2, temp_pruned_indexes_2 = retrain_model(x_train_pruned, y_train_pruned, num_samples, counter)\n",
        "\n",
        "    #         # 6. keep track of the highest accuracy. if we get >95% then stop.\n",
        "    #         if accuracy_2 > highest_accuracy:\n",
        "    #             print (\"Accuracy improved!\")\n",
        "    #             highest_accuracy = accuracy_2\n",
        "    #             if accuracy_2 > 0.95:\n",
        "    #                 break\n",
        "            \n",
        "    #         # 7. compare the two accuracies. keep the one with the highest accuracy. (next state, first child of first child of start state).\n",
        "    #         if accuracy_1 > accuracy_2:\n",
        "    #             print(f\"Accuracy 1: {accuracy_1} > Accuracy 2: {accuracy_2}\")\n",
        "    #             x_train_pruned, y_train_pruned = prune_points(x_train_pruned, y_train_pruned, temp_pruned_indexes_1)\n",
        "    #             pruned_indexes = temp_pruned_indexes_1\n",
        "    #         else:\n",
        "    #             print(f\"Accuracy 2: {accuracy_2} > Accuracy 1: {accuracy_1}\")\n",
        "    #             x_train_pruned, y_train_pruned = prune_points(x_train_pruned, y_train_pruned, temp_pruned_indexes_2)\n",
        "    #             pruned_indexes = temp_pruned_indexes_2\n",
        "                \n",
        "    # except KeyboardInterrupt as e:\n",
        "    #     print(e)\n",
        "    #     return pruned_indexes\n",
        "        \n",
        "    # else:\n",
        "    #     depth += 1\n",
        "    #     return pruned_indexes\n",
        "    \n",
        "    # 7. keep doing so until d depth or highest accuracy achieved? making a binary search tree.\n",
        "\n",
        "    # 8. use a search algorithm to find the best path to the goal state (highest accuracy). Use some heuristic that comes from the rules of how we corrupt the data (i.e. some digits are never corrupted. what digits get corrupted to what?).\n",
        "    \n",
        "    # depth += 1\n",
        "    \n",
        "    # return pruned_indexes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9967 - accuracy: 0.2405\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9536 - accuracy: 0.2707\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9418 - accuracy: 0.2756\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "Accuracy: 0.9046\n",
            "313/313 [==============================] - 1s 1ms/step - loss: 1.3554 - accuracy: 0.9046\n",
            "\n",
            "Running iteration 0\n",
            "Epoch 1/3\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 1.9360 - accuracy: 0.2780\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9309 - accuracy: 0.2802\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9262 - accuracy: 0.2816\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "Accuracy: 0.9079\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 1.4044 - accuracy: 0.9079\n",
            "Accuracy improved!\n",
            "Epoch 1/3\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 1.9237 - accuracy: 0.2820\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 1.9211 - accuracy: 0.2835\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 1.9180 - accuracy: 0.2835\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "Accuracy: 0.9155\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 1.4227 - accuracy: 0.9155\n",
            "Accuracy improved!\n",
            "Accuracy 2: 0.9154999852180481 > Accuracy 1: 0.9078999757766724\n",
            "\n",
            "Running iteration 0\n",
            "Epoch 1/3\n",
            "468/468 [==============================] - 6s 12ms/step - loss: 2.2028 - accuracy: 0.1268\n",
            "Epoch 2/3\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 2.1829 - accuracy: 0.1340\n",
            "Epoch 3/3\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 2.1807 - accuracy: 0.1370\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "Accuracy: 0.1859\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.4477 - accuracy: 0.1859\n",
            "Epoch 1/3\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 2.1789 - accuracy: 0.1378\n",
            "Epoch 2/3\n",
            "468/468 [==============================] - 7s 14ms/step - loss: 2.1778 - accuracy: 0.1419\n",
            "Epoch 3/3\n",
            "468/468 [==============================] - 7s 15ms/step - loss: 2.1772 - accuracy: 0.1438\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "Accuracy: 0.2245\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.4147 - accuracy: 0.2245\n",
            "Accuracy 2: 0.22450000047683716 > Accuracy 1: 0.1859000027179718\n",
            "\n",
            "Running iteration 0\n",
            "Epoch 1/3\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 2.1826 - accuracy: 0.1293\n",
            "Epoch 2/3\n",
            "468/468 [==============================] - 6s 13ms/step - loss: 2.1803 - accuracy: 0.1344\n",
            "Epoch 3/3\n",
            "160/468 [=========>....................] - ETA: 4s - loss: 2.1785 - accuracy: 0.1384\n",
            "pruned_indexes: {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 64: 64, 65: 65, 66: 66, 68: 68, 69: 69, 70: 70}\n"
          ]
        }
      ],
      "source": [
        "pruned_indexes = myPrunedSubsetMethod(x_train, y_train, model)\n",
        "print(f\"pruned_indexes: {pruned_indexes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60}\n",
            "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60}\n",
            "{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 64: 64, 65: 65, 66: 66, 68: 68, 69: 69, 70: 70}\n",
            "{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 64: 64, 65: 65, 66: 66, 68: 68, 69: 69, 70: 70}\n"
          ]
        }
      ],
      "source": [
        "for node in root[\"Next States\"]:\n",
        "    print(node[\"Pruned\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9965 - accuracy: 0.2354\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 1.9426 - accuracy: 0.2698\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 1.9342 - accuracy: 0.2758\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "Accuracy: 0.8947\n",
            "313/313 [==============================] - 0s 1ms/step - loss: 1.4137 - accuracy: 0.8947\n",
            "Accuracy: 0.8946999907493591\n",
            "Loss: 1.4137206077575684\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = trainAndEvaluateModel(x_train, y_train_onehot, x_test, y_test_onehot, model, pruned_indexes)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def add_nodes_and_edges(graph, parent_node):\n",
        "    decimal_points = 2\n",
        "    parent_state = parent_node[\"State\"]\n",
        "    for child in parent_node[\"Next States\"]:\n",
        "        child_state = child[\"State\"]\n",
        "        # Create a label with selected information\n",
        "        label = f\"{child_state}\\nAccuracy: {round(child['Accuracy'], decimal_points)}\\nLoss: {round(child['Loss'], decimal_points)}\"\n",
        "        graph.add_node(child_state, label=label)\n",
        "        graph.add_edge(parent_state, child_state)\n",
        "        add_nodes_and_edges(graph, child)\n",
        "\n",
        "def plot_tree(tree):\n",
        "    decimal_points = 2\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add the root node\n",
        "    root_state = tree[\"State\"]\n",
        "    root_label = f\"{root_state}\\nAccuracy: {round(tree['Accuracy'], decimal_points)}\\nLoss: {round(tree['Loss'], decimal_points)}\"\n",
        "    G.add_node(root_state, label=root_label)\n",
        "\n",
        "    # Add children recursively\n",
        "    add_nodes_and_edges(G, tree)\n",
        "\n",
        "    # Draw the graph\n",
        "    pos = nx.spring_layout(G)\n",
        "    labels = nx.get_node_attributes(G, 'label')\n",
        "    nx.draw(G, pos, labels=labels, with_labels=True, node_color=\"skyblue\", node_size=3000, font_size=8, font_weight=\"bold\", arrows=True)\n",
        "    plt.show()\n",
        "    \n",
        "plot_tree(root)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
